<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:media="http://search.yahoo.com/mrss/">
<channel>
<title>当然我在扯淡</title>
<link>http://www.yinwang.org/</link>
<description></description>
<language>zh-cn</language>
<lastBuildDate>Fri, 13 Nov 2015 04:22:04 +0800</lastBuildDate>
<item>
<title>大数据的高烧</title>
<link>http://yinwang.org/blog-cn/2015/11/11/big-data</link>
<description>&lt;p&gt;
  　　
  &lt;/p&gt;&lt;h2&gt;大数据的高烧&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;http://upload-images.jianshu.io/upload_images/68562-4b4c75a74fe19812.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;&quot;&gt;&lt;/p&gt;&lt;p&gt;大数据真的很热，烧得很热。有人说，现在数据是最宝贵的资源，超过石油，电和水，有了数据就有了一切。&lt;/p&gt;&lt;p&gt;Google的研究总监Peter Norvig，每次给实习生做讲座，都会鼓吹大数据的功效，说牛顿爱因斯坦算什么啊，比不上我们Google的大数据。看&lt;a href=&quot;https://translate.google.com&quot;&gt;Google Translate&lt;/a&gt;多么的厉害啊，它根本不需要parse和理解人类语言，只要把很多的对照翻译文本丢进去，然后它就能自己学习，实现机器自动翻译了。&lt;/p&gt;&lt;p&gt;我曾经帮助过一个做通信app的公司。一开头的时候，他们的CEO跟我说：“我们准备用大数据分析来发现UI设计中存在的问题。比如，通过分析用户在app注册过程的哪一个页面退出，再也没有回来，我们就知道那个页面有问题……”&lt;/p&gt;&lt;p&gt;如此种种的发烧症状，跟文化大革命的年代有得一拼…… 许多的人，只知道大数据的名字，就脑残一般的相信它能解决世界上所有的问题。可是大数据真的解决了问题吗？&lt;/p&gt;&lt;p&gt;Peter Norvig大力鼓吹的Google Translate，真的好用吗？你愿意看它翻译出来的文章或者书籍？你敢用它来生成多国语言的产品说明书？实际上Google Translate翻译的东西质量如此糟糕，表面上看上去貌似那个语言的样子，细看就发现其实不是很像人话。大部分时候，你只能说“凑合知道这文章在说什么”而已。实际上，你必须能够parse人类语言的句子，真正理解里面的结构和涵义，建立思维模型，才有可能真正准确的翻译人类语言。拿Google Translate就想证明大数据的功效，藐视语言学研究甚至其它科学的价值，这烧发得真够可以。&lt;/p&gt;&lt;p&gt;至于那个通信app公司，我只用了他们的app两次，就发现用户注册界面有一个严重的设计失误。屏幕显示：“新用户注册”。当你输入自己的电话号码之后，它会弹出一个窗口说：“现在我们要给你发一个短信，验证你的电话号码。OK？”然而，当你输入了电话号码，他们的服务器却发现你的电话号码已经注册过（是一个回头用户），他们就会转而给你另一个窗口，让你输入账号密码，而不是短信验证码。&lt;/p&gt;&lt;p&gt;输入密码和短信验证码的窗口样式，并没有很大差别，只不过输入栏旁边的小字有一点不同。由于之前已经提示要发短信验证码，所以用户根本不会去看那输入栏旁边的小字，就会认为这是要他输入短信验证码的界面，所以他就会一直等那条短信。可是由于这人已经注册，所以服务器不会发出短信，而是等待他输入密码。所以用户在等短信，而app在等密码。几分钟之后，用户终于决定，这是一个劣质的app，这么久没用还是这么烂，所以删了它……&lt;/p&gt;&lt;p&gt;当我把这个发现告诉他们老总的时候，他说：“这个不关你的事！这界面是我们请专业的UX设计师弄的，你外行就别管了！”设计问题明摆在眼前，玩几下就发现了，可有人就是视而不见，你拿他有啥办法？于是呢，他继续叫人分析这app的登录数据，然后向投资人鼓吹自己采用的各种尖端big data技术，最后用户越来越少，走上穷途末路……&lt;/p&gt;&lt;p&gt;所以你看到了，大数据并不能代替好的科学家，不能代替好的设计师。像任何工具一样，大数据有适用的地方，也有它解决不了的问题。我不否认大数据在某些方面能够提供帮助，然而这种认为“大数据能解决一切问题”的想法，只是高烧产生的幻觉而已。&lt;/p&gt;</description>
<author>yinwang0</author>
<guid isPermaLink="false">big-data</guid>
<pubDate>Wed, 11 Nov 2015 00:00:00 +0800</pubDate>
</item>
<item>
<title>图灵的光环</title>
<link>http://yinwang.org/blog-cn/2015/10/18/turing</link>
<description>&lt;p&gt;
  　　
  &lt;/p&gt;&lt;h2&gt;图灵的光环&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;http://upload-images.jianshu.io/upload_images/68562-5e4e06e891dfddfa.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;&quot;&gt;&lt;/p&gt;&lt;p&gt;仿佛全世界的人都知道，&lt;a href=&quot;https://en.wikipedia.org/wiki/Alan_Turing&quot;&gt;图灵&lt;/a&gt;（Alan Turing）是个天才，是他创造了计算机科学，是他破解了德国纳粹的Enigma密码。由于他的杰出贡献，计算机科学的最高荣誉，被叫做“图灵奖”。然而根据自己一直以来对图灵机等计算模型的看法，加上一些历史资料，我发现图灵本人的实际成就，相对于他所受到的崇拜，其实相差甚远。&lt;/p&gt;&lt;p&gt;由于二战以来各国政府对于当时谍报工作的保密措施造成的事实混淆，再加上图灵的不幸生世所引来的同情，图灵这个名字似乎拥有了一种扑朔迷离的光环。人们把很多本来不是图灵作出的贡献归结在他身上，把本来很平常的贡献过分地夸大。图灵的光环，掩盖了许多对这些领域做出过更加重要贡献的人。&lt;/p&gt;&lt;h3&gt;图灵传&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;http://upload-images.jianshu.io/upload_images/68562-ae05459a3fdc19cf.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;&quot;&gt;&lt;/p&gt;&lt;p&gt;2012年，在图灵诞辰&lt;a href=&quot;http://cacm.acm.org/magazines/2013/1/158780-who-begat-computing/fulltext&quot;&gt;一百周年&lt;/a&gt;的时候，人们风风火火的召开各种大会，纪念这位“计算机之父”，很多媒体也添油加醋地宣传他的丰功伟绩。还有个叫Andrew Hodges的人，抓住这个时机推销自己写的一本传记，叫做《&lt;a href=&quot;http://www.amazon.com/Alan-Turing-Enigma-Andrew-Hodges/dp/069115564X&quot;&gt;Alan Turing: The Enigma&lt;/a&gt;》。这本书红极一时，后来还被改编成了电影。&lt;/p&gt;&lt;p&gt;这本传记看似客观，引经据典，字里行间却可以感受到作者对图灵个人的膜拜和偏袒，他在倾心打造一个“天才”。作者片面地使用对图灵有利的证据，对不利的方面只字不提。仿佛图灵做的一切都是有理的，他做的不好的地方都是因为别人的问题，或者风水不好。提到别人做的东西，尽是各种缺陷和局限性，不是缺陷也要说成是缺陷；提到图灵的工作，总是史无前例，开天辟地的发明。别人先做出来的东西，生拉硬拽，硬要说成是受了图灵的“启发”，还怪别人没有引用图灵的论文。这让你感觉仿佛别人都在抄袭图灵伟大的研究成果，都在利用他，欺负他似的。如果你不想花钱买书，可以看看同一作者写的一个&lt;a href=&quot;http://www.turing.org.uk/publications/dnb.html&quot;&gt;图灵简要生平&lt;/a&gt;，足以从中感受到这种倾向。&lt;/p&gt;&lt;p&gt;我写这篇文章的很大一部分原因，就是因为这本传记。作者对图灵贡献的片面夸大，对其他一些学者的变相贬低，让我感到不平。图灵在计算机界的名声，本来就已经被严重的夸大和美化，被很多人盲目的崇拜。现在出了这本传记和电影，又在人们心中加重了这层误解。所以我觉得有必要澄清一些事实，让人们不再被迷惑。&lt;/p&gt;&lt;h3&gt;密码学&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;http://upload-images.jianshu.io/upload_images/68562-431f7a82e38fa57b.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;&quot;&gt;&lt;/p&gt;&lt;p&gt;很多人提到二战Enigma密码的故事，就会把功劳一股脑地归到图灵头上，只字不提其他人。其实呢，破解Enigma密码是很多人共同努力的结果，图灵只是其中的一员。这些人缺少了任何一个，都可能是灾难性的后果。其中好些人的想法早于图灵，启发过图灵，贡献比图灵的大，设计的东西比图灵的先进，却很少有人听说过他们的名字。论智力和贡献，图灵在其中只是中等水平，最后说起来倒好像是他单枪匹马拯救了大家，这是不公平的。&lt;/p&gt;&lt;p&gt;最初破掉Enigma密码的，其实不是英国人，而是波兰人。波兰人不但截获并且仿造了德国人的Enigma机器，而且发现了其中微妙的漏洞，发明了一种用于解密的机器叫做&lt;a href=&quot;https://en.m.wikipedia.org/wiki/Bomba_(cryptography)&quot;&gt;BOMBA&lt;/a&gt;，以及一种手工破解的方法叫做&lt;a href=&quot;https://en.wikipedia.org/wiki/Zygalski_sheets&quot;&gt;Zygalski sheets&lt;/a&gt;。BOMBA可以在两个小时之内破解掉Enigma密码。波兰人一声不吭地窃听了德国人的通信长达六年半，最后在二战爆发前夕把这技术送给了英法盟友。&lt;/p&gt;&lt;p&gt;BOMBA的工作原理，其实就是模拟好几个Enigma机器，“并发”运转，这样可以加速猜出秘钥。最开头这样还行，但后来德国人改进了Enigma机器，把可选的齿轮从3个增加到了5个。5选3，有60种情况，这样秘钥的空间增大了60倍。理论上BOMBA只要运转60倍多的Enigma机器，就可以破解这增大的解空间，然而那已经超出了波兰的物资和人力。再加上德国人就要打过去，所以波兰只好请英法盟友帮忙。&lt;/p&gt;&lt;p&gt;图灵最重要的贡献，就是改进波兰人的&lt;a href=&quot;https://en.m.wikipedia.org/wiki/Bomba_(cryptography)&quot;&gt;BOMBA&lt;/a&gt;，设计了一个更好的机器叫&lt;a href=&quot;https://en.m.wikipedia.org/wiki/Bombe&quot;&gt;BOMBE&lt;/a&gt;。BOMBE比起BOMBA，其实并没有质的飞跃，只不过BOMBE同时模拟的Enigma机器更多，转的更快。另外它加入了一些“优化”措施，尽早排除不可行的路径，所以速度快很多。图灵最初的设计，要求必须能够事先猜出很长的文本，所以基本不能用。后来&lt;a href=&quot;https://en.wikipedia.org/wiki/Gordon_Welchman&quot;&gt;Gordon Welchman&lt;/a&gt;发明了一种电路，叫做diagonal board，才使Bombe能够投入实用。关于Gordon Welchman的故事，你可以参考这个&lt;a href=&quot;https://www.youtube.com/watch?v=t8gPED2veig&quot;&gt;BBC纪录片&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;在Bombe能够投入使用之前，有一个叫&lt;a href=&quot;https://en.m.wikipedia.org/wiki/Herivel_tip&quot;&gt;John Herivel&lt;/a&gt;的人，发现了一种特殊的技巧，叫做Herivel tip，这种技术在Bombe投入使用之前几个月就已经投入实用，破解掉很多德军的消息，立下汗马功劳。如果Herivel tip没有被发明，盟军可能在1940年5月就已经战败，BOMBE也就根本没机会派上用场。&lt;/p&gt;&lt;p&gt;同时在Bletchley Park，还诞生了一台大型可编程电子计算机&lt;a href=&quot;http://www.cryptomuseum.com/crypto/colossus/index.htm&quot;&gt;Colossus&lt;/a&gt;，它是由一个叫&lt;a href=&quot;https://en.wikipedia.org/wiki/Tommy_Flowers&quot;&gt;Tommy Flowers&lt;/a&gt;的工程师设计的。Colossus不是用来破解Enigma密码的，而是用于破解&lt;a href=&quot;http://www.cryptomuseum.com/crypto/lorenz/sz40/index.htm&quot;&gt;Lorenz SZ-40&lt;/a&gt;。那是一种比Enigma还要先进的密码机器，用于发送希特勒的最高指令。&lt;/p&gt;&lt;p&gt;德国人后来又改进了他们的通信方式，使用了一种具有四个齿轮的Enigma机器。这大大的增加了破解的难度，普通的Bombe机器也破不了它了。后来是&lt;a href=&quot;https://en.wikipedia.org/wiki/Harold_Keen&quot;&gt;Harold Keen&lt;/a&gt;设计了一个叫做Mammoth的机器，后来加上美国海军的帮助，制造了更快的Bombe，才得以破解。&lt;/p&gt;&lt;p&gt;所以你看到了，所有这些人的工作加起来，才改善了二战的局面。波兰人的BOMBA，已经包含了最重要的思想。图灵的工作其实更多是量的改进，而不是质的飞跃。现在很多人喜欢跟风，片面的夸大图灵在其中的作用，这是不对的。如果你对Enigma机器的技术细节感兴趣，可以参考这两个视频：[&lt;a href=&quot;https://www.youtube.com/watch?v=G2_Q9FoD-oQ&quot;&gt;视频1&lt;/a&gt;][&lt;a href=&quot;https://www.youtube.com/watch?v=V4V2bpZlqx8&quot;&gt;视频2&lt;/a&gt;]。&lt;/p&gt;&lt;h3&gt;理论计算机科学&lt;/h3&gt;&lt;p&gt;&lt;a href=&quot;http://abstrusegoose.com/206&quot;&gt;&lt;img src=&quot;http://upload-images.jianshu.io/upload_images/68562-c0c3a9d622234896.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;&quot;&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;图灵被称为“计算机之父”，计算机科学界的最高荣誉，被叫做“图灵奖”（Turing Award）。然而如果你深入的理解了计算理论和程序语言理论就会发现，图灵对于理论计算机科学，其实并没产生长远而有益的影响。在某种程度上说，他其实帮了一个倒忙。图灵的理论复杂不堪，给人们造成很大的误导，阻碍了计算机科学的发展。而且他对于发表论文，对待研究的态度让我怀疑，我觉得图灵本人其实就是当今计算机学术界的一些不正之风的鼻祖。&lt;/p&gt;&lt;h4&gt;图灵机和lambda演算&lt;/h4&gt;&lt;p&gt;绝大部分计算机专业的人提到图灵，就会想起图灵机（Turing Machine）。稍微有点研究的人，可能知道图灵机与lambda演算在计算能力上的等价性。然而在“计算能力”上等价，并不等于说它们具有同样的价值，随便用哪个都无所谓。科学研究有一条通用的原则：如果多个理论可以解释同样的现象，取最简单的一个。虽然lambda演算和图灵机能表达同样的理论，却比图灵机简单，优雅，实用很多。&lt;/p&gt;&lt;p&gt;计算理论（Theory of Computation）这个领域，其实是被图灵机给复杂化了。图灵机的设计是丑陋，复杂，缺乏原则的。它的读写头，纸带，状态，操作，把本来很简单的语义搞得异常复杂。图灵机的读写两种操作同时发生，这恰好是编程上最忌讳的一种错误，类似于C语言的&lt;code&gt;i++&lt;/code&gt;。图灵机是如此的复杂和混淆，以至于你很难看出它到底要干什么，也很难用它清晰地表达自己的意思。这就是为什么每个人上“计算理论”课程，都会因为图灵机而头痛。如果你挖掘一点历史，也许会发现图灵机的原型，其实是图灵母亲使用的打字机。用一台打字机来建模所有的计算，这当然是可行的，然而却复杂不堪。&lt;/p&gt;&lt;p&gt;相比之下，lambda演算更加简单，优雅，实用。它是一个非常有原则的设计。Lambda演算不但能清晰地显示出你想要表达的意思，而且有直接的“物理实现”。你可以自然的把一个lambda演算表达式，看成是一个电子线路模块。对于现实的编程语言设计，系统设计，lambda演算有着巨大的指导和启发意义。以至于很多&lt;a href=&quot;https://existentialtype.wordpress.com/2011/03/16/languages-and-machines&quot;&gt;理解lambda演算的人&lt;/a&gt;都搞不明白，图灵机除了让一些理论显得高深莫测，还有什么存在的意义。&lt;/p&gt;&lt;h4&gt;历史的倒退&lt;/h4&gt;&lt;p&gt;图灵机比起lambda演算来说，其实是一个&lt;a href=&quot;http://www.users.waitrose.com/~hindley/SomePapers_PDFs/2006CarHin,HistlamRp.pdf&quot;&gt;历史&lt;/a&gt;的倒退。1928年，Alonzo Church发明了lambda演算（当时他25岁）。Lambda演算被设计为一个通用的计算模型，并不是为了解决某个特定的问题而诞生的。1929年，Church成为普林斯顿大学教授。1932年，Church在Annals of Mathematics发表了一篇&lt;a href=&quot;https://www.ics.uci.edu/~lopes/teaching/inf212W12/readings/church.pdf&quot;&gt;论文&lt;/a&gt;，纠正逻辑领域里几个常见的问题，他的论述中用了lambda演算。1935年，Church发表&lt;a href=&quot;http://www.jstor.org/stable/2371045&quot;&gt;论文&lt;/a&gt;，使用lambda演算证明基本数论中存在不可解决的问题。1936年4月，Church发表了一篇两页纸的“&lt;a href=&quot;https://users.fit.cvut.cz/~staryja2/MIVYC/church-a-note-on-the-entscheidungsproblem.pdf&quot;&gt;note&lt;/a&gt;”，指出自己1935年那篇论文可以推论得出，著名的Hilbert“&lt;a href=&quot;https://en.wikipedia.org/wiki/Entscheidungsproblem&quot;&gt;可判定性问题&lt;/a&gt;”是不可解决的。&lt;/p&gt;&lt;p&gt;1936年5月，当时还在剑桥读硕士的图灵，也写了一篇论文，使用自己设计的一种“计算机器”（后来被叫做图灵机）来证明同一个问题。图灵的论文投稿，比Church最早的结论发表，晚了整整一年。编辑从来没见过图灵机这样的东西，而且它纷繁复杂，远没有lambda演算来得优雅。就像所有人对图灵机的第一印象一样，编辑很难相信这打字机一样的操作方式，能够容纳“所有的计算”。他无法确定图灵的论述是否正确，只好找人帮忙。Church恐怕是当时世界上唯一能够验证图灵的论文正确性的人。所以一番好心之下，编辑写了封信给Church，说：“这个叫图灵的年轻人很聪明，他写了一篇论文，使用一种机器来证明跟你一样的结果。他会把论文寄给你。如果你发现他的结果是正确的而且有用，希望你帮助他拿到奖学金，进入Princeton跟你学习。”&lt;/p&gt;&lt;p&gt;图灵就是这样成为了Church的学生，然而图灵心高气傲，恐怕从来没把Church当成过老师，反倒总觉得Church抢先一步，破坏了自己名垂青史的机会。跟Church的其它学生不一样，图灵没能理解lambda演算的精髓，却认为自己的机器才是最伟大的发明。进入Princeton之后，图灵不虚心请教，只是一心想发表自己的论文，想让大家对自己的“机器”产生兴趣，结果遭到很大的挫折。当然了，一个名不见经传的人，做了个怪模怪样的机器，说它可以囊括宇宙里所有的计算，你不被当成民科才怪呢！&lt;/p&gt;&lt;p&gt;1937年，在Church的帮助下，图灵的那篇&lt;a href=&quot;http://plms.oxfordjournals.org/content/s2-42/1/230.full.pdf+html&quot;&gt;论文&lt;/a&gt;（起名为《Computable Numbers》）终于发表了。Church还是很器重图灵的，他把图灵的机器叫做“图灵机”。不幸的是，论文发表之后，学术界对此几乎没有任何反响，只有两人向图灵索取这篇论文。图灵当然不爽了，于是后来就到处推销自己的图灵机，想让大家承认那是伟大的发明。有了一个锤子，看什么都是钉子。后来每到一个地方，每做一个项目（见下一节），他都想把问题往自己那篇论文和图灵机上靠，东拉西扯的想证明它的价值，甚至称别人发明的东西全都是受到了图灵机的启发…… 经过人们很长的时间的以讹传讹之后，他终于成功了。&lt;/p&gt;&lt;p&gt;图灵当年的作法，其实跟当今计算机学术界的普遍现象差不多。我想发表自己的想法A，结果别人已经发表了B，解决了A要解决的问题，而且还比A简单和清晰。怎么办呢？首先，我声明自己从没看过B的论文，这样就可以被称为“独立的发现”。然后，我证明A和B在“本质”上是等价的。最后，我东拉西扯，挖掘一下B的局限性，A相对于B在某些边沿领域的优势…… 这样反复折腾，寻找A的优势，总有一天会成功发表的。一旦发表成功，就会有人给我唱高调，没用的东西也要说成是有用的。他们会在A的基础上发展他们自己的东西，最后把我推崇为大师。那发表更早，更简单优美的B，也就无人问津了。胜利！&lt;/p&gt;&lt;p&gt;现在不得不说一下《图灵传》对此的歪曲。Church的论文发表，比图灵的论文投稿还早一年，而且Church使用了比图灵机更简单优雅的计算模型。Church的成果本来天经地义应该受到更多的尊重，到头来作者却说：“...and Turing was &lt;em&gt;robbed&lt;/em&gt; of the full reward for his originality”（见第3节“&lt;a href=&quot;http://www.turing.org.uk/publications/dnb.html&quot;&gt;The Turing machine&lt;/a&gt;”）。让人感觉貌似是Church用不正当的手段“抢走”了图灵的“原创性”一样。你本来没有什么原创性，还丑陋复杂，所以何谈抢走呢？我怎么觉得恰恰相反，其实是图灵抢走了Church的原创性？现在提起Hilbert可判定性问题，可计算性理论，人们都想起图灵，有谁还想得起Church，有谁知道他是第一个解决了这问题的人，有谁知道他用了更优美的办法？&lt;/p&gt;&lt;h4&gt;Lambda演算与计算理论&lt;/h4&gt;&lt;p&gt;由于图灵到处推销自己的理论，把不好的东西说成是好的，把别人发明的机器硬往自己的理论上面靠，说他们受到了图灵机的“启发”，以至于很多人被蛊惑，以为它比起lambda演算确实有优势。再加上很多人为了自己的利益而以讹传讹，充当传教士，这就是为什么图灵机现在被人们普遍接受作为计算模型。然而这并不能改变它丑陋和混淆的本质。图灵机的设计，其实是专门为了证明Hilbert的可判定性问题不可解决，它并不是一个用途广泛的计算模型。图灵机之所以被人接受，很大部分原因在于人的无知和愚蠢。很多人（包括很多所谓“理论计算机科学家”）根本没好好理解过lambda演算，他们望文生义，以为图灵机是“物理的”，实际可用的“机器”，而lambda演算只是一个理论模型。&lt;/p&gt;&lt;p&gt;事实恰恰相反：lambda演算其实非常的实用，它的本质跟电子线路没什么两样。几乎所有现实可用的程序语言，其中的语义全都可以用lambda演算来解释。而图灵机却没有很多现实的意义，用起来非常蹩脚，所以只能在计算理论中作为模型。另外一个更加鲜为人知的事实是：lambda演算其实在计算理论方面也可以完全取代图灵机，它不但可以表达所有图灵机能表达的理论，而且能够更加简洁和精确地表达它们。&lt;/p&gt;&lt;p&gt;很多理论计算机科学家喜欢用图灵机，仿佛是因为用它作为模型，能让自己的理论显得高深莫测，晦涩难懂。普通的计算理论课本，往往用图灵机作为它的计算模型，使用苦逼的办法推导各种可计算性（computability）和复杂性（complexity）理论。特别是像Michael Sipser那本经典的&lt;a href=&quot;http://www.amazon.com/Introduction-Theory-Computation-Michael-Sipser/dp/113318779X&quot;&gt;计算理论教材&lt;/a&gt;，晦涩难懂，混淆不堪，有时候让我都怀疑作者自己有没有搞懂那些东西。&lt;/p&gt;&lt;p&gt;后来我发现，其实图灵机所能表达的理论，全都可以用更加简单的lambda演算（或者任何一种现在流行的程序语言）来表示。图灵机的每一个状态，不过对应了lambda演算（或者某种程序语言）里面的一个“AST节点”，然而用lambda演算来表示那些计算理论，却可以比图灵机清晰和容易很多。在Indiana大学做计算理论课程助教的时候，我把这种思维方式悄悄地讲述给了上课的学生们，他们普遍表示我的这种思维方式更易理解，而且更加贴近实际的编程。&lt;/p&gt;&lt;p&gt;举一个很简单的例子。我可以用一行lambda演算表达式，来显示Hilbert的“可判定性问题”是无解的：&lt;/p&gt;&lt;p&gt;&lt;code&gt;Halting(λm.not(Halting(m,m)), λm.not(Halting(m,m)))&lt;/code&gt;&lt;/p&gt;&lt;p&gt;完整的证明不到一页纸，请看我的另外一篇&lt;a href=&quot;https://yinwang0.wordpress.com/2012/10/25/halting&quot;&gt;文章&lt;/a&gt;（英文）。这也就是图灵在他的&lt;a href=&quot;http://plms.oxfordjournals.org/content/s2-42/1/230.full.pdf+html&quot;&gt;论文&lt;/a&gt;里，折腾了十多页纸证明的东西。&lt;/p&gt;&lt;p&gt;我曾经以为自己是唯一知道这个秘密的人，直到有一天我把这个秘密告诉了我的博士导师，Amr Sabry。他对我说：“哈哈！其实我早就知道这个，你可以参考一下Neil Jones写的一本书，叫做《Computability and Complexity: From a Programming Perspective》。这本书现在已经可以&lt;a href=&quot;http://www.diku.dk/~neil/comp2book2007/book-whole.pdf&quot;&gt;免费下载&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;此书作者用一种很简单的程序语言，阐述了一般人用图灵机来描述的那些理论（可计算性理论，复杂性理论）。他发现用程序语言来描述计算理论，不但简单直接，清晰明了，而且在某些方面可以更加精确地描述图灵机无法描述的定理。得到这本书，让我觉得如获至宝，原来世界上有跟我看法如此相似，对事物洞察力如此之高的人！&lt;/p&gt;&lt;p&gt;在一次会议上，我有幸地遇到了Neil Jones，跟他切磋思想。当提到这本书的模型与图灵理论的关系，老教授谦虚地说：“图灵的模型还是有它的价值的……” 然而到最后，他其实也没能说清楚这价值何在。我心里很清楚，他只是为了避免引起宗教冲突，或者避免显得狂妄自大，而委婉其词。眼前的这位教授，虽然从来没有得过图灵奖，很少有人听说过他的名字，然而他对于计算本质的理解，却比图灵本人还要高出很多。&lt;/p&gt;&lt;p&gt;总的说来，图灵机也许不是一文不值，然而由于lambda演算可以更加清晰地解释图灵机能表示的所有理论，图灵机的价值相对来说几乎为零。Church在1937年给图灵论文写的&lt;a href=&quot;http://www.jstor.org/stable/2268810&quot;&gt;Review&lt;/a&gt;指出，图灵机的优势，在于它可以让不懂很多数学，不理解lambda演算之类理论的人也可以看得懂。我怎么觉得图灵机对于不懂很多数学的人，理解起来其实更加痛苦呢？而且就算它真的对“外行”或者“笨人”的理解有好处，这价值貌似也不大吧？:P&lt;/p&gt;&lt;h3&gt;电子计算机&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;http://upload-images.jianshu.io/upload_images/68562-994f2434c9330f8b.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;&quot;&gt;&lt;/p&gt;&lt;p&gt;很多“理论计算机科学家”喜欢说，大家现在用的计算机，只不过是一个“Universal Turing Machine”。就算你根本不知道图灵是谁，自己辛苦设计出一个机器或者语言，他们总喜欢说：“是图灵启发了你，因为你那东西是跟图灵机等价的，是图灵完备的……”&lt;/p&gt;&lt;p&gt;那么现在让我们来看看，图灵本人和他的理论，真正对电子计算机的发展起过多大的作用吧。如果一个人对一个行业起过重大的作用，那我们可以说“没有他不行”。然而事实却是，即使没有图灵，计算机技术会照样像今天一样发展，丝毫不会受到影响。看一看历史，你也许会惊讶的发现，图灵的理论不但没能启发任何计算机的设计，而且图灵亲自设计的唯一一个计算机（ACE），最后也以悲惨的失败告终。&lt;/p&gt;&lt;h4&gt;什么是Universal Turing Machine（UTM）&lt;/h4&gt;&lt;p&gt;ACE失败的一个重要原因，是因为图灵过度的看重他自己发明的Universal Turing Machine（UTM）。所以我想首先来解密一下，这个被很多人吹得神乎其神的，似乎什么都可以往上面扯的UTM，到底是什么东西。&lt;/p&gt;&lt;p&gt;说白了，UTM就是一个&lt;a href=&quot;http://www.yinwang.org/blog-cn/2012/08/01/interpreter&quot;&gt;解释器&lt;/a&gt;，就像Python或者JavaScript的解释器一样。计算机的处理器（CPU）也是一个解释器，它是用来解释机器指令的。那这样说来，任何可编程，具有指令集的机器都是UTM了，所以图灵的理论启发了所有这些机器？你尽管跟我扯吧 :)&lt;/p&gt;&lt;p&gt;你应该知道，在图灵的UTM出现以前，Church的lambda演算里面早就有&lt;a href=&quot;https://github.com/yinwang0/lightsabers/blob/master/meta-interp.ss&quot;&gt;解释器&lt;/a&gt;的概念了，所以UTM根本不是什么新东西，而且它比起lambda演算的解释器，真是丑陋又复杂。而Church其实也不是第一个提出解释器这概念的人，像这类通用的概念，已经很难追溯是谁“发明”的了。也许并不是某一个人发明了它，而是历史上的很多人。&lt;/p&gt;&lt;p&gt;解释器这个概念的涵义实在是包罗万象，几乎无处不在。只要是“可编程”的机器，它本质上必然包含一个解释器。一个工程师在不知道解释器这概念的情况下，照样很有可能“不小心”设计出一个可编程的机器，所以如果你把这些全都归结成图灵或者Church的功劳，就太牵强了。&lt;/p&gt;&lt;h4&gt;图灵与ACE的故事&lt;/h4&gt;&lt;p&gt;事实上，最早的电子计算机，并不是图灵设计的，而是电子工程师跟其他一些数学家合作的结果。根据老一辈工程师的&lt;a href=&quot;http://www.bbc.com/news/technology-18327261&quot;&gt;叙述&lt;/a&gt;，图灵的工作和理论，对于现实的电子计算机设计，几乎没有任何的正面作用。很多工程师其实根本不知道图灵是谁，图灵机是什么。他们只是根据实际的需求，设计和制造了那些电路。这就是为什么我们今天看到的电子计算机，跟图灵机或者图灵的其他理论几乎完全不搭边。&lt;/p&gt;&lt;p&gt;世界上最早的两台电子计算机，ENIAC和EDVAC，都是美国人设计制造的。其中EDVAC的设计报告，是冯诺依曼（von Neumann）参与并签署的。提到EDVAC的设计，《图灵传》有一段有趣的介绍，它基本是这样说的：“冯诺依曼在Princeton的时候，很了解图灵开天辟地的发明—UTM。UTM只有一根纸带，而EDVAC把指令和数据放在同一个存储空间，所以EDVAC的设计肯定是受了UTM的启发。然而EDVAC的设计报告，却只字不提图灵和UTM的名字，更没有引用图灵划时代的论文《Computable Numbers》……”&lt;/p&gt;&lt;p&gt;这其实是在含沙射影的说，冯诺依曼和EDVAC团队抄袭了图灵的研究成果。照这种歪理，我洗衣服的时候，袜子和内裤放在同一个桶里洗，也是受了图灵的启发了，就因为UTM只有一条纸带？这世界上的事物，还有啥不是受了UTM启发的？这让我想起某些全靠打专利官司赚钱的公司（&lt;a href=&quot;https://en.wikipedia.org/wiki/Patent_troll&quot;&gt;patent troll&lt;/a&gt;）…… 冯诺依曼作为一代数学大师，比UTM重大的研究成果多得是了，他会在乎抄袭图灵的东西吗？其实人家恐怕是根本没把图灵和他的论文当回事。而且其他人（比如Church）早就有跟UTM等价的想法，而且还更好，更简单。之前抢了Church的风头，现在居然欺到冯诺依曼头上来了。哎，真受不了这种一辈子只想出过一个点子的人……&lt;/p&gt;&lt;p&gt;所以听说美国人造出了EDVAC，图灵开始各种羡慕嫉妒恨，感叹自己英才无用武之地。终于有一天，他的机会来了。在EDVAC诞生几个月之后，英国国家物理实验室（NPL）联系了图灵。他们想赶上美国的计算机技术发展，所以想招募图灵，让他帮忙山寨一个EDVAC的“英国特色版本”。图灵设计的机器叫做&lt;a href=&quot;https://en.wikipedia.org/wiki/Automatic_Computing_Engine&quot;&gt;ACE&lt;/a&gt;（Automatic Computing Engine）。最初，图灵给NPL一个很宏伟的蓝图：ACE可以如此的强大，以至于整个英国只需要这样一台计算机就够了，我们可以把它叫做“英国国家计算机”…… 然而再大的口号，也难逃脱现实的检验，ACE项目最终以失败告终。&lt;/p&gt;&lt;p&gt;《图灵传》把ACE失败的责任，推托到NPL和其它人的“近视”和“官僚”，然而ACE失败的主要责任，其实在于图灵自己：他完全没有设计一台现实的计算机的基本技能，却总是自以为是，设立高大空的目标。图灵的设计跟当时（包括现在的）所有实用的计算机都有巨大的差别。不出你所料，他最初的设计思路，是根据自己之前的论文《Computable Numbers》里提到的“Universal Turing Machine”，不过从中去掉了一些不实际的设计，比如用一根纸带来存储数据。这一点改进貌似做对了，可是呢，他又加入了一些让工程师们无语的设计，美其名曰“极简设计”（minimalism）。比如，ACE的硬件只提供AND, OR, NOT之类的逻辑运算作为“基本操作”，其它的算数操作，包括加减乘除，全部用代码来实现。图灵大师啊，你知不知道有一种重要的指标，叫做“效率”？&lt;/p&gt;&lt;p&gt;这还不算…… 后来他更加异想天开，终于扯上了“思考机器”（thinking machines）—他想让ACE成为可以像人一样思考的机器，还想让这机器能够自己写自己的代码。按照图灵的原话：“在ACE的工作中，我对人脑建模的兴趣，比实际的计算应用更感兴趣。” 他显然已经把ACE当成了自己一个人的玩具，而不再是解决人们实际需求的工具。只要有人反对这想法，他就会嘲笑说，你是怕我的机器太聪明了，抢了你的饭碗吧？其实图灵对于实际的人脑工作原理所知甚少，基本处于初中生理卫生课本水平，然而他总喜欢对人说，人脑不过就是一个UTM。看吧，它有输入，输出，状态转换，就跟UTM一样…… 所谓“图灵测试”（Turing Test），就是那时候提出来的。当然了，因为他扯到了“thinking machine”，就有后人把他称为人工智能（AI）的鼻祖。其实呢，图灵测试根本就不能说明一个机器具有了人的智能，它只是在测试一些肤浅的表象。后来，“&lt;a href=&quot;https://en.wikipedia.org/wiki/Thinking_Machines_Corporation&quot;&gt;thinking machines&lt;/a&gt;”成为了一种通用的幌子，用于筹集大笔科研经费，最后全都血本无归。&lt;/p&gt;&lt;p&gt;图灵设计了这机器，NPL当时却没有能力制造它。于是他们求助于另外两位实现过计算机的工程师：&lt;a href=&quot;https://en.wikipedia.org/wiki/Frederic_Calland_Williams&quot;&gt;F. C. Williams&lt;/a&gt;和&lt;a href=&quot;https://en.wikipedia.org/wiki/Maurice_Wilkes&quot;&gt;Maurice Wilkes&lt;/a&gt;（后来EDSAC计算机的设计者），请他们帮忙实现图灵的设计。可想而知，Williams和Wilkes都表示不喜欢ACE的设计，而且指出图灵的性格与自己的研究风格不匹配，不愿跟他合作，所以双双拒绝了NPL的邀请。最后，NPL新成立了一个电子部门，ACE的工程终于可以开始。然而，根据资深工程师们的讨论，觉得图灵提出的制造一个“电子人脑”和“智能机器”，并不是实际可行，或者在短期之内能派上用场的项目，所以决定做一些实际点的事情。图灵对此非常恼火，各种抱怨，说别人官僚啊，近视啊，没想象力啊之类的，然后开始公开的抵制NPL的决定。&lt;/p&gt;&lt;p&gt;最后工程师们和管理层都受不了他了，鉴于他名声在外，又不好意思开掉他，只好提出一个破天荒的提议：由NPL资助，让图灵回到剑桥大学去度年假（sabbatical），做一些纯数学的研究。于是ACE在图灵不在的情况下，终于开工了⋯⋯1950年，ACE运行了它的第一个程序。然而工程师们实现的ACE，完全偏离了图灵的设计，以至于实际的机器和图灵的设计之间，几乎没有任何相似性。一年之后，图灵还想回到NPL，继续影响ACE的设计，然而NPL的领导们却建议他继续留在大学里做纯理论的研究，并且让曼彻斯特大学给他一个职位。最后图灵接受了这个建议，这下大家伙儿都松了一口气…… :P&lt;/p&gt;&lt;p&gt;图灵设计的唯一一个计算机ACE，终究以图灵完全退出整个项目而告终。今天回头看来，如果当时图灵留下来了，NPL真的按照图灵的意思来做，ACE恐怕直到今天都造不出来。由于图灵不切实际的设计和高傲的性格，NPL失去了最优秀的人的帮助。1949年，Maurice Wilkes按照EDVAC的思路，成功制造了&lt;a href=&quot;https://en.wikipedia.org/wiki/Electronic_Delay_Storage_Automatic_Calculator&quot;&gt;EDSAC&lt;/a&gt;，速度是ACE的两倍以上，而且更加实用。所以你看到了，图灵并不是一个实干家，他的双脚飘在半空中。他的理论，他设计的机器，他的代码，全都停留在纸上。他并没有帮助造出任何一台实际可用的计算机，他对计算机的工程实现几乎没有任何有益的影响。可惜的是，有些人喜欢把实干家们千辛万苦造出来，真正可以用的东西，牵强附会地归功于某些高谈阔论的理论家，仿佛那是理论家的功劳似的。这也许就是为什么图灵被他们称为“计算机之父”吧。&lt;/p&gt;&lt;p&gt;如果对ACE和其它早期计算机感兴趣，你可以参考一下更详细的&lt;a href=&quot;http://ed-thelen.org/comp-hist/EarlyBritish-05-12.html#Ch-05&quot;&gt;资料&lt;/a&gt;。你也可以看一看《图灵传》，虽然它观点荒唐，对图灵各种偏袒，然而图灵和其他人的通信，基本的史实，他应该不好意思篡改。&lt;/p&gt;&lt;h3&gt;总结&lt;/h3&gt;&lt;p&gt;我说这些是为了什么呢？我当然不是想否认图灵所做出的贡献。像许多的计算机工作者一样，他的某些工作当然是有意义的。然而那种意义并不像很多人所吹嘘的那么伟大，它们甚至不包含很多的创新。&lt;/p&gt;&lt;p&gt;我觉得很多后人给图灵带上的光环，掩盖了太多其它值得我们学习和尊敬的人，给人们对于计算机科学的概念造成了误导。计算机科学不是图灵一个人造出来的，图灵并不是计算机科学的鼻祖，他甚至不是在破解Enigma密码和电子计算机诞生过程中起最重要作用的人。&lt;/p&gt;&lt;p&gt;许许多多的计算机科学家和电子工程师们，是他们造就了今天的计算科学。他们的聪明才智和贡献，不应该被图灵的光环所掩盖，他们应该受到像跟图灵一样的尊敬。希望大家不要再神化图灵，不要再神化任何人。不要因为膜拜某些人，而失去向另一些人学习的机会。&lt;/p&gt;</description>
<author>yinwang0</author>
<guid isPermaLink="false">turing</guid>
<pubDate>Sun, 18 Oct 2015 00:00:00 +0800</pubDate>
</item>
<item>
<title>谈谈Parser</title>
<link>http://yinwang.org/blog-cn/2015/09/19/parser</link>
<description>&lt;p&gt;
  　　
  &lt;/p&gt;&lt;h2&gt;谈谈Parser&lt;/h2&gt;&lt;p&gt;一直很了解人们对于parser的误解，可是一直都提不起兴趣来阐述对它的观点。然而我觉得是有必要解释一下这个问题的时候了。我感觉得到大部分人对于parser的误解之深，再不澄清一下，恐怕这些谬误就要写进歪曲的历史教科书，到时候就没有人知道真相了。&lt;/p&gt;&lt;h3&gt;什么是Parser&lt;/h3&gt;&lt;p&gt;首先来科普一下。所谓parser，一般是指把某种格式的文本（字符串）转换成某种数据结构的过程。最常见的parser，是把程序文本转换成编译器内部的一种叫做“抽象语法树”（AST）的数据结构。也有简单一些的parser，用于处理CSV，JSON，XML之类的格式。&lt;/p&gt;&lt;p&gt;举个例子，一个处理算数表达式的parser，可以把“1+2”这样的，含有&lt;code&gt;1&lt;/code&gt;，&lt;code&gt;+&lt;/code&gt;，&lt;code&gt;2&lt;/code&gt;三个字符的字符串，转换成一个对象（object）。这个对象就像&lt;code&gt;new BinaryExpression(ADD, new Number(1), new Number(2))&lt;/code&gt;这样的Java构造函数调用生成出来的那样。&lt;/p&gt;&lt;p&gt;之所以需要做这种从字符串到数据结构的转换，是因为编译器是无法直接操作“1+2”这样的字符串的。实际上，代码的本质根本就不是字符串，它本来就是一个具有复杂拓扑的数据结构，就像电路一样。“1+2”这个字符串只是对这种数据结构的一种“编码”，就像ZIP或者JPEG只是对它们压缩的数据的编码一样。&lt;/p&gt;&lt;p&gt;这种编码可以方便你把代码存到磁盘上，方便你用文本编辑器来修改它们，然而你必须知道，文本并不是代码本身。所以从磁盘读取了文本之后，你必须先“解码”，才能方便地操作代码的数据结构。比如，如果上面的Java代码生成的AST节点叫&lt;code&gt;node&lt;/code&gt;，你就可以用&lt;code&gt;node.operator&lt;/code&gt;来访问&lt;code&gt;ADD&lt;/code&gt;，用&lt;code&gt;node.left&lt;/code&gt;来访问&lt;code&gt;1&lt;/code&gt;，&lt;code&gt;node.right&lt;/code&gt;来访问&lt;code&gt;2&lt;/code&gt;。这是很方便的。&lt;/p&gt;&lt;p&gt;对于程序语言，这种解码的动作就叫做parsing，用于解码的那段代码就叫做parser。&lt;/p&gt;&lt;h3&gt;Parser在编译器中的地位&lt;/h3&gt;&lt;p&gt;那么貌似这样说来，parser是编译器里面很关键的一个部分了？显然，parser是必不可少的，然而它并不像很多人想象的那么重要。Parser的重要性和技术难度，被很多人严重的夸大了。一些人提到“编译器”，就跟你提LEX，YACC，ANTLR等用于构造parser的工具，仿佛编译器跟parser是等价的似的。还有些人，只要听说别人写了个parser，就觉得这人编程水平很高，开始膜拜了。这些其实都显示出人的肤浅。&lt;/p&gt;&lt;p&gt;我喜欢把parser称为“万里长征的第0步”，因为等你parse完毕得到了AST，真正精华的编译技术才算开始。一个先进的编译器包含许多的步骤：语义分析，类型检查/推导，代码优化，机器代码生成，…… 这每个步骤都是在对某种中间数据结构（比如AST）进行分析或者转化，它们完全不需要知道代码的字符串形式。也就是说，一旦代码通过了parser，在后面的编译过程里，你就可以完全忘记parser的存在。所以parser对于编译器的地位，其实就像ZIP之于JVM，就像JPEG之于PhotoShop。Parser虽然必不可少，然而它比起编译器里面最重要的过程，是处于一种辅助性，工具性，次要的地位。&lt;/p&gt;&lt;p&gt;鉴于这个原因，好一点的大学里的程序语言（PL）课程，都完全没有关于parser的内容。学生们往往直接用Scheme这样代码数据同形的语言，或者直接使用AST数据结构来构造程序。在Kent Dybvig这样编译器大师的课程上，学生直接跳过parser的构造，开始学习最精华的语义转换和优化技术。实际上，Kent Dybvig根本不认为parser算是编译器的一部分。因为AST数据结构其实才是程序本身，而程序的文本只是这种数据结构的一种编码形式。&lt;/p&gt;&lt;h3&gt;Parser技术发展的误区&lt;/h3&gt;&lt;p&gt;既然parser在编译器中处于次要的地位，可是为什么还有人花那么大功夫研究各种炫酷的parser技术呢。LL，LR，GLR，LEX, YACC，Bison，parser combinator，ANTLR，PEG，…… 制造parser的工具似乎层出不穷，每出现一个新的工具都号称可以处理更加复杂的语法。&lt;/p&gt;&lt;p&gt;很多人盲目地设计复杂的语法，然后用越来越复杂的parser技术去parse它们，这就是parser技术仍然在发展的原因。其实，向往复杂的语法，是程序语言领域流传非常广，危害非常大的错误倾向。在人类历史的长河中，留下了许多难以磨灭的历史性糟粕，它们固化了人类对于语言设计的理念。很多人设计语言似乎不是为了拿来好用的，而是为了让用它的人迷惑或者害怕。&lt;/p&gt;&lt;p&gt;有些人假定了数学是美好的语言，所以他们盲目的希望程序语言看起来更加像数学。于是他们模仿数学，制造了各种奇怪的操作符，制定它们的优先级，这样你就可以写出&lt;code&gt;2 &amp;lt;&amp;lt; 7 - 2 * 3&lt;/code&gt;这样的代码，而不需要给子表达式加上括号。还有很多人喜欢让语法变得“简练”，就为了少打几个括号，分号，花括号，…… 可是由此带来的结果是复杂，不一致，有多义性，难扩展的语法，以及障眼难读，模棱两可的代码。&lt;/p&gt;&lt;p&gt;更有甚者，对数学的愚蠢做法执迷不悟的人，设计了像Haskell和Coq那样的语言。在Haskell里面，你可以在代码里定义新的操作符，指定它的“结合律”（associativity）和“优先级”（precedence）。这样的语法设计，要求parser必须能够在parse过程中途读入并且加入新的parse规则。Coq试图更加“强大”一些，它让你可以定义“mixfix操作符”，也就是说你的操作符可以连接超过两个表达式。这样你就可以定义像&lt;code&gt;if...then...else...&lt;/code&gt;这样的“操作符”。&lt;/p&gt;&lt;p&gt;制造这样复杂难懂的语法，其实没有什么真正的好处。不但给程序员的学习造成了不必要的困难，让代码难以理解，而且也给parser的作者带来了严重的挑战。可是有些人就是喜欢制造问题，就像一句玩笑话说的：有困难要上，没有困难，制造困难也要上！&lt;/p&gt;&lt;p&gt;如果你的语言语法很简单（像Scheme那样），你是不需要任何高深的parser理论的。说白了，你只需要知道如何parse匹配的括号。最多一个小时，几百行Java代码，我就能写出一个Scheme的parser。&lt;/p&gt;&lt;p&gt;可是很多人总是嫌问题不够有难度，于是他们不停地制造更加复杂的语法，甚至会故意让自己的语言看起来跟其它的不一样，以示“创新”。当然了，这样的语言就得用更加复杂的parser技术，这正好让那些喜欢折腾复杂parser技术的人洋洋得意。&lt;/p&gt;&lt;h3&gt;编译原理课程的误导&lt;/h3&gt;&lt;p&gt;程序员们对于parser的误解，很大程度上来自于大学编译原理课程照本宣科的教育。很多老师自己都不理解编译器的精髓，所以就只有按部就班的讲一些“死知识”，灌输“业界做法”。一般大学里上编译原理课，都是捧着一本大部头的“&lt;a href=&quot;http://en.wikipedia.org/wiki/Compilers:_Principles,_Techniques,_and_Tools&quot;&gt;龙书&lt;/a&gt;”或者“&lt;a href=&quot;https://www.cs.princeton.edu/~appel/modern&quot;&gt;虎书&lt;/a&gt;”，花掉一个学期1/3甚至2/3的时间来学写parser。由于parser占据了大量时间，以至于很多真正精华的内容都被一笔带过：语义分析，代码优化，类型推导，静态检查，机器代码生成，…… 以至于很多人上完了编译原理课程，记忆中只留下写parser的痛苦回忆。&lt;/p&gt;&lt;p&gt;“龙书”之类的教材在很多人心目中地位是如此之高，被誉为“经典”，然而其实除了开头很大篇幅来讲parser理论，这本书其它部分的水准其实相当低。大部分学生的反应其实是“看不懂”，然而由于一直以来没有更好的选择，它经典的地位真是难以动摇。“龙书”后来的新版我浏览过一下，新加入了类型检查/推导的部分，可是我看得出来，其实作者们自己对于类型理论都是一知半解，所以也就没法写清楚，让人可以看懂了。&lt;/p&gt;&lt;p&gt;龙书作者的水平，跟Dan Friedman，Kent Dybvig这样真正的大师比起来，其实差的老远。如果你想真的深入理解编译理论，最好是从PL课程的读物，比如&lt;a href=&quot;http://www.eopl3.com&quot;&gt;EOPL&lt;/a&gt;开始。我可以说PL这个领域，真的是高于编译器领域的。请不要指望编译器的作者能够轻易设计出好的语言，因为他们可能根本不理解很多语言设计的东西，他们只是会按部就班地实现某些别人设计的语言。可是反过来，理解了PL的理论，编译器的东西只不过是把一种语言转换成另外一种语言（机器语言）而已。工程的细枝末节很麻烦，可是当你掌握了精髓的原理，那些都容易摸索出来。&lt;/p&gt;&lt;h3&gt;我写parser的心得和秘诀&lt;/h3&gt;&lt;p&gt;虽然我已经告诉你，给过度复杂的语言写parser其实是很苦逼，没有意思的工作，然而有些历史性的错误已经造成了深远的影响，所以很多时候虽然心知肚明，你也不得不妥协一下。由于像C++，Java，JavaScript，Python之类语言的流行，有时候你是被迫要给它们写parser。在这一节，我告诉你一些秘诀，也许可以帮助你更加容易的写出这些语言的parser。&lt;/p&gt;&lt;p&gt;很多人都觉得写parser很难，一方面是由于语言设计的错误思想导致了复杂的语法，另外一方面是由于人们对于parser构造过程的思维误区。很多人不理解parser的本质和真正的用途，所以他们总是试图让parser干一些它们本来不应该干的事情，或者对parser有一些不切实际的标准。当然，他们就会觉得parser非常难写，非常容易出错。&lt;/p&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;尽量拿别人写的parser来用。维护一个parser是相当繁琐耗时，回报很低的事情。一旦语言有所改动，你的parser就得跟着改。所以如果你能找到免费的parser，那就最好不要自己写。现在的趋势是越来越多的语言在标准库里提供可以parse它自己的parser，比如Python和Ruby。这样你就可以用那语言写一小段代码调用标准的parser，然后把它转换成一种常用的数据交换格式，比如JSON。然后你就可以用通用的JSON parser解析出你想要的数据结构了。&lt;/p&gt;

&lt;p&gt;如果你直接使用别人的parser，最好不要使用它原来的数据结构。因为一旦parser的作者在新版本改变了他的数据结构，你所有的代码都会需要修改。我的秘诀是做一个“AST转换器”，先把别人的AST结构转换成自己的AST结构，然后在自己的AST结构之上写其它的代码，这样如果别人的parser修改了，你可以只改动AST转换器，其它的代码基本不需要修改。&lt;/p&gt;

&lt;p&gt;用别人的parser也会有一些小麻烦。比如Python之类语言自带的parser，丢掉了很多我需要的信息，比如函数名的位置，等等。我需要进行一些hack，找回我需要的数据。相对来说，这样小的修补还是比从头写一个parser要划得来。但是如果你实在找不到一个好的parser，那就只好自己写一个。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;很多人写parser，很在乎所谓的“one-pass parser”。他们试图扫描一遍代码文本就构造出最终的AST结构。可是其实如果你放松这个条件，允许用多pass的parser，就会容易很多。你可以在第一遍用很容易的办法构造一个粗略的树结构，然后再写一个递归树遍历过程，把某些在第一遍的时候没法确定的结构进行小规模的转换，最后得到正确的AST。&lt;/p&gt;

&lt;p&gt;想要一遍就parse出最终的AST，可以说是一种过早优化（premature optimization）。有些人盲目地认为只扫描一遍代码，会比扫描两遍要快一些。然而由于你必须在这一遍扫描里进行多度复杂的操作，最终的性能也许还不如很快的扫完第一遍，然后再很快的遍历转换由此生成的树结构。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;另外一些人试图在parse的过程中做一些本来不属于它做的事情，比如进行一些基本的语义检查。有些人会让parser检查“使用未定义的变量”等语义错误，一旦发现就在当时报错，终止。这种做法其实混淆了parser的作用，造成了不必要的复杂性。&lt;/p&gt;

&lt;p&gt;就像我说的，parser其实只是一个解码器。parser要做的事情，应该是从无结构的字符串里面，解码产生有结构的数据结构。而像“使用未定义的变量”这样的语义检查，应该是在生成了AST之后，使用单独的树遍历来进行的。人们常常混淆“解码”，“语法”和“语义”三者的不同，导致他们写出过度复杂，效率低下，难以维护的parser。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;另一种常见的误区是盲目的相信YACC，ANTLR之类所谓“parser generator”。实际上parser generator的概念看起来虽然美好，可是实际用起来几乎全都是噩梦。事实上最好的parser，比如EDG C++ parser，几乎全都是直接用普通的程序语言手写而成的，而不是自动生成的。&lt;/p&gt;

&lt;p&gt;这是因为parser generator都要求你使用某种特殊的描述语言来表示出语法，然后自动把它们转换成parser的程序代码。在这个转换过程中，这种特殊的描述语言和生成的parser代码之间，并没有很强的语义连接关系。如果生成的parser有bug，你很难从生成的parser代码回溯到语法描述，找到错误的位置和原因。你没法对语法描述进行debug，因为它只是一个文本文件，根本不能运行。&lt;/p&gt;

&lt;p&gt;所以如果你真的要写parser，我建议你直接用某种程序语言手写代码，使用普通的递归下降（recursive descent）写法，或者parser combinator的写法。只有手写的parser才可以方便的debug，而且可以输出清晰，人类可理解的出错信息。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;有些人喜欢死扣BNF范式，盲目的相信“LL”，“LR”等语法的区别，所以他们经常落入误区，说“哎呀，这个语法不是LL的”，于是采用一些像YACC那样的LR parser generator，结果落入非常大的麻烦。其实，虽然有些语法看起来不是LL的，它们的parser却仍然可以用普通的recursive descent的方式来写。&lt;/p&gt;

&lt;p&gt;这里的秘诀在于，语言规范里给出的BNF范式，其实并不是唯一的可以写出parser的做法。BNF只是一个基本的参照物，它让你可以对语法有个清晰的概念，可是实际的parser却不一定非得按照BNF的格式来写。有时候你可以把语法的格式稍微改一改，变通一下，却照样可以正确地parse原来的语言。其实由于很多语言的语法都类似于C，所以很多时候你写parser只需要看一些样例程序，然后根据自己的经验来写，而不需要依据BNF。&lt;/p&gt;

&lt;p&gt;Recursive descent和parser combinator写出来的parser其实可以非常强大，甚至可以超越所谓“上下文无关文法”，因为在递归函数里面你可以做几乎任意的事情，所以你甚至可以把上下文传递到递归函数里，然后根据上下文来决定对当前的节点做什么事情。而且由于代码可以得到很多的上下文信息，如果输入的代码有语法错误，你可以根据这些信息生成非常人性化的出错信息。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;h3&gt;总结&lt;/h3&gt;&lt;p&gt;所以你看到了，parser并不是编译器，它甚至不属于编译里很重要的东西。程序语言和编译器里面有比parser重要很多，有趣很多的东西。Parser的研究，其实是在解决一些根本不存在，或者人为制造的问题。复杂的语法导致了复杂的parser技术，它们仍然在给计算机世界带来不必要的困扰和麻烦。对parser写法的很多误解，过度工程和过早优化，造成了很多人错误的高估写parser的难度。&lt;/p&gt;&lt;p&gt;能写parser并不是什么了不起的事情，其实它是非常苦逼，真正的程序语言和编译器专家根本不屑于做的事情。所以如果你会写parser，请不要以为是什么了不起的事情，如果你看到有人写了某种语言的parser，也不要表现出让人哭笑不得的膜拜之情。&lt;/p&gt;</description>
<author>yinwang0</author>
<guid isPermaLink="false">parser</guid>
<pubDate>Sat, 19 Sep 2015 00:00:00 +0800</pubDate>
</item>
<item>
<title>数学和编程</title>
<link>http://yinwang.org/blog-cn/2015/07/04/math</link>
<description>&lt;p&gt;
  　　
  &lt;/p&gt;&lt;h2&gt;数学和编程&lt;/h2&gt;&lt;p&gt;好些人来信问我，要成为一个好的程序员，数学基础要达到什么样的程度？十八年前，当我成为大学计算机系新生的时候，也为同样的问题所困扰。面对学数学，物理等学科的同学，我感到自卑。经常有人说那些专业的知识更加精华一些，难度更高一些，那些专业的人毕业之后如果做编程工作，水平其实比计算机系毕业的还要高。直到几年前深入研究程序语言之后，对这个问题我才得到了答案和解脱。由于好多编程新手遇到同样的困扰，所以我想在这里把这个问题详细的阐述一下。&lt;/p&gt;&lt;h3&gt;数学并不是计算机科学的基础&lt;/h3&gt;&lt;p&gt;很多人都盲目的认为，计算机科学是数学的一个分支，数学是计算机科学的基础，数学是更加博大精深的科学。这些人以为只要学会了数学，编程的事情全都不在话下，然而事实却并非如此。&lt;/p&gt;&lt;p&gt;事实其实是这样的：&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;计算机科学其实根本不是数学，它只不过借用了非常少，非常基础的数学，比高中数学还要容易一点。所谓“高等数学”，在计算机科学里面基本用不上。&lt;/li&gt;
&lt;li&gt;计算机是比数学更加基础的工具，就像纸和笔一样。计算机可以用来解决数学的问题，也可以用来解决不是数学的问题，比如工程的问题，艺术的问题，经济的问题，社会的问题等等。&lt;/li&gt;
&lt;li&gt;计算机科学是完全独立的学科。学习了数学和物理，并不能代替对计算机科学的学习。你必须针对计算机科学进行学习，才有可能成为好的程序员。&lt;/li&gt;
&lt;li&gt;数学家所用的语言，比起常见的程序语言（比如C++，Java）来说，其实是非常落后而糟糕的设计。所谓“数学的美感”，其实大部分是夜郎自大。&lt;/li&gt;
&lt;li&gt;99%的数学家都写不出像样的代码。&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;数学是异常糟糕的语言&lt;/h3&gt;&lt;p&gt;这并不是危言耸听。如果你深入研究过程序语言的理论，就会发现其实数学家们使用的那些符号，只不过是一种非常糟糕的程序语言。数学的理论很多是有用的，然而数学家门用于描述这些理论所用的语言，却是纷繁复杂，缺乏一致性，可组合性（composability），简单性，可用性。这也就是为什么大部分人看到数学就头痛。这不是他们不够聪明，而是数学语言的“&lt;a href=&quot;http://www.yinwang.org/blog-cn/2015/03/17/design&quot;&gt;设计&lt;/a&gt;”有问题。人们学习数学的时候，其实只有少部分时间在思考它的精髓，而大部分时间是在折腾它的语法。&lt;/p&gt;&lt;p&gt;举一个非常简单的例子。如果你说cos&lt;sup&gt;2&lt;/sup&gt;θ表示(cos θ)&lt;sup&gt;2&lt;/sup&gt;，那么理所当然，cos&lt;sup&gt;-1&lt;/sup&gt;θ就应该表示1/(cos θ)了？可它偏偏不是！别被数学老师们的教条和借口欺骗啦，他们总是告诉你：“你应该记住这些！” 可是你想过吗：“凭什么？” cos&lt;sup&gt;2&lt;/sup&gt;θ表示(cos θ)&lt;sup&gt;2&lt;/sup&gt;，而cos&lt;sup&gt;-1&lt;/sup&gt;θ，明明是一模一样的形式，表示的却是arccos θ。一个是求幂，一个是调用反函数，风马不及，却写成一个样子。这样的语言设计混淆不堪，却喜欢以“约定俗成”作为借口。&lt;/p&gt;&lt;p&gt;如果你再多看一些数学书，就会发现这只是数学语言几百年累积下来的糟粕的冰山一角。数学书里尽是各种上标下标，带括号的上标下标，x，y，z，a，b，c，f，g，h，各种扭来扭去的希腊字母，希伯来字母…… 斜体，黑体，花体，双影体，……用不同的字体来表示不同的“类型”。很多符号的含义，在不同的子领域里面都不一样。有些人上一门数学课，到最后还没明白那些符号是什么意思。&lt;/p&gt;&lt;p&gt;直到今天，数学家们写书仍然非常不严谨。他们常犯的一个错误是把x&lt;sup&gt;2&lt;/sup&gt;这样的东西叫做“函数”（function）。其实x&lt;sup&gt;2&lt;/sup&gt;根本不是一个函数，它只是一个表达式。你必须同时指明“x是参数”，加上x&lt;sup&gt;2&lt;/sup&gt;，才会成为一个函数。所以正确的函数写法其实看起来像这样：f(x) = x&lt;sup&gt;2&lt;/sup&gt;。或者如果你不想给它一个名字，可以借用lambda calculus的写法，写成：λx.x&lt;sup&gt;2&lt;/sup&gt;。&lt;/p&gt;&lt;p&gt;可是数学家们灰常的喜欢“约定俗成”。他们定了一些不成文的规矩是这样：凡是叫“x”的，都是函数的参数，凡是叫“y”的，都可能是一个函数…… 所以你写x&lt;sup&gt;2&lt;/sup&gt;就可以表示λx.x&lt;sup&gt;2&lt;/sup&gt;，而不需要显式的写出“λx”。殊不知这些约定俗成，看起来貌似可以让你少写几个字，却造成了许许多多的混淆和麻烦。比如，你在Mathematica里面可以对 &lt;a href=&quot;http://www.wolframalpha.com/input/?i=D%5Bx%5E2%2By%2Cx%5D&quot;&gt;x&lt;sup&gt;2&lt;/sup&gt;+y&lt;/a&gt; 求关于&lt;code&gt;x&lt;/code&gt;的导数，而且会得到 &lt;code&gt;y&#39;(x) + 2x&lt;/code&gt; 这样蹊跷的结果，因为它认为&lt;code&gt;y&lt;/code&gt;可能是一个函数。更奇怪的是，如果你在后面多加一个&lt;code&gt;a&lt;/code&gt;，也就是对&lt;a href=&quot;http://www.wolframalpha.com/input/?i=D%5Bx%5E2%2By%2Ba%2Cx%5D&quot;&gt;x&lt;sup&gt;2&lt;/sup&gt;+y+a&lt;/a&gt;求导，你会得到 &lt;code&gt;2x&lt;/code&gt;！那么 &lt;code&gt;y&#39;(x)&lt;/code&gt; 到哪里去了？莫名其妙……&lt;/p&gt;&lt;p&gt;相对而言，程序语言就严谨很多，所有的程序语言都要求你必须指出函数的参数叫什么名字。像x&lt;sup&gt;2&lt;/sup&gt;这样的东西，在程序语言里面不是一个函数（function），而只是一个表达式（expression）。即使 JavaScript 这样毛病众多的语言都是这样。比如，你必须写：&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;javascript&quot;&gt;function (x) { return x * x }
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;那个括号里的&lt;code&gt;(x)&lt;/code&gt;，显式的声明了变量的名字，避免了可能出现的混淆。我不是第一个指出这些问题的人。其实现代逻辑学的鼻祖Gottlob Frege在一百多年以前就在他的论文“&lt;a href=&quot;http://www.olimon.org/uan/frege-writings.pdf&quot;&gt;Function and Concept&lt;/a&gt;”里批评了数学家们的这种做法。可是数学界的表达方式直到今天还是一样的混乱。&lt;/p&gt;&lt;p&gt;很多人学习微积分都觉得困难，其实问题不在他们，而在于莱布尼兹（Leibniz）。莱布尼兹设计来描述微积分的语言（∫，dx, dy, ...），从现代语言设计的角度来看，其实非常之糟糕，可以说是一塌糊涂。我不能怪莱布尼兹，他毕竟是几百年前的人了，他不知道我们现在知道的很多东西。然而古人的设计，现在还不考虑改进，反而当成教条灌输给学生，那就是不思进取了。&lt;/p&gt;&lt;p&gt;数学的语言不像程序语言，它的历史太久，没有经过系统的，考虑周全的，统一的设计。各种数学符号的出现，往往是历史上某个数学家有天在黑板上随手画出一些古怪的符号，说这代表什么，那代表什么，…… 然后就定下来了。很多数学家只关心自己那块狭窄的子领域，为自己的理论随便设计出一套符号，完全不管这些是否跟其它子领域的符号相冲突。这就是为什么不同的数学子领域里写出同样的符号，却可以表示完全不同的涵义。在这种意义上，数学的语言跟Perl（一种非常糟糕的程序语言）有些类似。Perl把各种人需要的各种功能，不加选择地加进了语言里面，造成语言繁复不堪，甚至连Perl的创造者自己都不能理解它所有的功能。&lt;/p&gt;&lt;p&gt;数学的证明，使用的其实也是极其不严格的语言——古怪的符号，加上含糊不清，容易误解的人类语言。如果你知道什么是&lt;a href=&quot;https://en.wikipedia.org/wiki/Curry%E2%80%93Howard_correspondence&quot;&gt;Curry-Howard Correspondence&lt;/a&gt;就会明白，其实每一个数学证明都不过是一段代码。同样的定理，可以有许多不同版本的证明（代码）。这些证明有的简短优雅，有的却冗长繁复，像面条一样绕来绕去，没法看懂。你经常在数学证明里面看到“未定义的变量”，证明的逻辑也包含着各种隐含知识，思维跳跃，非常难以理解。很多数学证明，从程序的观点来看，连编译都不会通过，就别提运行了。&lt;/p&gt;&lt;p&gt;数学家们往往不在乎证明的优雅性。他们认为只要能证明出定理，你管我的证明简不简单，容不容易看懂呢。你越是看不懂，就越是觉得我高深莫测，越是感觉你自己笨！这种思潮到了编程的时候就显出弊端了。数学家写代码，往往忽视代码的优雅性，简单性，模块化，可读性，性能，数据结构等重要因素，认为代码只要能算出结果就行。他们把代码当成跟证明一样，一次性的东西，所以他们的代码往往不能满足实际工程的严格要求。&lt;/p&gt;&lt;p&gt;数学里最在乎语言设计的分支，莫过于逻辑学了。很多人（包括很多程序语言专家）都盲目的崇拜逻辑学家，盲目的相信数理逻辑是优雅美好的语言。在程序语言界，数理逻辑已经成为一种灾害，明明很容易就能解释清楚的语义，非得写成一堆稀奇古怪，含义混淆的逻辑公式。殊不知其实数理逻辑也是有很大的历史遗留问题和误区的。研究逻辑学的人经常遇到各种“不可判定”（undecidable）问题和所谓“悖论”（paradox），研究几十年也没搞清楚，而其实那些问题都是他们自己造出来的。你只需要把语言改一下，去掉一些不必要的功能，问题就没了。但逻辑学家们总喜欢跟你说，那是某天才老祖宗想出来的，多么多么的了不起啊，不能改！&lt;/p&gt;&lt;p&gt;用一阶逻辑（first-order logic）这样的东西，你可以写出一些毫无意义的语句。逻辑老师们会告诉你，记住啦，这些是没有意义的，如果写出来这些东西，是你的问题！他们没有意识到，如果一个人可以用一个语言写出毫无意义的东西，那么这问题在于这个语言，而不在于这个人。一阶逻辑号称可以“表达所有数学”，结果事实却是，没有几个数学家真的可以用它表达很有用的知识。到后来，稍微明智一点的逻辑学家们开始研究这些老古董语言到底出了什么毛病，于是他们创造了Model Theory这样的理论。写出一些长篇大部头，用于“验证”这些逻辑语言的合理性。这些问题在我看来都是显而易见的，因为很多逻辑的语言根本就不是很好很有用的东西。去研究它们“为什么有毛病”，其实是白费力气。自己另外设计一个更好语言就完事了。&lt;/p&gt;&lt;p&gt;在我看来，除了现代逻辑学的鼻祖&lt;a href=&quot;https://en.wikipedia.org/wiki/Gottlob_Frege&quot;&gt;Gottlob Frege&lt;/a&gt;理解了逻辑的精髓，其它逻辑学家基本都是照本宣科，一知半解。他们喜欢把简单的问题搞复杂，制造一些新名词，说得玄乎其玄灵丹妙药似的。如果你想了解逻辑学的精华，建议你看看&lt;a href=&quot;http://www.olimon.org/uan/frege-writings.pdf&quot;&gt;Frege的文集&lt;/a&gt;。看了之后你也许会发现，Frege思想的精华，其实已经融入在几乎所有的程序语言里了。&lt;/p&gt;&lt;h3&gt;编程是一门艺术&lt;/h3&gt;&lt;p&gt;从上面你也许已经明白了，普通程序员使用的编程语言，就算是C++这样毛病众多的语言，其实也已经比数学家使用的语言好很多。用数学的语言可以写出含糊复杂的证明，在期刊或者学术会议上蒙混过关，用程序语言写出来的代码却无法混过计算机这道严格的关卡。因为计算机不是人，它不会迷迷糊糊的点点头让你混过去，或者因为你是大师就不懂装懂。代码是需要经过现实的检验的。如果你的代码有问题，它迟早会导致出问题。&lt;/p&gt;&lt;p&gt;计算机科学并不是数学的一个分支，它在很大程度上是优于数学，高于数学的。有些数学的基本理论可以被计算机科学所用，然而计算机科学并不是数学的一部分。数学在语言方面带有太多的历史遗留糟粕，它其实是泥菩萨过河，自身难保，它根本解决不了编程中遇到的实际问题。&lt;/p&gt;&lt;p&gt;编程真的是一门艺术，因为它符合艺术的各种特征。艺术可以利用科学提供的工具，然而它却不是科学的一部分，它的地位也并不低于科学。和所有的艺术一样，编程能解决科学没法解决的问题，满足人们新的需求，开拓新的世界。所以亲爱的程序员们，别再为自己不懂很多数学而烦恼了。数学并不能帮助你写出好的程序，然而能写出好程序的人，却能更好的理解数学。我建议你们先学编程，再去看数学。&lt;/p&gt;&lt;p&gt;如果你想了解更多关于数学语言的弊病以及程序语言对它们的改进，我建议你看看这个Gerald Susman的&lt;a href=&quot;http://www.infoq.com/presentations/Expression-of-Ideas&quot;&gt;讲座&lt;/a&gt;。&lt;/p&gt;</description>
<author>yinwang0</author>
<guid isPermaLink="false">math</guid>
<pubDate>Sat, 04 Jul 2015 00:00:00 +0800</pubDate>
</item>
<item>
<title>谈程序的正确性</title>
<link>http://yinwang.org/blog-cn/2015/07/02/program-correctness</link>
<description>&lt;p&gt;
  　　
  &lt;/p&gt;&lt;h2&gt;谈程序的正确性&lt;/h2&gt;&lt;p&gt;不管在学术圈还是在工业界，总有很多人过度的关心所谓“程序的正确性”，有些甚至到了战战兢兢，舍本逐末的地步。下面举几个例子：&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;很多人把测试（test）看得过于重要。代码八字还没一撇呢，就吵着要怎么怎么严格的测试，防止“将来”有人把代码改错了。这些人到后来往往被测试捆住了手脚，寸步难行。不但代码bug百出，连测试里面也很多bug。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;有些人对于“使用什么语言”这个问题过度的在乎，仿佛只有用最新最酷，功能最多的语言，他们才能完成一些很基本的任务。这种人一次又一次的视一些新语言为“灵丹妙药”，然后一次又一次的幻灭，最后他们什么有用的代码也没写出来。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;有些人过度的重视所谓“类型安全”（type safety），经常抱怨手头的语言缺少一些炫酷的类型系统功能，甚至因此说没法写代码了！他们没有看到，即使缺少一些由编译器静态保障的类型安全，代码其实一点问题都没有，而且也许更加简单。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;有些人走上极端，认为所有的代码都必须使用所谓“形式化方法”（formal methods），用机器定理证明的方式来确保它100%的没有错误。这种人对于证明玩具大小的代码乐此不疲，结果一辈子也没写出过能解决实际问题的代码。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;100%可靠的代码，这是多么完美的理想！可是到最后你发现，天天念叨着要“正确性”，“可靠性”的人，几乎总是眼高手低，说的比做的多。自己没写出什么解决实际问题的代码，倒是很喜欢对别人的“代码质量”评头论足。这些人自己的代码往往复杂不堪，喜欢使用各种看似高深的奇技淫巧，用以保证所谓“正确”。他们的代码被很多所谓“测试工具”和“类型系统”捆住手脚，却仍然bug百出。到后来你逐渐发现，对“正确性”的战战兢兢，其实是这些人不解决手头问题的借口。&lt;/p&gt;&lt;h3&gt;衡量程序最重要的标准&lt;/h3&gt;&lt;p&gt;这些人其实不明白一个重要的道理：你得先写出程序，才能开始谈它的正确性。看一个程序好不好，最重要的标准，是看它能否有效地解决问题，而不是它是否正确。如果你的程序没有解决问题，或者解决了错误的问题，或者虽然解决问题但却非常难用，那么这程序再怎么正确，再怎么可靠，都不是好的程序。&lt;/p&gt;&lt;p&gt;正确不等于简单，不等于优雅，不等于高效。一个不简单，不优雅，效率低的程序，就算你费尽周折证明了它的正确，它仍然不会很好的工作。这就像你得先有了房子，才能开始要求房子是安全的。想想吧，如果一个没有房子的流浪汉，路过一座没有人住的房子，他会因为这房子“不是100%安全”，而继续在野外风餐露宿吗？写出代码就像有了房子，而代码的正确性，就像房子的安全性。写出可以解决问题的程序，永远是第一位的。而这个程序的正确性，不管它如何的重要，永远是第二位的。对程序的正确性的强调，永远不应该高于写出程序本身。&lt;/p&gt;&lt;p&gt;每当谈起这个问题，我就喜欢打一个比方：如果“黎曼猜想”被王垠证明出来了，它会改名叫“王垠定理”吗？当然不会。它会被叫做“黎曼定理”！这是因为，无论一个人多么聪明多么厉害，就算他能够证明出黎曼猜想，但这个猜想并不是他最先想出来的。如果黎曼没有提出这个猜想，你根本不会想到它，又何谈证明呢？所以我喜欢说，一流的数学家提出猜想，二流的数学家证明别人的猜想。同样的道理，写出解决问题的代码的人，比起那些去证明（测试）他的代码正确性的人，永远是更重要的。因为如果他没写出这段代码，你连要证明（测试）什么都不知道！&lt;/p&gt;&lt;h3&gt;如何提高程序的正确性&lt;/h3&gt;&lt;p&gt;话说回来，虽然程序的正确性相对于解决问题，处于相对次要的地位，然而它确实是不可忽视的。但这并不等于天天鼓吹要“测试”，要“形式化证明”，就可以提高程序的正确性。&lt;/p&gt;&lt;p&gt;如果你深入研究过程序的逻辑推导就会知道，测试和形式化证明的能力都是非常有限的。测试只能测试到最常用的情况，而无法覆盖所有的情况。别被所谓“测试覆盖”（test coverage）给欺骗了。一行代码被测试覆盖而没有出错，并不等于在那里不会出错。一行代码是否出错，取决于在它运行之前所经过的所有条件。这些条件的数量是组合爆炸关系，基本上没有测试能够覆盖所有这些前提条件。&lt;/p&gt;&lt;p&gt;形式化方法对于非常简单直接的程序是有效的，然而一旦程序稍微大点，形式化方法就寸步难行。你也许没有想到，你可以用非常少的代码，写出&lt;a href=&quot;https://en.wikipedia.org/wiki/Collatz_conjecture&quot;&gt;Collatz Conjecture&lt;/a&gt;这样至今没人证明出来的数学猜想。实际使用中的代码，比这种数学猜想要复杂不知道多少倍。你要用形式化方法去证明所有的代码，基本上等于你永远也没法完成项目。&lt;/p&gt;&lt;p&gt;那么提高程序正确性最有效的方法是什么呢？在我看来，最有效的方法莫过于对代码反复琢磨推敲，让它变得简单，直观，直到你一眼就可以看得出它不可能有问题。&lt;/p&gt;</description>
<author>yinwang0</author>
<guid isPermaLink="false">program-correctness</guid>
<pubDate>Thu, 02 Jul 2015 00:00:00 +0800</pubDate>
</item>
<item>
<title>DRY原则的误区</title>
<link>http://yinwang.org/blog-cn/2015/06/14/dry-principle</link>
<description>&lt;p&gt;
  　　
  &lt;/p&gt;&lt;h2&gt;DRY原则的误区&lt;/h2&gt;&lt;p&gt;很多编程的人，喜欢鼓吹各种各样的“原则”，比如KISS原则，DRY原则…… 总有人把这些所谓原则奉为教条或者秘方，以为兢兢业业地遵循这些，空喊几个口号，就可以写出好的代码。同时，他们对违反这些原则的人嗤之以鼻——你不知道，不遵循或者藐视这些原则，那么你就是菜鸟。所谓“&lt;a href=&quot;http://en.wikipedia.org/wiki/Don%27t_repeat_yourself&quot;&gt;DRY原则&lt;/a&gt;”（Don&#39;t Repeat Yourself，不要重复你自己）就是这些教条其中之一。盲目的迷信DRY原则，在实际的工程中带来了各种各样的问题，却经常被忽视。&lt;/p&gt;&lt;p&gt;简言之，DRY原则鼓励对代码进行抽象，但是鼓励得过了头。DRY原则说，如果你发现重复的代码，就把它们提取出去做成一个“模板”或者“框架”。对于抽象我非常的在行，实际上程序语言专家做的许多研究，就是如何设计更好的抽象。然而我并不奉行所谓DRY原则，并不是尽一切可能避免“重复”。“避免重复”并不等于“抽象”。有时候适当的重复代码是有好处的，所以我有时候会故意的进行重复。&lt;/p&gt;&lt;h3&gt;抽象与可读性的矛盾&lt;/h3&gt;&lt;p&gt;代码的“抽象”和它的“可读性”（直观性），其实是一对矛盾的关系。适度的抽象和避免重复是有好处的，它甚至可以提高代码的可读性，然而如果你尽“一切可能”从代码里提取模板，甚至把一些微不足道的“共同点”也提出来进行“共享”，它就开始有害了。这是因为，模板并不直接显示在“调用”它们的位置。提取出模板，往往会使得阅读代码时不能一目了然。如果由此带来的直观性损失超过了模板所带来的好处时，你就应该考虑避免抽象了。要知道，代码读的次数要比写的次数多很多。很多人为了一时的“写的快感”，过早的提取出不必要的模板，其实损失了读代码时的直观性。如果自己的代码连自己都不能一目了然，你就不能写出优雅的代码。&lt;/p&gt;&lt;p&gt;举一个实际的例子。奉行DRY原则的人，往往喜欢提取类里面的“共同field”，把它们放进一个父类，然后让原来的类继承这个父类。比如，本来的代码可能是：&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;java&quot;&gt;class A {
  int a;
  int x;
  int y;
}

class B {
  int a;
  int u;
  int v;
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;奉行DRY原则的人喜欢把它改成这样：&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;java&quot;&gt;class C {
  int a;
}

class A extends C {
  int x;
  int y;
}

class B extends C {
  int u;
  int v;
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;后面这段代码有什么害处呢？它的问题是，当你看到&lt;code&gt;class A&lt;/code&gt;和&lt;code&gt;class B&lt;/code&gt;的定义时，你不再能一目了然的看到&lt;code&gt;int a&lt;/code&gt;这个field。“可见性”，对于程序员能够产生直觉，是非常重要的。这种无关紧要的field，其实大部分时候都没必要提出去，造出一个新的父类。很多时候，不同类里面虽然有同样的&lt;code&gt;int a&lt;/code&gt;这样的field，然而它们的含义却是完全不同的。有些人不管三七二十一就来个“DRY”，结果不但没带来好处，反而让程序难以理解。&lt;/p&gt;&lt;h3&gt;抽象的时机问题&lt;/h3&gt;&lt;p&gt;奉行DRY原则的人还有一个问题，就是他们随时都在试图发现“将来可能重用”的代码，而不是等到真的出现重复的时候再去做抽象。很多时候他们提取出一个貌似“经典模板”，结果最后过了几个月发现，这个模板在所有代码里其实只用过一次。这就是因为他们过早的进行了抽象。&lt;/p&gt;&lt;p&gt;抽象的思想，关键在于“发现两个东西是一样的”。然而很多时候，你开头觉得两个东西是一回事，结果最后发现，它们其实只是肤浅的相似，而本质完全不同。同一个&lt;code&gt;int a&lt;/code&gt;，其实可以表示很多种风马牛不及的性质。你看到都是&lt;code&gt;int a&lt;/code&gt;就提出来做个父类，其实反而让程序的概念变得混乱。还有的时候，有些东西开头貌似同类，后来你增添了新的逻辑之后，发现它们的用途开始特殊化，后来就分道扬镳了。过早的提取模板，反而捆住了你的手脚，使得你为了所谓“一致性”而重复一些没用的东西。这样的一致性，其实还不如针对每种情况分别做特殊处理。&lt;/p&gt;&lt;p&gt;防止过早抽象的方法其实很简单，它的名字叫做“等待”。其实就算你不重用代码，真的不会死人的。时间能够告诉你一切。如果你发现自己仿佛正在重复以前写过代码，请先不要停下来，请坚持把这段重复的代码写完。如果你不把它写出来，你是不可能准确的发现重复的代码的，因为它们很有可能到最后其实是不一样的。&lt;/p&gt;&lt;p&gt;你还应该避免没有实际效果的抽象。如果代码才重复了两次，你就开始提取模板，也许到最后你会发现，这个模板总共也就只用了两次！只重复了两次的代码，大部分时候是不值得为它提取模板的。因为模板本身也是代码，而且抽象思考本身是需要一定代价的。所以最后总的开销，也许还不如就让那两段重复的代码待在里面。&lt;/p&gt;&lt;p&gt;这就是为什么我喜欢一种懒懒的，笨笨的感觉。因为我懒，所以我不会过早的思考代码的重用。我会等到事实证明重用一定会带来好处的时候，才会开始提取模板，进行抽象。经验告诉我，每一次积极地寻找抽象，最后的结果都是制造一些不必要的模板，搞得自己的代码自己都看不懂。很多人过度强调DRY，强调代码的“重用”，随时随地想着抽象，结果被这些抽象搅混了头脑，bug百出，寸步难行。如果你不能写出“可用”（usable）的代码，又何谈“可重用”（reusable）的代码呢？&lt;/p&gt;&lt;h3&gt;谨慎的对待所谓原则&lt;/h3&gt;&lt;p&gt;说了这么多，我是在支持DRY，还是反对DRY呢？其实不管是支持还是反对它，都会表示我在乎它，而其实呢，我完全不在乎这类原则，因为它们非常的肤浅。这就像你告诉我说你有一个重大的发现，那就是“1+1=2”，我该支持你还是反对你呢？我才懒得跟你说话。人们写程序，本来自然而然就会在合适的时候进行抽象，避免重复，怎么过了几十年后，某个菜鸟给我们的做法起了个名字叫DRY，反而他成了“大师”一样的人物，我倒要用“DRY”这个词来描述我一直在干的事情呢？所以我根本不愿意提起“DRY”这个名字。&lt;/p&gt;&lt;p&gt;所以我觉得这个DRY原则根本就不应该存在，它是一个根本没有资格提出“原则”的人提出来的。看看他鼓吹的其它低劣东西（比如Agile，Ruby），你就会发现，他是一个兜售减肥药的“软件工程专家”。世界上有太多这样的肤浅的所谓原则，我不想对它们一一进行评价，这是在浪费我的时间。世界上有比这些喜欢提出“原则”的软件工程专家深邃很多的人，他们懂得真正根本的原理。&lt;/p&gt;</description>
<author>yinwang0</author>
<guid isPermaLink="false">dry-principle</guid>
<pubDate>Sun, 14 Jun 2015 00:00:00 +0800</pubDate>
</item>
<item>
<title>所谓软件工程</title>
<link>http://yinwang.org/blog-cn/2015/06/07/software-engineering</link>
<description>&lt;p&gt;
  　　
  &lt;/p&gt;&lt;h2&gt;所谓软件工程&lt;/h2&gt;&lt;p&gt;很多编程的人包括我，头衔叫做“软件工程师”（software engineer），然而我却不喜欢这个名字。我喜欢把自己叫做“程序员”（programmer）或者“计算机科学家”（computer scientist）。这是为什么呢？这需要从“软件工程”（software engineering）在现实中的涵义谈起。&lt;/p&gt;&lt;p&gt;有人把软件工程这个领域的本质总结为：“How to program if you cannot？”（如果你不会编程，那么你如何编程？）我觉得这句话说得很好，因为我发现软件工程这整个领域，基本就是吹牛扯淡卖“减肥药”的。软件行业的大部分莫名其妙的愚昧行为，很多是由所谓“软件工程专家”发明的。总有人提出一套套的所谓“方法论”或者“原则”，比如Extreme Programming，Design Patterns，Agile，Pair Programming，Test Driven Development（TDD），DRY principle，…… 他们把这些所谓方法论兜售给各个软件公司，鼓吹它们的各种好处，说使用这些方法，就可以用一些平庸的“软件工程师”，制造出高质量低成本的软件。这就跟减肥药的广告一样：不用运动，不用节食，一个星期瘦20斤。你开头还不以为然，觉得这些肤浅的说法能造成什么影响。结果久而久之，这些所谓“方法论”和“原则”成为了整个行业的教条，造成了文化大革命一样的风气。违反这些教条的人，必然被当成菜鸟一样的鄙视，当成小学生一样的教育，当成“反革命”一样的批斗。就算你技术比这些教条的提出者还高明不知道多少倍也一样。&lt;/p&gt;&lt;p&gt;打破这些软件工程专家们制造的幻觉的一个办法，就是实地去看看这些所谓专家们自己用这些方法论做出了什么好东西。你会惊奇的发现，这些提出各种玄乎其玄的新名词的所谓“专家”，几乎都是从不知道什么旮旯里冒出来的民科，没有一个做出过什么有技术含量的东西，他们根本没有资格对别人编程的方式做出指导。这些人做出来少数有点用的东西（比如JUnit），其实非常容易，以至于每个初学编程的人都应该做得出来。可世界上就是有这样划算的职业，你虽然写不出好的代码，你对计算原理的理解非常肤浅，却可以通过一些手段，得到评价别人的“代码质量”的权力，占据软件公司的管理层位置。久而久之，别人还以为你是什么泰斗。你仔细看过提出Java Design Pattern的四个人（GoF），到底做出过什么厉害的东西吗？没有。提出“DRY Principle”的作者，做出过什么好东西吗？没有。再看看Agile，Pair Programming，TDD……的提出者？全都是一群饭桶。他们其实根本就不懂很多编程的东西，写出文章和书来也是极其肤浅，一知半解。&lt;/p&gt;&lt;p&gt;所谓“软件工程”，并不像土木工程，机械工程，电机工程，是建立在实际的，科学的基础上的。跟这些“硬工程”不一样，软件弄得不好不会出人命，也不会跟做芯片的公司那样，出一个bug立即导致上亿的损失，身败名裂。所以研究软件工程，似乎特别容易钻空子，失败了之后容易找借口和替罪羊。如果你说我的方法不好，你有什么证据吗？口说无凭，我浪费了你多少时间呢？你的具体执行是不是完全照我说的来的呢？你肯定有什么细节没按我说的做，所以才会失败。总之，如果你用了我的办法不管用，那是你自己的问题！&lt;/p&gt;&lt;p&gt;想起这些借口我就想起一个笑话：两夫妻睡觉发现床上有跳蚤，身上被咬了好多大包。去买了号称“杀伤率100%”的跳蚤药，撒了好多在床上。第二天早上起来，发现又被咬了好多新的大包。妻子责怪丈夫，说他没看说明书就乱撒。结果丈夫打开说明书一看，内容如下：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;本跳蚤药使用方法：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;抓住跳蚤&lt;/li&gt;
&lt;li&gt;掰开跳蚤的嘴&lt;/li&gt;
&lt;li&gt;把药塞进跳蚤嘴里&lt;/li&gt;
&lt;li&gt;合上跳蚤的嘴&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;&lt;p&gt;我发现很多软件工程的所谓方法论失败之后的借口，跟这跳蚤药的说明书很像 :)&lt;/p&gt;&lt;p&gt;人都想省钱，雇用高质量的程序员不容易呀，所以很多公司还是上钩了。他们请这些“软件工程专家”来到公司，推行各种各样的软件方法论，可是发现最后都失败了。这是为什么呢？因为再高明的方法论，也无法代替真正的，精华的计算机科学教育。直到今天还有很多公司推行所谓的Agile，煞有介事的搞一些stand-up meeting, scrum之类的形式主义东西，以为这些过家家似的做法就能提高开发质量和效率。很多开发人员也很把一些软件工程的工具当回事，喜欢折腾Git，Maven等工具一些偏僻的“新功能”。他们很在乎所谓的版本控制，测试等东西，以为熟练的掌握这些就能开发出高质量，可靠的代码。可是你最后发现，无论你如何高效的使用这些工具，它们都只能起到辅助的，次要的作用。编程工具永远不是程序本身，对编程工具的熟练掌握，永远也无法代替真正的对程序和计算的理解。过分强调这些工具的使用，是本末倒置的，让工程走上失败道路的作法。&lt;/p&gt;&lt;p&gt;编程真的是一门艺术，它完全符合艺术的各种特征，编程界也充满了艺术界的独有特征。有些初学艺术的人（比如10年前的我），总是挑剔手上的工具，非要用最新最炫的工具，用它们最偏僻最难用的“特性”，才觉得自己能够做出优秀的作品。很多人照不出好的照片，就怪相机不好。买了几万块钱的笨重高档相机，照出来的照片还不如别人用手机照的。这些人不明白，好的摄影师和不好的摄影师，区别在于眼睛，而不是相机。一个真正的艺术家，可以用任何在手上的工具创造出色的作品。有些甚至可以用一些废品垃圾，拙劣的工具，做出杰出的，别具风味的艺术品。因为艺术存在于人的心里，而不在他们使用的工具里面。&lt;/p&gt;</description>
<author>yinwang0</author>
<guid isPermaLink="false">software-engineering</guid>
<pubDate>Sun, 07 Jun 2015 00:00:00 +0800</pubDate>
</item>
<item>
<title>编程的宗派</title>
<link>http://yinwang.org/blog-cn/2015/04/03/paradigms</link>
<description>&lt;p&gt;
  　　
  &lt;/p&gt;&lt;h2&gt;编程的宗派&lt;/h2&gt;&lt;p&gt;总是有人喜欢争论这类问题，到底是“函数式编程”（FP）好，还是“面向对象编程”（OOP）好。既然出了两个帮派，就有人积极地做它们的帮众，互相唾骂和鄙视。然后呢又出了一个“好好先生帮”，这个帮的人喜欢说，管它什么范式呢，能解决问题的工具就是好工具！我个人其实不属于这三帮人中的任何一个。&lt;/p&gt;&lt;h3&gt;面向对象编程（Object-Oriented Programming）&lt;/h3&gt;&lt;p&gt;如果你看透了表面现象就会发现，其实“面向对象编程”本身没有引入很多新东西。所谓“面向对象语言”，其实就是经典的“过程式语言”（比如Pascal），加上一点抽象能力。所谓“类”和“对象”，基本是过程式语言里面的记录（record，或者叫结构，structure），它本质其实是一个从名字到数据的“映射表”（map）。你可以用名字从这个表里面提取相应的数据。比如&lt;code&gt;point.x&lt;/code&gt;，就是用名字&lt;code&gt;x&lt;/code&gt;从记录&lt;code&gt;point&lt;/code&gt;里面提取相应的数据。这比起数组来是一件很方便的事情，因为你不需要记住存放数据的下标。即使你插入了新的数据成员，仍然可以用原来的名字来访问已有的数据，而不用担心下标错位的问题。&lt;/p&gt;&lt;p&gt;所谓“对象思想”（区别于“面向对象”），实际上就是对这种数据访问方式的进一步抽象。一个经典的例子就是平面点的数据结构。如果你把一个点存储为：&lt;/p&gt;&lt;pre&gt;&lt;code&gt;struct Point {
  double x;
  double y;
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;那么你用&lt;code&gt;point.x&lt;/code&gt;和&lt;code&gt;point.y&lt;/code&gt;可以直接访问它的X和Y坐标。但你也可以把它存储为“极坐标”方式：&lt;/p&gt;&lt;pre&gt;&lt;code&gt;struct Point {
  double r;
  double angle;
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这样你可以用&lt;code&gt;point.r&lt;/code&gt;和&lt;code&gt;point.angle&lt;/code&gt;访问它的模和角度。可是现在问题来了，如果你的代码开头把Point定义为第一种XY的方式，使用&lt;code&gt;point.x&lt;/code&gt;, &lt;code&gt;point.y&lt;/code&gt;访问X和Y坐标，可是后来你决定改变Point的存储方式，用极坐标，你却不想修改已有的含有&lt;code&gt;point.x&lt;/code&gt;和&lt;code&gt;point.y&lt;/code&gt;的代码，怎么办呢？&lt;/p&gt;&lt;p&gt;这就是“对象思想”的价值，它让你可以通过“间接”（indirection，或者叫做“抽象”）来改变&lt;code&gt;point.x&lt;/code&gt;和&lt;code&gt;point.y&lt;/code&gt;的语义，从而让使用者的代码完全不用修改。虽然你的实际数据结构里面根本没有&lt;code&gt;x&lt;/code&gt;和&lt;code&gt;y&lt;/code&gt;这两个成员，但由于&lt;code&gt;.x&lt;/code&gt;和&lt;code&gt;.y&lt;/code&gt;可以被重新定义，所以你可以通过改变&lt;code&gt;.x&lt;/code&gt;和&lt;code&gt;.y&lt;/code&gt;的定义来“模拟”它们。在你使用&lt;code&gt;point.x&lt;/code&gt;和&lt;code&gt;point.y&lt;/code&gt;的时候，系统内部其实在运行两片代码，它们的作用是从&lt;code&gt;r&lt;/code&gt;和&lt;code&gt;angle&lt;/code&gt;计算出&lt;code&gt;x&lt;/code&gt;和&lt;code&gt;y&lt;/code&gt;的值。这样你的代码就感觉&lt;code&gt;x&lt;/code&gt;和&lt;code&gt;y&lt;/code&gt;是实际存在的成员一样，而其实它们是被临时算出来的。在Python之类的语言里面，你可以通过定义“&lt;a href=&quot;https://docs.python.org/2/library/functions.html#property&quot;&gt;property&lt;/a&gt;”来直接改变&lt;code&gt;point.x&lt;/code&gt;和&lt;code&gt;point.y&lt;/code&gt;的语义。在Java里稍微麻烦一些，你需要使用&lt;code&gt;point.getX()&lt;/code&gt;和&lt;code&gt;point.getY()&lt;/code&gt;这样的写法。然而它们最后的目的其实都是一样的——它们为数据访问提供了一层“间接”（抽象）。&lt;/p&gt;&lt;p&gt;这种抽象有时候是个好主意，它甚至可以跟量子力学的所谓“不可观测性”扯上关系。你觉得这个原子里面有10个电子？也许它们只是像&lt;code&gt;point.x&lt;/code&gt;给你的幻觉一样，也许宇宙里根本就没有电子这种东西，也许你每次看到所谓的电子，它都是临时生成出来逗你玩的呢？然而，对象思想的价值也就到此为止了。你见过的所谓“面向对象思想”，几乎无一例外可以从这个想法推广出来。面向对象语言的绝大部分特性，其实是过程式语言早就提供的。因此我觉得，其实没有语言可以叫做“面向对象语言”。就像一个人为一个公司贡献了一点点代码，并不足以让公司以他的名字命名一样。&lt;/p&gt;&lt;p&gt;“对象思想”作为数据访问的方式，是有一定好处的。然而“面向对象”（多了“面向”两个字），就是把这种本来良好的思想东拉西扯，牵强附会，发挥过了头。很多面向对象语言号称“所有东西都是对象”（Everything is an Object），把所有函数都放进所谓对象里面，叫做“方法”（method），把普通的函数叫做“静态方法”（static method）。实际上呢，就像我之前的例子，只有极少需要抽象的时候，你需要使用内嵌于对象之内，跟数据紧密结合的“方法”。其他的时候，你其实只是想表达数据之间的变换操作，这些完全可以用普通的函数表达，而且这样做更加简单和直接。这种把所有函数放进方法的做法是本末倒置的，因为函数其实并不属于对象。绝大部分函数是独立于对象的，它们不能被叫做“方法”。强制把所有函数放进它们本来不属于的对象里面，把它们全都作为“方法”，导致了面向对象代码逻辑过度复杂。很简单的想法，非得绕好多道弯子才能表达清楚。很多时候这就像把自己的头塞进屁股里面。&lt;/p&gt;&lt;p&gt;这就是为什么我喜欢开玩笑说，面向对象编程就像“&lt;a href=&quot;http://zh.wikipedia.org/wiki/%E5%9C%B0%E5%B9%B3%E8%AA%AA&quot;&gt;地平说&lt;/a&gt;”（Flat Earth Theory）。当然你可以说地球是一个平面。对于局部的，小规模的现象，它没有问题。然而对于通用的，大规模的情况，它却不是自然，简单和直接的。直到&lt;a href=&quot;http://www.theflatearthsociety.org/cms&quot;&gt;今天&lt;/a&gt;，你仍然可以无止境的寻找证据，扭曲各种物理定律，自圆其说地平说的幻觉，然而这会让你的理论非常复杂，经常需要缝缝补补还难以理解。&lt;/p&gt;&lt;p&gt;面向对象语言不仅有自身的根本性错误，而且由于面向对象语言的设计者们常常是半路出家，没有受到过严格的语言理论和设计训练却又自命不凡，所以经常搞出另外一些奇葩的东西。比如在JavaScript里面，每个函数同时又可以作为构造函数（constructor），所以每个函数里面都隐含了一个this变量，你嵌套多层对象和函数的时候就发现没法访问外层的this，非得bind一下。Python的变量定义和赋值不分，所以你需要访问全局变量的时候得用global关键字，后来又发现如果要访问“中间层”的变量，没有办法了，所以又加了个nonlocal关键字。Ruby先后出现过四种类似lambda的东西，每个都有自己的怪癖…… 有些人问我为什么有些语言设计成那个样子，我只能说，很多语言设计者其实根本不知道自己在干什么！&lt;/p&gt;&lt;p&gt;软件领域就是喜欢制造宗派。“面向对象”当年就是乘火打劫，扯着各种幌子，成为了一种宗派，给很多人洗了脑。到底什么样的语言才算是“面向对象语言”？这样基本的问题至今没有确切的答案，足以说明所谓面向对象，基本都是扯淡。每当你指出某个OO语言X的弊端，就会有人跟你说，其实X不是“地道的”OO语言，你应该去看看另外一个OO语言Y。等你发现Y也有问题，有人又会让你去看Z…… 直到最后，他们告诉你，只有Smalltalk才是地道的OO语言。这不是很搞笑吗，说一个根本没人用的语言才是地道的OO语言，这就像在说只有死人的话才是对的。这就像是一群政客在踢皮球，推卸责任。等你真正看看Smalltalk才发现，其实面向对象语言的根本毛病就是由它而来的，Smalltalk并不是很好的语言。很多人至今不知道自己所用的“面向对象语言”里面的很多优点，都是从过程式语言继承来的。每当发生函数式与面向对象式语言的口水战，都会有面向对象的帮众拿出这些过程式语言早就有的优点来进行反驳：“你说面向对象不好，看它能做这个……” 拿别人的优点撑起自己的门面，却看不到事物实质的优点，这样的辩论纯粹是鸡同鸭讲。&lt;/p&gt;&lt;h3&gt;函数式编程（Functional Programming）&lt;/h3&gt;&lt;p&gt;函数式语言一直以来比较低调，直到最近由于并发计算编程瓶颈的出现，以及Haskell，Scala之类语言社区的大力鼓吹，它忽然变成了一种宗派。有人盲目的相信函数式编程能够奇迹般的解决并发计算的难题，而看不到实质存在的，独立于语言的问题。被函数式语言洗脑的帮众，喜欢否定其它语言的一切，看低其它程序员。特别是有些初学编程的人，俨然把函数式编程当成了一天瘦二十斤的减肥神药，以为自己从函数式语言入手，就可以对经验超过他十年以上的老程序员说三道四，仿佛别人不用函数式语言就什么都不懂一样。&lt;/p&gt;&lt;h4&gt;函数式编程的优点&lt;/h4&gt;&lt;p&gt;函数式编程当然提供了它自己的价值。函数式编程相对于面向对象最大的价值，莫过于对于函数的正确理解。在函数式语言里面，函数是“一类公民”（first-class）。它们可以像1, 2, &quot;hello&quot;，true，对象…… 之类的“值”一样，在任意位置诞生，通过变量，参数和数据结构传递到其它地方，可以在任何位置被调用。这些是很多过程式语言和面向对象语言做不到的事情。很多所谓“面向对象设计模式”（design pattern），都是因为面向对象语言没有first-class function，所以导致了每个函数必须被包在一个对象里面才能传递到其它地方。&lt;/p&gt;&lt;p&gt;函数式编程的另一个贡献，是它们的类型系统。函数式语言对于类型的思维，往往非常的严密。函数式语言的类型系统，往往比面向对象语言来得严密和简单很多，它们可以帮助你对程序进行严密的逻辑推理。然而类型系统一是把双刃剑，如果你对它看得太重，它反而会带来不必要的复杂性和过度工程。这个我在下面讲讲。&lt;/p&gt;&lt;h4&gt;各种“白象”（white elephant）&lt;/h4&gt;&lt;p&gt;所谓白象，“white elephant”，是指被人奉为神圣，价格昂贵，却没有实际用处的东西。函数式语言里面有很好的东西，然而它们里面有很多多余的特性，这些特性跟白象的性质类似。&lt;/p&gt;&lt;p&gt;函数式语言的“拥护者”们，往往认为这个世界本来应该是“纯”（pure）的，不应该有任何“副作用”。他们把一切的“赋值操作”看成低级弱智的作法。他们很在乎所谓尾递归，类型推导，fold，currying，maybe type等等。他们以自己能写出使用这些特性的代码为豪。可是殊不知，那些东西其实除了能自我安慰，制造高人一等的幻觉，并不一定能带来真正优秀可靠的代码。&lt;/p&gt;&lt;h5&gt;纯函数&lt;/h5&gt;&lt;p&gt;半壶水都喜欢响叮当。很多喜欢自吹为“函数式程序员”的人，往往并不真的理解函数式语言的本质。他们一旦看到过程式语言的写法就嗤之以鼻。比如以下这个C函数：&lt;/p&gt;&lt;pre&gt;&lt;code&gt;int f(int x) {
    int y = 0;
    int z = 0;
    y = 2 * x;
    z = y + 1;
    return z / 3;
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;很多函数式程序员可能看到那几个赋值操作就皱起眉头，然而他们看不到的是，这是一个真正意义上的“纯函数”，它在本质上跟Haskell之类语言的函数是一样的，也许还更加优雅一些。&lt;/p&gt;&lt;p&gt;盲目鄙视赋值操作的人，也不理解“数据流”的概念。其实不管是对局部变量赋值还是把它们作为参数传递，其实本质上都像是把一个东西放进一个管道，或者把一个电信号放在一根导线上，只不过这个管道或者导线，在不同的语言范式里放置的方向和样式有一点不同而已！&lt;/p&gt;&lt;h5&gt;对数据结构的忽视&lt;/h5&gt;&lt;p&gt;函数式语言的帮众没有看清楚的另一个重要的，致命的东西，是数据结构的根本性和重要性。数据结构的有些问题是“物理”和“本质”地存在的，不是换个语言或者换个风格就可以奇迹般消失掉的。函数式语言的拥护者们喜欢盲目的相信和使用列表（list），而没有看清楚它的本质以及它所带来的时间复杂度。列表带来的问题，不仅仅是编程的复杂性。不管你怎么聪明的使用它，很多性能问题是根本没法解决的，因为列表的拓扑结构根本就不适合用来干有些事情！&lt;/p&gt;&lt;p&gt;从数据结构的角度看，Lisp所谓的list就是一个单向链表。你必须从上一个节点才能访问下一个，而这每一次“间接寻址”，都是需要时间的。在这种数据结构下，很简单的像length或者append之类函数，时间复杂度都是O(n)！为了绕过这数据结构的不足，所谓的“Lisp风格”告诉你，不要反复append，因为那样复杂度是O(n&lt;sup&gt;2&lt;/sup&gt;)。如果需要反复把元素加到列表末尾，那么应该先反复cons，然后再reverse一下。很可惜的是，当你同时有递归调用，就会发现cons+reverse的做法颠来倒去的，非常容易出错。有时候列表是正的，有时候是反的，有时候一部分是反的…… 这种方式用一次还可以，多几层递归之后，自己都把自己搞糊涂了。好不容易做对了，下次修改可能又会出错。然而就是有人喜欢显示自己聪明，喜欢自虐，迎着这类人为制造的“困难”勇往直前 :)&lt;/p&gt;&lt;p&gt;富有讽刺意味的是，半壶水的Lisp程序员都喜欢用list，真正深邃的Lisp大师级人物，却知道什么时候应该使用记录（结构）或者数组。在Indiana大学，我曾经上过一门Scheme（一种现代Lisp方言）编译器的课程，授课的老师是R. Kent Dybvig，他是世界上最先进的Scheme编译器Chez Scheme的作者。我们的课程编译器的数据结构（包括AST）都是用list表示的。期末的时候，Kent对我们说：“你们的编译器已经可以生成跟我的Chez Scheme媲美的代码，然而Chez Scheme不止生成高效的目标代码，它的编译速度是你们的700倍以上。它可以在5秒钟之内编译它自己！” 然后他透露了一点Chez Scheme速度之快的原因。其中一个原因，就是因为Chez Scheme的内部数据结构根本不是list。在编译一开头的时候，Chez Scheme就已经把输入的代码转换成了数组一样的，固定长度的结构。后来在工业界的经验教训也告诉了我，数组比起链表，确实在某些时候有大幅度的性能提升。在什么时候该用链表，什么时候该用数组，是一门艺术。&lt;/p&gt;&lt;h5&gt;副作用的根本价值&lt;/h5&gt;&lt;p&gt;对数据结构的忽视，跟纯函数式语言盲目排斥副作用的“教义”有很大关系。过度的使用副作用当然是有害的，然而副作用这种东西，其实是根本的，有用的。对于这一点，我喜欢跟人这样讲：在计算机和电子线路最开头发明的时候，所有的线路都是“纯”的，因为逻辑门和导线没有任何记忆数据的能力。后来有人发明了触发器（flip-flop），才有了所谓“副作用”。是副作用让我们可以存储中间数据，从而不需要把所有数据都通过不同的导线传输到需要的地方。没有副作用的语言，就像一个没有无线电，没有光的世界，所有的数据都必须通过实在的导线传递，这许多纷繁的电缆，必须被正确的连接和组织，才能达到需要的效果。我们为什么喜欢WiFi，4G网，Bluetooth，这也就是为什么一个语言不应该是“纯”的。&lt;/p&gt;&lt;p&gt;副作用也是某些重要的数据结构的重要组成元素。其中一个例子是哈希表。纯函数语言的拥护者喜欢盲目的排斥哈希表的价值，说自己可以用纯的树结构来达到一样的效果。然而事实却是，这些纯的数据结构是不可能达到有副作用的数据结构的性能的。所谓纯函数数据结构，因为在每一次“修改”时都需要保留旧的结构，所以往往需要大量的拷贝数据，然后依赖垃圾回收（GC）去消灭这些旧的数据。要知道，内存的分配和释放都是需要时间和能量的。盲目的依赖GC，导致了纯函数数据结构内存分配和释放过于频繁，无法达到有副作用数据结构的性能。要知道，副作用是电子线路和物理支持的高级功能。盲目的相信和使用纯函数写法，其实是在浪费已有的物理支持的操作。&lt;/p&gt;&lt;h5&gt;fold以及其他&lt;/h5&gt;&lt;p&gt;大量使用fold和&lt;a href=&quot;http://www.yinwang.org/blog-cn/2013/04/02/currying&quot;&gt;currying&lt;/a&gt;的代码，写起来貌似很酷，读起来却不必要的痛苦。很多人根本不明白fold的本质，却老喜欢用它，因为他们觉得那是函数式编程的“精华”，可以显示自己的聪明。然而他们没有看到的是，其实fold包含的，只不过是在列表（list）上做递归的“通用模板”，这个模板需要你填进去三个参数，就可以生成一个新的递归函数调用。所以每一个fold的调用，本质上都包含了一个在列表上的递归函数定义。fold的问题在于，它定义了一个递归函数，却没有给它一个一目了然的名字。使用fold的结果是，每次看到一个fold调用，你都需要重新读懂它的定义，琢磨它到底是干什么的。而且fold调用只显示了递归模板需要的部分，而把递归的主体隐藏在了fold本身的“框架”里。比起直接写出整个递归定义，这种遮遮掩掩的做法，其实是更难理解的。比如，当你看到这句Haskell代码：&lt;/p&gt;&lt;pre&gt;foldr (+) 0 [1,2,3]&lt;/pre&gt;&lt;p&gt;你知道它是做什么的吗？也许你一秒钟之后就凭经验琢磨出，它是在对&lt;code&gt;[1,2,3]&lt;/code&gt;里的数字进行求和，本质上相当于&lt;code&gt;sum [1,2,3]&lt;/code&gt;。虽然只花了一秒钟，可你仍然需要琢磨。如果fold里面带有更复杂的函数，而不是&lt;code&gt;+&lt;/code&gt;，那么你可能一分钟都琢磨不透。写起来倒没有费很大力气，可为什么我每次读这段代码，都需要看到&lt;code&gt;+&lt;/code&gt;和&lt;code&gt;0&lt;/code&gt;这两个跟自己的意图毫无关系的东西？万一有人不小心写错了，那里其实不是&lt;code&gt;+&lt;/code&gt;和&lt;code&gt;0&lt;/code&gt;怎么办？为什么我需要搞清楚&lt;code&gt;+&lt;/code&gt;, &lt;code&gt;0&lt;/code&gt;, &lt;code&gt;[1,2,3]&lt;/code&gt;的相对位置以及它们的含义？这样的写法其实还不如老老实实写一个递归函数，给它一个有意义名字（比如&lt;code&gt;sum&lt;/code&gt;），这样以后看到这个名字被调用，比如&lt;code&gt;sum [1,2,3]&lt;/code&gt;，你想都不用想就知道它要干什么。定义&lt;code&gt;sum&lt;/code&gt;这样的名字虽然稍微增加了写代码时的工作，却给读代码的时候带来了方便。为了写的时候简洁或者很酷而用fold，其实增加了读代码时的脑力开销。要知道代码被读的次数，要比被写的次数多很多，所以使用fold往往是得不偿失的。然而，被函数式编程洗脑的人，却看不到这一点。他们太在乎显示给别人看，我也会用fold！&lt;/p&gt;&lt;p&gt;与fold类似的白象，还有&lt;a href=&quot;http://www.yinwang.org/blog-cn/2013/04/02/currying&quot;&gt;currying&lt;/a&gt;，Hindley-Milner类型推导等特性。看似很酷，但等你仔细推敲才发现，它们带来的麻烦，比它们解决的问题其实还要多。有些特性声称解决的问题，其实根本就不存在。现在我把一些函数式语言的特性，以及它们包含的陷阱简要列举一下：&lt;/p&gt;&lt;ol&gt;
&lt;li&gt;fold。fold等“递归模板”，相当于把递归函数定义插入到调用的敌方，而不给它们名字。这样导致每次读代码都需要理解几乎整个递归函数的定义。&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://www.yinwang.org/blog-cn/2013/04/02/currying&quot;&gt;currying&lt;/a&gt;。貌似很酷，可是被部分调用的参数只能从左到右，依次进行。如何安排参数的顺序成了问题。大部分时候还不如直接制造一个新的lambda，在内部调用旧的函数，这样可以任意的安排参数顺序。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Hindley-Milner类型推导。为了避免写参数和返回值的类型，结果给程序员写代码增加了很多的限制。为了让类型推导引擎开心，导致了很多完全合法合理优雅的代码无法写出来。其实还不如直接要程序员写出参数和返回值的类型，这工作量真的不多，而且可以准确的帮助阅读者理解参数的范围。HM类型推导的根本问题其实在于它使用unification算法。Unification其实只能表示数学里的“等价关系”（equivalence relation），而程序语言最重要的关系，subtyping，并不是一个等价关系，因为它不具有对称性（symmetry）。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;代数数据类型（algebraic data type）。所谓“代数数据类型”，其实并不如普通的类型系统（比如Java的）通用。很多代数数据类型系统具有所谓sum type，这种类型其实带来过多的类型嵌套，不如通用的union type。盲目崇拜代数数据类型的人，往往是因为盲目的相信“数学是优美的语言”。而其实事实是，数学是一种历史遗留的，毛病很多的语言。数学的语言根本没有经过系统的，全球协作的设计。往往是数学家在黑板上随便写个符号，说这个表示XX概念，然后就定下来了。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Tuple。有代数数据类型的的语言里面经常有一种构造叫做Tuple，比如Haskell里面可以写&lt;code&gt;(1, &quot;hello&quot;)&lt;/code&gt;，表示一个类型为&lt;code&gt;(Int, String)&lt;/code&gt;的结构。这种构造经常被人看得过于高尚，以至于用在超越它能力的地方。其实Tuple就是一个没有名字的结构（类似C的structure），而且结构里面的域也没有名字。临时使用Tuple貌似很方便，因为不需要定义一个结构类型。然而因为Tuple没有名字，而且里面的域没法用名字访问，一旦里面的数据多一点就发现很麻烦了。Tuple往往只能通过模式匹配来获得里面的域，一旦你增加了新的域进去，所有含有这个Tuple的模式匹配代码都需要改。所以Tuple一般只能用在大小不超过3的情况下，而且必须确信以后不会增加新的域进去。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://www.yinwang.org/blog-cn/2013/04/01/lazy-evaluation&quot;&gt;惰性求值&lt;/a&gt;（lazy evaluation）。貌似数学上很优雅，但其实有严重的逻辑漏洞。因为bottom（死循环）成为了任何类型的一个元素，所以取每一个值，都可能导致死循环。同时导致代码性能难以预测，因为求值太懒，所以可能临时抱佛脚做太多工作，而平时浪费CPU的时间。由于到需要的时候才求值，所以在有多个处理器的时候无法有效地利用它们的计算能力。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;尾递归。大部分尾递归都相当于循环语句，然而却不像循环语句一样具有一目了然的意图。你需要仔细看代码的各个分支的返回条件，判断是否有分支是尾递归，然后才能判断这代码是个循环。而循环语句从关键字（for，while）就知道是一个循环。所以等价于循环的尾递归，其实最好还是写成特殊的循环语句。当然，尾递归在另一些情况下是有用的，这些情况不等价于循环。在这种情况下使用循环，经常需要复杂的break或者continue条件，导致循环不易理解。所以循环和尾递归，其实都是有必要的。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;h3&gt;好好先生&lt;/h3&gt;&lt;p&gt;很多人避免“函数式vs面向对象”的辩论，于是他们成为了“好好先生”。这种人没有原则的认为，任何能够解决当前问题的工具就是好工具。也就是这种人，喜欢使用shell script，喜欢折腾各种Unix工具，因为显然，它们能解决他“手头的问题”。&lt;/p&gt;&lt;p&gt;然而这种思潮是极其有害的，它的害处其实更胜于投靠函数式或者面向对象。没有原则的好好先生们忙着“解决问题”，却不能清晰地看到这些问题为什么存在。他们所谓的问题，往往是由于现有工具的设计失误。由于他们的“随和”，他们从来不去思考，如何从根源上消灭这些问题。他们在一堆历史遗留的垃圾上缝缝补补，妄图使用设计恶劣的工具建造可靠地软件系统。当然，这代价是非常大的。不但劳神费力，而且也许根本不能解决问题。&lt;/p&gt;&lt;p&gt;所以每当有人让我谈谈“函数式vs面向对象”，我都避免说“各有各的好处”，因为那样的话我会很容易被当成这种毫无原则的好好先生。&lt;/p&gt;&lt;h3&gt;符号必须简单的对世界建模&lt;/h3&gt;&lt;p&gt;从上面你已经看出，我既不是一个铁杆“函数式程序员”，也不是一个铁杆“面向对象程序员”，我也不是一个爱说“各有各的好处”的好好先生。我是一个有原则的批判性思维者。我不但看透了各种语言的本质，而且看透了它们之间的统一关系。我编程的时候看到的不是表面的语言和程序，而是一个类似电路的东西。我看到数据的流动和交换，我看到效率的瓶颈，而这些都是跟具体的语言和范式无关的。&lt;/p&gt;&lt;p&gt;在我的心目中其实只有一个概念，它叫做“编程”（programming），它不带有任何附加的限定词（比如“函数式”或者“面向对象”）。我的老师Dan Friedman喜欢把自己的领域称为“Programming Languages”，也是一样的原因。因为我们研究的内容，不局限于某一个语言，也不局限于某一类语言，而是所有的语言。在我们的眼里，所有的语言都不过是各个特性的组合。在我们的眼里，最近出现的所谓“新语言”，其实不大可能再有什么真正意义上的创新。我们不喜欢说“发明一个程序语言”，不喜欢使用“发明”这个词，因为不管你怎么设计一个语言，所有的特性几乎都早已存在于现有的语言里面了。我更喜欢使用“设计”这个词，因为虽然一个语言没有任何新的特性，它却有可能在细节上更加优雅。&lt;/p&gt;&lt;p&gt;编程最重要的事情，其实是让写出来的符号，能够简单地对实际或者想象出来的“世界”进行建模。一个程序员最重要的能力，是直觉地看见符号和现实物体之间的对应关系。不管看起来多么酷的语言或者范式，如果必须绕着弯子才能表达程序员心目中的模型，那么它就不是一个很好的语言或者范式。有些东西本来就是有随时间变化的“状态”的，如果你偏要用“纯函数式”语言去描述它，当然你就进入了那些monad之类的死胡同。最后你不但没能高效的表达这种副作用，而且让代码变得比过程式语言还要难以理解。如果你进入另一个极端，一定要用对象来表达本来很纯的数学函数，那么你一样会把简单的问题搞复杂。Java的所谓design pattern，很多就是制造这种问题的，而没有解决任何问题。&lt;/p&gt;&lt;p&gt;关于建模的另外一个问题是，你心里想的模型，并不一定是最好的，也不一定非得设计成那个样子。有些人心里没有一个清晰简单的模型，觉得某些语言“好用”，就因为它们能够对他那种扭曲纷繁的模型进行建模。所以你就跟这种人说不清楚，为什么这个语言不好，因为显然这个语言对他是有用的！如何简化模型，已经超越了语言的范畴，在这里我就不细讲了。&lt;/p&gt;&lt;p&gt;我设计Yin语言的宗旨，就是让人们可以用最简单，最直接的方式来对世界进行建模，并且帮助他们优化和改进模型本身。&lt;/p&gt;</description>
<author>yinwang0</author>
<guid isPermaLink="false">paradigms</guid>
<pubDate>Fri, 03 Apr 2015 00:00:00 +0800</pubDate>
</item>
<item>
<title>智商的圈套</title>
<link>http://yinwang.org/blog-cn/2015/03/20/trap-of-intelligence</link>
<description>&lt;p&gt;
  　　
  &lt;/p&gt;&lt;h2&gt;智商的圈套&lt;/h2&gt;&lt;p&gt;上次买了个&lt;a href=&quot;http://www.jianshu.com/p/b501a1675f4d&quot;&gt;任天堂3DS&lt;/a&gt;游戏机，觉得里面的游戏很无聊，所以第二天就把游戏机连同游戏一起，转手倒卖给了别人。从那天之后，我开始琢磨一个问题——到底是什么让我觉得一个游戏好玩或者不好玩。我似乎对事物有一种很特别的品味，很多别人说“好玩”，“有趣”的游戏或者电影，我一看就觉得很无趣，或者很自虐。我一生中玩过最好玩的游戏，其实没有几个，可能掰着手指头都数得出来：&lt;a href=&quot;http://braid-game.com&quot;&gt;Braid&lt;/a&gt;，&lt;a href=&quot;http://en.wikipedia.org/wiki/Limbo_%28video_game%29&quot;&gt;Limbo&lt;/a&gt;，&lt;a href=&quot;http://en.wikipedia.org/wiki/Klonoa:_Door_to_Phantomile&quot;&gt;Klonoa&lt;/a&gt;（風のクロノア door to phantomile），《&lt;a href=&quot;https://itunes.apple.com/cn/app/ji-nian-bei-gu/id728293409&quot;&gt;纪念碑谷&lt;/a&gt;》，&lt;a href=&quot;http://en.wikipedia.org/wiki/Metal_Gear_Solid&quot;&gt;Metal Gear Solid&lt;/a&gt;，……&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;http://www.yinwang.org/images/Klonoa.jpg&quot;&gt;&lt;/p&gt;&lt;p&gt;如果你觉得我智商太高，所以才觉得很多游戏没有挑战性，不好玩，那么你其实并不了解我。我并不是一个“智商达人”，我不追求挑战性。我觉得很多游戏缺乏的不是挑战性和“难度”，而是设计的巧妙。很多游戏我根本没法玩过关，却只是觉得呆板，繁琐，老套，公式化。我并不会因为游戏玩不过关，作业做不出来，或者书看不懂而沮丧。恰恰相反，我认为我的智力根本就不应该是用来干这些事情的。如果有事情让我觉得沮丧，我一般都认为是这个事情有问题，而不是我有问题。如果说我也有错的话，那么我的错误就在于选择了参与这项活动，我根本不应该做这件事情。这就是为什么我大部分时候都比一般人开心。&lt;/p&gt;&lt;p&gt;我觉得很多人有一种奇怪的倾向，他们喜欢挑战或者彰显自己的智商。每当我向人推荐类似Braid的游戏，他们就会认为我喜欢“解谜题”，于是他们给我推荐类似&lt;a href=&quot;http://www.zelda.com&quot;&gt;Zelda&lt;/a&gt;或者&lt;a href=&quot;http://www.antichamber-game.com&quot;&gt;Antichamber&lt;/a&gt;之类的游戏，告诉我它们很考智力。可是这样的游戏，我一般玩不到几分钟就开始觉得无聊。这说明我并不是喜欢“解谜题”，而是因为另外一些特征而喜欢某些游戏。喜欢玩Zelda，Antichamber，或者《生化危机》一类游戏的人，往往有一种自虐倾向。这种人似乎很在乎自己的智商，所以游戏玩了不久之后，就会被“套牢”。他们会认为能够把某个游戏打通关，是对自己智商的认可。如果你跟他说这游戏太难太麻烦，他就会开始鄙视你的智力，吹嘘自己只花了多么短的时间就玩通关了。&lt;/p&gt;&lt;p&gt;然而如果你退后一步，就会发现这些游戏，其实都存在某种“&lt;a href=&quot;http://www.jianshu.com/p/b501a1675f4d&quot;&gt;设计公式&lt;/a&gt;”。一旦掌握了这些公式，你就可以轻而易举地制造出这样的游戏。然后你就会发现，热衷于这些游戏的人，其实并不聪明，因为他们被游戏的设计者玩弄于鼓掌之中，而没能发现其中的设计公式。这些人为了得到别人的认可，检验或者训练所谓的“智力”，甚至为了“合群”，选择了这类只能叫做“自虐型”的游戏。&lt;/p&gt;&lt;p&gt;这种游戏玩到后来，你就会发现这不是在娱乐，而是在完成任务，不是你在玩游戏，而是游戏在玩你。你盼望它早点结束，但却无法立即罢手，因为你对自己说：“如果现在半途而废，我就是一个懦夫，一个笨蛋，就不再是一个天才……” 你在虚拟的空间中来回的游走，摸索和寻找那些能打开机关的“钥匙”，而它们被游戏的设计者故意放在一些让人恼火的地方。你感觉到的不是快乐，而是繁琐，沮丧和空虚。&lt;/p&gt;&lt;p&gt;我发现容易落入这种圈套的人，他们在日常生活和工作中也容易出现类似的倾向。总的说来，这种人正如卓别林的《大独裁者》最后的&lt;a href=&quot;http://tinyurl.com/bbqfs6s&quot;&gt;演讲&lt;/a&gt;所描述的，“想得太多，感觉太少”（think too much, feel too little）。这种人如果沿着这条道路发展下去，就会变成像机器一样思考的人。正是这种人，给世界带来了灾难。希特勒就是这样一种人的典型代表，他太在乎自己是否优秀和聪明，却感觉不到人间的爱和痛苦，所以他对自己认为是劣等民族的人进行残酷的屠杀。&lt;/p&gt;&lt;p&gt;所以，我其实并不是因为智力上的挑战性而喜欢Braid，Limbo，Klonoa等游戏。我喜欢它们，是因为它们充满了创意和想象力，却又不让人觉得繁琐和累赘。在这样的游戏里，你能做一些你从前根本没想到过的事情，它们的设计可以用“妙不可言”来形容。这种游戏的逻辑很连贯流畅，你不需要到处瞎撞，来回跑动，而是一气呵成，行云流水，却又不乏波澜起伏和机智巧妙之处。这就像自己在演出一场出神入化的电影。你感觉到的不是沮丧，迷茫，不是对自己智力的考验和评价，而是真正的愉悦和解脱。&lt;/p&gt;&lt;p&gt;当我推荐Klonoa给一个朋友的时候，我说：“玩这个游戏就感觉是在梦里……” 结果他对我说：“你知道另外一个叫什么什么的游戏里面，也有个四维空间吗？……” 其实我根本不是在跟他讨论“梦是什么”这种学术问题，而是在说“梦幻的感觉”。这位朋友就属于我前面提到的，“想得太多”的类型。我说像是在梦里，说的是一种感觉，只有心才看得见；而他所理解的“梦”，是一种很理论的东西，就像数学里的多维空间，需要用脑才分析得出来。由于过度理性，他总是忙于分析一些“深层次”的理论，而看不见我能轻松感觉到的乐趣。我对他的建议是：少想一点，少分析一点，多用心感觉。只有用心去体会，你才会理解，Klonoa这样的游戏的价值，其实不在于智力和难度，而在于它让你感觉到的梦幻，创意，自由，想象力，和艺术。&lt;/p&gt;</description>
<author>yinwang0</author>
<guid isPermaLink="false">trap-of-intelligence</guid>
<pubDate>Fri, 20 Mar 2015 00:00:00 +0800</pubDate>
</item>
<item>
<title>我为什么不再公开开发Yin语言</title>
<link>http://yinwang.org/blog-cn/2015/03/18/yin-lang-secret</link>
<description>&lt;p&gt;
  　　
  &lt;/p&gt;&lt;h2&gt;我为什么不再公开开发Yin语言&lt;/h2&gt;&lt;p&gt;有些人可能知道我在设计一个程序语言，叫做Yin语言。最开头宣布要做这个语言的时候，很多人热血沸腾，可是过了不久，我发现自己很不喜欢这样的气氛，越来越厌倦跟人讨论，所以后来悄悄地丢掉这些人，淡出了。我现在想告诉你我是怎么想的。&lt;/p&gt;&lt;p&gt;我从来没有想让Yin语言流行起来。我对程序语言的认识，其实超乎所有人的想象。我默默地看着各种新语言扯着各种幌子进行推广，可是它们的设计没有吸取历史教训，所以继续犯一些古老的错误，或者犯一些我根本不会犯的新错误，或者解决一些根本不需要解决的问题。其实程序语言已经不缺很多新的功能了，这些语言缺少的不是功能，而是简单和优雅。要达到简单和优雅，必须要有品位，而品位就像一个艺术家的心，是非常难得到的。没有经过Indiana式教育的人，是几乎不可能达到这种品位的。就算Friedman培养出来的那么多学生，也极少有人可以达到我这种地步。&lt;/p&gt;&lt;p&gt;我清楚的知道，其它语言设计者是完全没法达到我的一些精华思想的。我其实很懒，我希望有其他人做出我设想的语言，这样我就不需要亲自动手了，然而至今没有发现任何人可以做到，甚至根本没有想到。这些想法，早在多年前就已经被我在多个原型中实现过，所以具体做起来不会是问题。在我的心目中，Yin语言就像原子弹的技术。我不想搞核扩散，我并不想让所有人都得到它。我曾经觉得应该把它与全世界分享，后来我发现，你越是愿意分享，别人越是不拿你的东西当回事。我觉得这个世界不配拥有这样的语言，因为人类是那么的愚蠢。最可怕的是，很多人根本不知道自己其实很愚蠢。在资本主义这种奖励贪婪人的制度下，Yin语言被所有人掌握，很可能不是在造福世界，而是一种灾难。所以即使我把Yin语言做出来了，它也只会属于少数人，我不想让这样的技术落到贪婪或者愚蠢的人手里。&lt;/p&gt;&lt;p&gt;我很不喜欢人们对于一个新语言的反应。我不会因为人们对我设计的东西显示出盲目的热血沸腾而受到鼓舞，因为越是盲目热情的人，越可能脑子有问题，越可能在将来浪费我的口水，有理也说不清。我宣布开发Yin语言的第一天，就有人想把其他语言的“社区”的概念放到它身上。就像很多语言都搞得像宗教一样，一下子出现了好些想做“Yin语言传教士”的人。有些人太积极了，未经我同意就建立了IRC聊天室之类的东西，还有人立即买了个域名给Yin语言（然后控制权还没有给我）。他们其实没有想到，那些都不是我想要的。我很不喜欢其他语言的宗教性质，社区意识，阵营意识。我感觉有很多人其实只是想抢占“第一把交椅”，就像很多其他语言的狂热分子一样。他们让我感觉，从一开头我就已经失去了对这个语言的控制，仿佛它不再是我的设计。Yin语言跟其它语言不一样，它不应该有一个社区，不应该成为一个宗教，因为我是一个科学家，我的设计完全出于理性的思考。&lt;/p&gt;&lt;p&gt;我也不喜欢很多人对Yin语言肤浅的赞美或者质疑。有些人激动地对我说：“美国人，日本人，都有可以设计语言的人了，我们中国终于也有了！” 有的人甚至建议让它看起来像中文，符合中国人的思维方式。这些说法显示出人们对语言设计的无知和品位的低下。日本人做出了什么语言呢？我只知道一个日本人造的语言，它是一个彻底的垃圾 :P 美国人也没几个真正设计好点的语言。而且一个程序语言本来就跟人类语言扯不上任何关系，只不过有些关键字是人类语言的单词而已。也没有所谓“符合中国人的思维方式”一说，因为人脑其实根本就不是用人类语言在思考。人脑的思维方式更像是一种程序语言，一种电路，而不是人类语言。只有需要跟另一个人交流的时候，人脑才会把内部的“数据结构”转换成人类语言，就像Java的toString()方法一样。这就是我现在正在做的事，我很清楚我现在给你们打的这些字，不可能完全符合我的思维，也就是言不达意。所以无论把一个语言设计得像中文或者像英语，最后都是一个错误，因为普通的程序语言（比如C）早就在很多方面超越了人类语言，没有人类语言特有的那些历史遗留问题。SQL和COBOL之类的语言试图设计得像英语，结果惹出更多的麻烦事，得不偿失。想要一个程序语言有“中国特色”，其实显示出国人的自卑心理，也是在贬低我的价值。“中国第一”对我来说毫无意义，这让我感觉他们其实认为Yin语言跟“国产Linux”是一类的。要知道，我是世界上最强的语言设计者之一（很可能没有之一），我的&lt;a href=&quot;http://www.yinwang.org/blog-cn/2015/01/29/human-value&quot;&gt;价值&lt;/a&gt;是不限定于任何国家的，它不需要任何人的肯定。&lt;/p&gt;&lt;p&gt;还有的人开始发送各种github issue，请求他们在其他语言（比如Ruby）里用过，认为重要的“特性”。有人要求我给这个语言“定性”，一本正经的要我说明这是什么“范式”的语言。有人义正言辞的索取1.0版本的specification。有人开始质疑我的一些设计，甚至自作聪明的做了改动。有人质疑我为什么不用正则表达式来做lexer，跟我说Java的lexer多么的严谨，因为它用了正则表达式…… 这一切都让我觉得越来越傻，越来越无语，越来越浪费时间和口水。他们不知道，他们头脑里的很多概念几乎全都不存在于我的脑子里。我不喜欢有人自作聪明，觉得好像自己懂很多似的，好像还可以评价甚至教育我。每一次不耻下问都发现似乎有人真以为我不懂，以为自己是专家了。Yin语言也许根本不符合任何一种“语言范式”，然而它也不会像Scala一样弄成个大杂烩。它应该是天衣无缝的设计，就像一句&lt;a href=&quot;http://www.brainyquote.com/quotes/quotes/a/antoinedes121910.html&quot;&gt;名言&lt;/a&gt;说的：“一个设计师知道他达到了完美，并不是当他不能再加进任何东西，而是当没有任何东西可以被去掉。”&lt;/p&gt;&lt;p&gt;另外，很多人认为重要的特性，很有可能是有问题的。他们不明白，现有程序语言的问题，不是没有实现某些特性，而是实现了多余的特性，有问题的特性。如果错误的特性被加了进去，一旦有人开始用这个语言，就再也没法去掉了。所以作为一个优秀的语言设计者，我的一项重要任务是防止多余或者有问题的特性进入语言里。我也很不喜欢有人拿我的语言，我的开发实力跟其它语言作对比，因为比较本身就是一种不尊重的行为。比如有些人质疑Yin语言有没有&lt;a href=&quot;http://www.yinwang.org/blog-cn/2014/04/18/golang&quot;&gt;Go语言&lt;/a&gt;好，其实是在贬低我，因为我的水平跟Go语言的设计者根本不是一个档次的。Go语言的设计者其实基础知识都没搞清楚还自以为了不起，所以当我的学生都不合格。&lt;/p&gt;&lt;p&gt;所以，Yin语言的设计开发其实仍然在缓慢地进行中，然而已经不再公开，不再开源。我觉得所谓“开源精神”纯属扯淡，很多人开源不过是为了提高自己代码扩散的速度，提高知名度，这样可以带来利益，其实没有人真的是想做什么“贡献”的。这样的虚伪行为带来了开源社区的代码质量普遍低下，各种浮夸之风盛行，有人却看不出来。我不是Paul Graham，我不会吹牛，扬言要做个叫Arc的Lisp方言，结果最后做出来的东西连退步都不是（not even a step backwards）。我是有真正的实力，受过系统的精深的教育的，然而我真的有自己的困难和自己的生活。太多不合格的程序语言设计者占据了重要的语言设计岗位，很多公司已经完全不明白谁才是真正的专家。由于没有经济支持，我的大部分时间得用来做其他工作。设计语言是一个吃力不讨好的活，我想找到其它事情，甚至进入另一个领域，利用我的特殊品味来创造更大的价值。我不是工作狂，我也需要休闲和娱乐。我已经为技术耗费了太多的生命，我觉得我的人生是不完整的。为自己工作其实仍然算是工作，工作和生活需要平衡。我需要享受生活，需要陪我的猫咪，需要跟朋友玩，所以显然是不会浪费周末明媚的阳光，蹲在家里写代码的。所以Yin语言的开发虽然在进行，进度是不会很快的。即使我完成了，可能也不会给很多人用的。所以你们还是继续忍受现有语言和系统的扯淡和煎熬吧 :)&lt;/p&gt;</description>
<author>yinwang0</author>
<guid isPermaLink="false">yin-lang-secret</guid>
<pubDate>Wed, 18 Mar 2015 00:00:00 +0800</pubDate>
</item>
<item>
<title>设计的重要性</title>
<link>http://yinwang.org/blog-cn/2015/03/17/design</link>
<description>&lt;p&gt;
  　　
  &lt;/p&gt;&lt;h2&gt;设计的重要性&lt;/h2&gt;&lt;p&gt;我曾经在一篇&lt;a href=&quot;http://www.yinwang.org/blog-cn/2014/07/17/rest&quot;&gt;文章&lt;/a&gt;里谈过关于设计的问题，然而那篇文章由于标题不够醒目，可能很多人没有注意看。我觉得现在有必要把里面的内容专门提出来讲一下，因为设计在我的心目中具有至关重要的地位，却被很多计算机科学家和程序员所轻视。&lt;/p&gt;&lt;p&gt;我觉得自己不但是一个计算机科学家和程序员，在很大程度上我还是一个设计师。我不但是一个程序语言的设计师，而且是其它很多东西的设计师。我设计的东西不但常常比别人的简洁好用，而且我经常直接看出其他人的设计里面的问题。我写的代码不仅自己容易看懂，而且别人也容易理解。我有时候受命修补前人的BUG，结果没法看懂他们的代码。在这种情况下，我的解决方案是推翻重写。经我重写之后的代码，不仅没有BUG，而且简洁很多。&lt;/p&gt;&lt;p&gt;很多人自己的设计有问题，太复杂不易用，到头来却把责任推在用户身上，使用类似“皇帝的新装”的技巧，让用户有口难言。之前一篇&lt;a href=&quot;http://www.yinwang.org/blog-cn/2015/02/24/human-errors&quot;&gt;文章&lt;/a&gt;提到的严重交通事故，就是一个设计问题，却被很多人归结为“人为错误”。这种出人命的事情都这么难引起人们对设计的关注，就更不要说软件行业那些无关性命的恼人之处了。有些人写的代码过度复杂，BUG众多，却仿佛觉得自己可以评估其他人的智商，打心眼里觉得自己是专家，看不懂他代码的人都是笨蛋。&lt;/p&gt;&lt;p&gt;很多程序员有意把“用户”和自己区别开来，好像程序员应该高人一等，不能以用户的标准。所以他们觉得程序员就是应该会用各种难用的工具，难用的操作系统，程序语言，编辑器，…… 他们觉得只要你追求这些东西的“易用性”或者“直观性”，就说明你智商有问题。只要你说某个东西太复杂，另一个东西好用些，他们就会跟你说：“专家才用这个，你那个是菜鸟用的。” 这些人不明白，程序员其实也是用户，而且他们是自己的代码的用户，每一次调用自己写的函数，自己都是自己的用户。可是这种鄙视用户的风气之胜行，带来了整个行业不但设计过度复杂，而且以复杂为豪的局面。&lt;/p&gt;&lt;p&gt;经常有人自豪的声称自己的项目有多少万行代码，仿佛代码的行数是衡量一个软件质量的标准，行数越多质量越好，然而事实却恰恰相反。你可能需要经历过Indiana式的教育才能真正的理解这一点。如果你拿一些引以为豪的代码给Dan Friedman看，他可能瞟一眼就说：“太长了。当年这个东西我两行代码就写出来了……” 你摸着脑袋怀疑他是不是在吹牛，怎么可能！然后过了几个星期，你把代码重写了好多遍之后，真的发现只需要两行！这时候他才会微笑着点点头，一副龟仙人的味道。就是这样的教育，让我能够在短短几个星期之内，完成Google一个小组的人花几年也没法完成的项目。看过我写的代码，你也许会理解这句&lt;a href=&quot;http://zh.wikipedia.org/wiki/%E5%AE%89%E6%89%98%E4%B8%87%C2%B7%E5%BE%B7%E5%9C%A3%E5%9F%83%E5%85%8B%E7%B5%AE%E4%BD%A9%E9%87%8C&quot;&gt;《小王子》作者&lt;/a&gt;的&lt;a href=&quot;http://www.brainyquote.com/quotes/quotes/a/antoinedes121910.html&quot;&gt;名言&lt;/a&gt;：“一个设计师知道他达到了完美，并不是当他不能再加进任何东西，而是当没有任何东西可以被去掉。”&lt;/p&gt;&lt;p&gt;如果你跟我一样关心设计，却发现身边的人喜欢显示自己能搞懂复杂的东西，跟你说容易的东西都是菜鸟用的，那么你需要一个朋友。书籍是人类最好的朋友，因为它的作者可以跨越时间和空间的限制，给你最需要的支持和鼓励。这就是当我阅读这本1988年出版的《&lt;a href=&quot;http://www.amazon.com/Design-Everyday-Things-Revised-Expanded-ebook/dp/B00E257T6C&quot;&gt;The Design of Everyday Things&lt;/a&gt;》（简称DOET）时的感觉。我觉得，终于有人懂我了！有趣的是，它的作者 Don Norman 曾经是 Apple Fellow，也是《&lt;a href=&quot;http://web.mit.edu/~simsong/www/ugh.pdf&quot;&gt;The Unix-Haters Handbook&lt;/a&gt;》一书序言的作者。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;http://www.yinwang.org/images/doet.jpg&quot;&gt;&lt;/p&gt;&lt;p&gt;DOET 不但包含并且支持了我的博文《&lt;a href=&quot;http://www.yinwang.org/blog-cn/2014/04/11/hacker-culture&quot;&gt;黑客文化的精髓&lt;/a&gt;》以及《&lt;a href=&quot;http://www.yinwang.org/blog-cn/2014/01/25/pl-and&quot;&gt;程序语言与……&lt;/a&gt;》里的基本观点，而且提出了比《&lt;a href=&quot;http://www.yinwang.org/blog-cn/2012/05/18/user-friendliness&quot;&gt;什么是“对用户友好”&lt;/a&gt;》更精辟可行的解决方案。&lt;/p&gt;&lt;p&gt;我觉得这应该是每个程序员必读的书籍。为什么每个程序员必读呢？因为虽然这本书是设计类专业的必读书籍，而计算机及其编程语言和工具，其实才是作者指出的缺乏设计思想的“重灾区”。看了它，你会发现很多所谓的“人为错误”，其实是工具的设计不合理造成的。一个设计良好的工具，应该只需要很少量的文档甚至不需要文档。这本书将提供给你改进一切事物的原则和灵感。你会恢复你的人性。&lt;/p&gt;&lt;p&gt;值得一提的是，虽然 Don Norman 曾经是 Apple Fellow，但我觉得 Apple 产品设计的人性化程度与 Norman 大叔的思维高度还是有一定的差距的。因为我看了这书之后，立马发现了iPhone的一些设计问题。&lt;/p&gt;&lt;p&gt;如果你跟我一样不想用眼睛看书，可以到 Audible 买本&lt;a href=&quot;http://www.audible.com/pd/Science-Technology/The-Design-of-Everyday-Things-Audiobook/B005I5MDGQ&quot;&gt;有声书&lt;/a&gt;来听。&lt;/p&gt;</description>
<author>yinwang0</author>
<guid isPermaLink="false">design</guid>
<pubDate>Tue, 17 Mar 2015 00:00:00 +0800</pubDate>
</item>
<item>
<title>我为什么在乎这一个A+</title>
<link>http://yinwang.org/blog-cn/2015/03/13/a-plus</link>
<description>&lt;p&gt;
  　　
  &lt;/p&gt;&lt;h2&gt;我为什么在乎这一个A+&lt;/h2&gt;&lt;p&gt;我知道有些人至今仍然嘲笑和鄙视我，因为我曾经说过，我在Dan Friedman的两门课程B521（程序语言理论)和B621（高级程序语言理论）都得了A+。只要提到我，他们就会拿出这个把柄来，好像我是一个只在乎分数的肤浅的人。实际上我觉得这些人只是为了鄙视而鄙视，所以他们发现貌似一个把柄，也不搞清楚Dan Friedman是谁，也不搞清楚这个A+的分量，拿着半截就开跑，抓住不放了。所以即使我没提过这分数的事情，他们一样会找到其它话题来损我。我一直都懒得回应这些人的言论，不过今天我有兴致显示一下自己的价值，所以想花点时间告诉你，这个A+到底意味着什么。&lt;/p&gt;&lt;p&gt;从我的&lt;a href=&quot;http://www.yinwang.org/blog-cn/2014/01/04/authority&quot;&gt;人生历史&lt;/a&gt;里面，你应该很明显的看出来，课程，考试，分数，名校，权威，事业，成就，贡献，以至于图灵奖，诺贝尔奖，对于我来说真的什么都不是。你觉得一个在乎这些东西的人，会以优秀的成绩从清华，Cornell，Indiana三所大学退学吗？在漫长的学术生涯中，我上过许多的课程，很多最后结果都是A或者A+，也有几门课的分数低到C。因为我从来不觉得任何人有资格出题来考我，所以自上大学以来，我给自己定的标准就是“及格万岁”。我是一个非常不喜欢上课的人，我觉得普通的课堂讲座本身就是一种极其低效的教学方式，所以一旦觉得老师水平不够或者不懂教学就开始翘课，自己看书自学。所以，最后无论什么分数都不能衡量我的价值，反而有时候觉得高分是对我价值的侮辱——本来有时候老师教的，课本上的东西就不对，得高分意味着我得跟他们错得一样。然而，我为什么唯独在乎在一个非名校，“非名师”手里上的这门课程，并且愿意告诉你我在里面的成绩呢？&lt;/p&gt;&lt;p&gt;其实，这个分数的意义远远不止是一个A+，它涵盖的内容可能超乎你的想象。也许你可以从一个很小的例子看出它到底意味着什么。在课程进行到一半的时候，我花了一个星期的时间，独立解决了曾经困扰程序语言领域十多年的难题——CPS变换。CPS变换有什么用呢？如果你写过Node.js或者其它类似的东西，就知道所谓“call back hell”的代码样式，其本质就是程序语言专家所谓的“CPS”（continuation-passing style）。“CPS变换”就是可以自动把代码变换成那种样式的过程，它在本质上就是一个编译器。实际上有些函数式语言的编译器（比如SML），其中最重要的过程就是CPS变换。CPS变换之后，你可以掌握代码中的“控制流”，实现所谓“超轻量线程”，进而可以实现最近很流行的，所谓“大规模并发”。所以你看到了，这些很流行的概念，在程序语言专家看来，并不是什么稀奇的东西，甚至不是新的想法。&lt;/p&gt;&lt;p&gt;在这十几年里面，有众多的世界级专家参与过这个问题的研究，包括程序语言领域的鼻祖之一，爱丁堡大学教授，英国皇家学会院士&lt;a href=&quot;http://en.wikipedia.org/wiki/Gordon_Plotkin&quot;&gt;Gordon Plotkin&lt;/a&gt;，天才的丹麦Aarhus大学教授Olivier Danvy，CMU的Andrzej Filinski（现在DIKU），Indiana的Dan Friedman以及他的得意门生，天才的Matthias Felleisen，Felleisen的得意门生，天才的Amr Sabry（我的导师），普林斯顿大学教授Andrew Appel（编译器教材“&lt;a href=&quot;http://www.amazon.com/Modern-Compiler-Implementation-Andrew-Appel/dp/0521607655&quot;&gt;虎书&lt;/a&gt;”的作者）。这些人为这个话题发表了不知道多少论文，Andrew Appel还为此专门写了一本书，叫做《&lt;a href=&quot;http://www.amazon.com/Compiling-Continuations-Andrew-W-Appel/dp/052103311X&quot;&gt;Compiling with Continuations&lt;/a&gt;》。我之所以会去解决这个问题，是因为Friedman耍老顽童的花样，别出心裁地把这个问题作为了一道附加题目放进了B521的作业里。我不知道这个问题有如此之难，所以愣头愣脑，真把它当成作业题给解决了。按照作业的“道德规范”，完全从问题出发，不看书不看论文不查网络，全凭自己的头脑，在一个星期之内，把代码反反复复重写了几十次，最后得到了最优的结果。这就是所谓“王垠40行代码”的含义，虽然最后只剩下40行，然而却不知道删掉了多少。为了这40行代码，一个人七天，一群人十年，我想你应该知道这是什么概念。&lt;/p&gt;&lt;p&gt;当我最后把代码交给Dan Friedman的时候，他不相信我的代码是正确的，因为历史上有许多的学生声称做出了这道题目，然而他们几乎全都是错的，或者采用了效率很低的做法。只有深入到精髓，才会明白怎么写出这些代码。那么多大牛花了那么多年工夫才研究清楚，所以Friedman把这问题放在作业里面，其实根本就没指望有人能够解决。所以自然，他很难相信任何人能够做出这道题目。那天Friedman用惊讶又怀疑的眼神看着我，然后给了我一篇30多页的&lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.46.84&quot;&gt;论文&lt;/a&gt;。这篇论文是历史上这个问题的一个重大突破，作者是他的好朋友，Danvy和Filinsky。可是这论文写得含混晦涩，所以我花了超过一个月才琢磨清楚这篇论文是怎么回事，我至今被那些公式弄得眼花缭乱。可是最后我发现，我的自己写出来的代码完全的实现了它最后的思想，而且还要更加优雅。所以当最后我在班上讲解这片代码是怎么回事的时候，Friedman对大家说：“你们可要听仔细了，这个值100美元！”&lt;/p&gt;&lt;p&gt;我的名字叫做王垠（父亲起名含义是谐音“亡垠”，无边无垠的意思），所以我将会永不停息的完善自己，永远不会拿某一个东西自居。解决这个难题只是对我这个人内在品质的一种反映而已，而且它只是我在B521做出的好几个“课外练习”的其中一个。在短短一学期的时间里，我还进行了其它几个重量级的练习，包括重新实现miniKanren语言，加入constraint logic programming功能和一种非常强大的逻辑逆（negation）操作符，等等。这些练习，全都是独立依靠自己领悟摸索完成，没有查阅任何书籍和论文资料。从这些练习里面，我获得了让我受益终生的独立思考能力。也就是这种能力，让我可以在Google，Coverity之类的公司，轻松解决其他人咋咋呼呼，认为不可能完成的任务。这就是为什么我会讲这个课程的&lt;a href=&quot;http://www.yinwang.org/blog-cn/2012/07/04/dan-friedman&quot;&gt;故事&lt;/a&gt;，并且告诉你我得了A+。&lt;/p&gt;&lt;p&gt;有趣的是，学期结束的时候，成绩单上出现的分数其实是I（Incomplete）。这种成绩表示有课程任务没有完成，如果在一年之内不弥补，就会变成F（不及格）。我很纳闷，发信去问Friedman。他回答说：“对不起，是秘书搞错了！” 然后急忙发信给秘书说：“这个人的分数应该是A+！实际上如果可能的话，我希望给他A+++++++！”&lt;/p&gt;&lt;p&gt;现在你还觉得我是因为肤浅才告诉你这个A+分数吗？B521教会我的，是一生最重要的东西，它让我真正的理解了什么叫做“简单”，它使得我去追寻它。它赋予我的独立思考能力，继续在帮助我用巧妙简单的方法解决其他人望而却步的问题。这不是一个普通的A+，这是一个把我送上世界巅峰，给予我勇气和自由思想的A+。&lt;/p&gt;&lt;p&gt;就像爱因斯坦说的，任何一个傻瓜都可以把问题搞复杂，你需要一点天才，还有很多勇气，才能达到简单。很多牛人用“简单”来标榜自己设计的东西，然而我发现他们对简单的理解其实很肤浅。大部分时候他们用一种类似“皇帝的新装”的心理技巧——你如果不能理解他的东西，他就说你是傻瓜或者菜鸟，不能理解这种简单。所以没有人敢说他们设计的东西太复杂。&lt;/p&gt;&lt;p&gt;你觉得世界上有几个人能够在B521上得A+呢？谦虚是一种美德，不要随便评判别人，然而当看到这么多大牛都那么不谦虚，耀武扬威的，很多人用他们作为评判其他人的依据，所以我只好冒着评判他们的风险，告诉你一些事实。其实Donald Knuth, Dennis Ritchie, Bjarne Stroustrup, Guido van Rossum, Brendan Eich, Linus Torvalds, Rob Pike, ... 这些很多人仰慕的大牛，如果上B521肯定是连A都拿不到的。有些甚至不能及格，因为有些人根本不知道他们在干什么，设计出一堆复杂的垃圾，然后仗着自己的威望和强权迫使你去“学习”。其实我对计算机的理解跟这些大牛们，早就不在一个数量级上了。我心里有数他们该得什么分数，你们自己猜猜吧。&lt;/p&gt;&lt;p&gt;本来不想这么赤裸裸的跟人比较的，然而我发现我的话语权和我对事物的认识深度比起来，实在相差太多。当我说到一些事情的时候，经常有人抬出这些人的语录来压制，说得好像圣经似的，对我各种评判，所以觉得有必要特此说明一下。这些大牛在我心目中真的一点权威都没有的，我反而清楚他们肚子里到底有多少货，思维方式有哪些误区和局限性。&lt;/p&gt;&lt;p&gt;也许我现在可以毫不担心的告诉你了，我在&lt;a href=&quot;http://en.wikipedia.org/wiki/R._Kent_Dybvig&quot;&gt;Kent Dybvig&lt;/a&gt;的编译器课程上得的也是A+。Kent恐怕是世界上最厉害的编译器作者，他几乎从来不给人A+，而我恐怕是他20多年来最厉害的一个学生。我们做了一个Scheme编译器，它的难度和工作量，是C语言编译器的两倍以上。由于我喜欢别出心裁，不按他的写法，我的课堂编译器的某些方面，其实超越了他的Chez Scheme。比如，我的编译器曾一度生成比他更高效的X64机器指令。然而Kent很会背地里偷学武功，闷声发大财。据课程助教说，Kent有几次偷偷在我的代码上做“侦探工作”挺久…… 再加上他几十年深藏不露的&lt;a href=&quot;http://www.cs.indiana.edu/~dyb/pubs/hocs.pdf&quot;&gt;经验&lt;/a&gt;，所以他现在恐怕仍然比我强 :)&lt;/p&gt;</description>
<author>yinwang0</author>
<guid isPermaLink="false">a-plus</guid>
<pubDate>Fri, 13 Mar 2015 00:00:00 +0800</pubDate>
</item>
<item>
<title>不要做聪明人</title>
<link>http://yinwang.org/blog-cn/2015/03/08/be-a-fool</link>
<description>&lt;p&gt;
  　　
  &lt;/p&gt;&lt;h2&gt;不要做聪明人&lt;/h2&gt;&lt;p&gt;世界上有三种人：聪明人，傻瓜，傻b。傻瓜和傻b的区别是，傻b是令人讨厌的傻瓜。很多人想做聪明人，比其他人都聪明，结果他们变成了傻b。为什么会这样呢？&lt;/p&gt;&lt;p&gt;其实很多人所谓的“聪明”，要么是能够高效的完成一些机械化的任务，要么是能够高效的绕过一些前人的设计失误。他们的所谓“知识”，建立在一堆历史遗留的糟粕之上，他们以记得住这些脆弱的“知识”为豪。所以，这些人连聪明是什么都不知道，又怎么可能成为聪明人？有些人很傻，只会死记硬背，却自认为很聪明，所以他们让人厌恶，进而升级成为傻b。&lt;/p&gt;&lt;p&gt;如果你想做聪明人，那你往往不可能成为聪明人。想做聪明人的欲望，很容易让人变成傻b。有些人随时都在担心自己不如别人聪明，随时都在比较，害怕别人比他更聪明。纳什（John Nash）因为一辈子都在跟人计较谁更聪明，结果发疯了。他还算好点的，很多“天才”因为跟人计较谁更聪明，最后自杀了。世界上最傻的事情，就是拿自己跟别人作比较。跟人比较的结果，最终都是不快乐，甚至给自己的身心带来伤害。&lt;/p&gt;&lt;p&gt;想做聪明人的欲望，让人变得喜欢争执，喜欢咄咄逼人的想证明自己是对的。它也使人变得固执和盲从，仓促而盲目的相信或者排斥一些事物。有趣的是，这些人选择相信或者排斥的条件，往往在于最后的结果是否能让自己显得聪明。想做聪明人的人，往往只关心自己知道的那点东西，发现别人貌似不懂就穷追猛打，抓住小辫子不放，教育这些不懂的人！却没发现自己有多么无知。因为想证明自己比别人聪明，所以解决问题的时候，总喜欢选择更困难，更复杂，看似更高深的解决方案。结果不但劳神费力，还阻碍了技术的简化和进步。&lt;/p&gt;&lt;p&gt;聪明是可遇而不可求的。聪明可能是一种结果，一种事实，却不可以是一种欲望，一种目标。想要成为聪明人的欲望，多半会让人变成傻b。世界上所有试图成为聪明人的人，终究都会悟出一个道理。他们发现，自己更愿意做一个傻瓜。很多人所谓的“聪明才智”，越来越多的被机器所代替和超越。随着科技的进步，人们耐以生存所需要的死知识，会越来越少。这个世界越来越不需要聪明人，它更需要的是可爱的人。傻瓜往往很可爱。&lt;/p&gt;&lt;p&gt;未来的世界属于傻瓜。所以，我觉得每个人都应该放弃做聪明人的企图，反而应该有做傻瓜的欲望。做一个傻瓜，才能给你真正意义上的实惠和幸福。&lt;/p&gt;</description>
<author>yinwang0</author>
<guid isPermaLink="false">be-a-fool</guid>
<pubDate>Sun, 08 Mar 2015 00:00:00 +0800</pubDate>
</item>
<item>
<title>怎样尊重一个程序员</title>
<link>http://yinwang.org/blog-cn/2015/03/03/how-to-respect-a-programmer</link>
<description>&lt;p&gt;
  　　
  &lt;/p&gt;&lt;h2&gt;怎样尊重一个程序员&lt;/h2&gt;&lt;p&gt;得知一位久违的同学来到了旧金山湾区，然而我见到他时，这人正处于一生中最痛苦的时期。他告诉我，自己任职的公司在他加入之前和之后，判若两人。录取的时候公司对他说，我们对你在实习期间的表现和学术背景非常满意，你不用面试，甚至不用毕业拿学位，直接就可以加入我们公司成为正式员工。然而短短一年后的今天，这位同学已经完全感觉不到公司对自己技能的尊重。Manager让他做一些乱七八糟没技术含量的事情，还抱怨说他做事太慢，并且在他的evaluation上很是写了一笔。在人格尊严和工作安全感的双重打击之下，这位同学压力非常大，周末经常偷偷地加班，仍然无法让manager满意。&lt;/p&gt;&lt;p&gt;我很了解这位同学的能力，在任何一流公司任职，肯定是绰绰有余了。他的名字我当然保密，然而他所任职的公司因为太过嚣张，我不得不直接指出来——这就是被很多人向往得像天堂一样的地方，Google。这位同学所描述的遭遇，跟我几年前在Google的实习经历如出一辙。我仍然记得，Google的队友在旁边看着我用Emacs，用小学老师似的口气对我说：“按Ctrl-k！” 我仍然记得，在提交队友完全无法写出来的高难度代码时，被指责和嘲笑不会用Perforce。我仍然记得，吃饭时同事们对所谓“Google牛人”眉飞色舞的艳羡。我仍然记得，最后我一个人做出整个团队做梦都做不出来的项目的时候，有人发出沉闷的咆哮：“快——写——测——试！” ……&lt;/p&gt;&lt;p&gt;我的这位同学也算得上本领域顶尖的专家了。如此的践踏一个专家的价值，用肤浅的标准来评判和对待他们，Google并不是唯一一个这样的公司。我之前任职的好几个公司，或多或少都存在类似的问题。很多时候也不一定是公司管理层无端施加压力，而是程序员之间互斗的厉害，互相评判，伤害自尊。从最近&lt;a href=&quot;http://arstechnica.com/business/2015/01/linus-torvalds-on-why-he-isnt-nice-i-dont-care-about-you&quot;&gt;Linus Torvalds&lt;/a&gt;在演讲现场公然对观众无理，你可以看出这种只关心技术，不尊重人的思潮，在程序员的社区里是非常普及的。&lt;/p&gt;&lt;p&gt;后来我发现，并不是程序员故意想要藐视对方或者互相攻击，而是他们真的不明白什么叫做“尊重”，他们不知道如何说话才可以不伤害另一个程序员，所以有时不小心就让人怒火中烧。所以说，尊重他人其实是一个“技术问题”，而不是有心就可以做到的。因为这个原因，我想在下文里从心理和技术角度出发，指出IT业界不尊重人现象的起源，同时提出几点建议，告诉人们如何真正的尊重一个程序员。我希望这些建议对公司的管理层有借鉴意义，也希望它们能给与正在经受同样痛苦的程序员们一些精神上的鼓励。&lt;/p&gt;&lt;p&gt;我觉得为了建设一个程序员之间互相尊重的公司文化，应该注意以下几个要点。&lt;/p&gt;&lt;h3&gt;认识和承认计算机系统里的历史遗留糟粕&lt;/h3&gt;&lt;p&gt;很多不尊重人现象的起源，都是因为某些人偏执的相信某种技术就是世界上最好的，每个人都必须知道，否则他就不是一个合格的程序员。这种现象在Unix（Linux）的世界尤为普遍。Unix系统的鼓吹者们（我曾经是其中之一）喜欢到处布道，告诉你其它系统的设计有多蠢，你应该遵从Unix的“哲学”。他们仿佛认为Unix就是世界终极的操作系统，然而事实却是，Unix是一个设计非常糟糕的系统。它似乎故意被设计为难学难用，容易犯错，却美其名曰“强大”，“灵活”。眼界开阔一点的程序员都知道，Unix的设计者其实基本不懂设计，他们并不是世界上最好的程序员，却有一点做得很成功，那就是他们很会制造宗教，煽动人们的盲从心理。Unix设计者把自己的设计失误推在用户身上，让用户觉得学不会或者搞错了都是自己的错。&lt;/p&gt;&lt;p&gt;如果你对计算机科学理解到一定程度，就会发现我们其实仍然生活在计算机的石器时代。特别是软件系统，建立在一堆历史遗留的糟糕设计之上。各种蹩脚脑残的操作系统（比如Unix，Linux），程序语言（比如C++，JavaScript，PHP，Go)，数据库，编辑器，版本控制工具，…… 时常困扰着我们，这就是为什么你需要那么多的所谓“经验”和“知识”。然而，很多IT公司不喜欢承认这一点，他们一向以来的作风是“一切都是程序员的错！”，“作为程序员，你应该知道这些！” 这就造成了一种“皇帝的新装现象”——大家都不喜欢用一些设计恶劣的工具，却都怕别人嘲笑或者怀疑自己的能力，所以总是喜欢显示自己“会用”，“能学”，而没有人敢说它难用，敢指出设计者的失误。&lt;/p&gt;&lt;p&gt;我这个人呢，就是这种“&lt;a href=&quot;http://www.yinwang.org/blog-cn/2014/04/11/hacker-culture&quot;&gt;黑客文化&lt;/a&gt;”的一个反例。我所受到的多元化教育，让我从这些偏激盲从，教条主义的心理里面跳了出来。每当有人因为不会某种工具或者语言来请教我时，我总是很轻松的调侃这工具的设计者，然后告诉他，你没理由知道这些破玩意儿，但其实它就是这么回事。然后我一针见血的告诉他这东西怎么回事，怎么用，是哪些设计缺陷导致了我们现在的诡异用法…… 我觉得所有的IT从业人员对于这些工具，都应该是这样的调侃态度。只有这样，软件行业才会得到实质性的进步，而不是被一些自虐的设计所困扰，造成思维枷锁。&lt;/p&gt;&lt;p&gt;总之，这是一个非常重要的“态度问题”。虽然在现阶段，我们有必要知道如何绕过一些蹩脚的工具，利用它们来完成自己的任务。然而在此同时，我们必须正视和承认这些工具的恶劣本质，而不能拿它们当教条，把什么事都怪罪于程序员。只有分清工具设计者的失误和程序员自己的失误，不把工具的设计失误怪罪于程序员，我们才能有效地尊重程序员们的智商，鼓励他们做出简单，优雅，完善的产品。&lt;/p&gt;&lt;h3&gt;分清精髓知识和表面知识，不要太拿经验当回事&lt;/h3&gt;&lt;p&gt;在任何领域，都只有少数知识是精髓的，另外大部分都是表面的，肤浅的，是从精髓知识衍生出来的。精髓知识和表面知识都是有用的，然而它们的分量和重要性却是不一样的。所以必须区分精髓知识和表面知识，不能混为一谈，对待它们的态度应该是不一样的。由于表面知识基本是死的，而且很容易从精髓知识推导衍生出来。我们不应该因为自己知道很多表面知识，就自以为比掌握了精髓知识的人还要强。不应该因为别人不知道某些表面知识，就以为自己高人一等。&lt;/p&gt;&lt;p&gt;IT公司经常有这样的人，以为精通一些看似复杂的命令行，或者某些难用的程序语言就很了不起似的。他们如果听说你不知道某个命令的用法，那简直就像法国人不知道拿破仑，美国人不知道华盛顿一样。这些人没有发现，自己身边有些同事其实掌握着精髓的知识，他们完全有能力从自己已有的知识，衍生制造出所有这些工具，而不只是使用它们，甚至设计得更加完善和方便易用。这种能够设计制造出更好工具的人，往往身负更加重要的任务，所以他们往往会在被现有工具的用法迷惑的时候，非常谦虚的请同事帮助解决，大胆的承认自己的糊涂。&lt;/p&gt;&lt;p&gt;如果你是这个精通工具用法的人，切不可以把同事的谦虚请求当成可以显摆自己“资历”的时候。这同事往往真的是在“不耻下问”。他并不是搞不懂，而是根本不屑于，也没有时间去考虑这种低级问题。他的迷惑，往往来源于工具设计者的失误。他很清楚这一点，他也知道自己的技术水平其实是高于这工具的设计者的。然而为了礼貌，他经常不直接批评这工具的设计，而是谦虚的责怪自己。所以同事向你“虚心请教”，完全是为了制造一种友好融洽的气氛，这样可以节省下时间来干真正重要的事情。这种虚心并不等于他在膜拜你，承认自己的技术能力不如你。&lt;/p&gt;&lt;p&gt;所以正确的对待方式应该是诚恳的表示对这种迷惑的理解，并且坦率的承认工具设计上的不合理，蹩脚之处。如果你能够以这种谦和的态度，而不是自以为专家的态度，同事会高兴地从你这里“学到”他需要的，肤浅的死知识，并且记住它，避免下次再为这种无聊事来打扰你。如果你做出一副“天下只有我知道这奇技淫巧”的态度，同事往往会对你，连同这工具一起产生鄙视的情绪。他下次会照样记不住这东西的用法，然而他却再也不会来找你帮忙，而是一拖再拖。&lt;/p&gt;&lt;h3&gt;不要自以为聪明，不要评判别人的智商和能力&lt;/h3&gt;&lt;p&gt;在IT公司里，总是有很多人觉得自己聪明，想显示自己比别人聪明。这种人似乎随时都在评判（judge）别人，你说的任何话，不管认真的还是开玩笑的，都会被他们拿去作为评估你智商和能力的依据。&lt;/p&gt;&lt;p&gt;有时候你写了一些代码，自己知道时间不够，可是当时有更重要的事情要做，所以打算以后再改进。如果你提交代码时被这种人看到了，他们就会坚定地认为你一辈子只能写出那样的代码。这就是所谓“wishful thinking”，人只能看到他希望看到的东西。这种人随时都在希望自己比别人聪明，所以他们随时都在监听别人显得不如他聪明的时候，而对别人比他高明的时候视而不见。他们只能看到别人疏忽的时候，因为那是可以证明他们高人一等的有利证据。&lt;/p&gt;&lt;p&gt;当然，谁会喜欢这样的人呢，可是他们在IT公司里相当的普遍。你不敢跟他们说话，特别是不敢开玩笑，因为他们会把你稀里糊涂的玩笑话全部作为你智商低下或者经验不足的证据。你不敢问他们问题，因为他们会认为你问问题，说明你不懂！我发现具有这种心理的人，一般潜意识里都存在着自卑。他们有某些方面（包括智力在内）不如别人，所以总是找机会显得高人一等。我还没有想出可以纠正这种心理问题的有效方法，但如我上节所说，意识到整个行业，包括你仰慕的鼻祖们，其实都不懂很多东西，都是混饭吃的，是一个有效的放松这种心理的手段。&lt;/p&gt;&lt;p&gt;有时候我喜欢自嘲，对人说：“我们这行业的祖先做了这么多BUG来让我们修补。现在你做了一坨屎，我也做了一坨屎，我的屎貌似比你的屎香一点。”这样一来，不但显示出心理的平等和尊重，而且避免了因为谦虚而让对方产生高人一等的情绪。说真的，做这行根本不需要很高的智力，所以最好是完全放弃对人智力的判断。你不比任何人更聪明，也不比他们笨。&lt;/p&gt;&lt;h3&gt;解释高级意图，不要使用低级命令&lt;/h3&gt;&lt;p&gt;随时都要记住，同事和下属是跟你智力相当的人。他们是通情达理的人，然而却不会简单地服从你的低级命令。像我在Google的队友的做法，就是一个很好的反面教材。其实这位Googler只是想告诉我：“删掉这行文本，然后改成这样……” 就是如此一个简单的事情，然而她却故弄玄虚，不直接告诉我这个“高级意图”，而是使用非常低级的指令：“按Ctrl-k！……” 语气像是在对一个不懂事的小学生说话，好像自己懂很多，别人什么都不知道似的。&lt;/p&gt;&lt;p&gt;有哪个Emacs用户不知道Ctrl-k是删掉一行字呢，况且你现在面对的其实是一个资深Emacs用户。我想大家都看出来这里的问题了吧。这样的低级命令不但逻辑不清楚，而且是对另一个人的智力的严重侮辱。你当我是什么啊？猴子？如果这位Googler表明自己的高级意图，就会很容易在心理上和逻辑上让人接受，比如她可以说：“配置文件的这行应该删掉，改成……”&lt;/p&gt;&lt;p&gt;在项目管理的时候也需要注意。在让人做某一件事之前，应该先解释为什么要做这件事，以及它的重要性。这样才能让人理解，才能尊重程序员的智商。&lt;/p&gt;&lt;h3&gt;不要期望新人向自己学习&lt;/h3&gt;&lt;p&gt;很多IT公司喜欢把新人当初学者，期望他们“从新的起跑线出发”，向自己“学习”。比如，Google把新员工叫做“Noogler”（Newbie Googler的意思），甚至给他们发一种特殊的螺旋桨帽子，其寓意在于告诉他们，小屁孩要谦虚，要向伟大的Google学习，将来才可以飞黄腾达。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;http://www.yinwang.org/images/noogler-hat.jpg&quot; width=&quot;60%&quot;&gt;&lt;/p&gt;&lt;p&gt;这其实是非常错误的作法，因为它完全不尊重新员工早已具备的背景知识，把自己的地位强加于他们头上。并不是你说“新的起跑线”就真的可以把人的过去都抹杀了的。新人不了解你们的代码结构和工程方式，并不等于你们的方式就会先进一些。Google里面真的有很多值得学习的东西吗？学校的教育真的一文不值吗？其实恰恰相反。我可以坦然的说，我从自己的教授身上学会了最精髓的知识，而从Google得到的，只是一些很肤浅的，死记硬背就可以掌握的技能，而且其中有挺多其实是糟粕。我在Google做出的所有创新成果，全都是从学校获得的精髓知识的衍生物。很多PhD学生鄙视Google，就是因为Google不但自己技术平庸，反倒喜欢把自己包装成最先进的，超越其它公司和学校的，并且嚣张的期望别人向他们“学习”。&lt;/p&gt;&lt;p&gt;一个真正尊重人才的公司会去了解，尊重和发挥新人从外界带来的特殊技能，施展他们特有的长处，而不是一味期望他们向自己“学习”。只有这样，我们才能保持这些锐利武器的棱角，在激烈的竞争中让自己立于不败之地。如果你一味的让新人“学习”，而无视他们特有的长处，最后就不免沦为平庸。&lt;/p&gt;&lt;h3&gt;不要以老师自居，分清“学习”和“了解”&lt;/h3&gt;&lt;p&gt;如上文所说，IT行业的很多所谓“知识”，只不过是一些奇技淫巧，用以绕过前人设计上的失误。所以遇到别人不知道一些东西的时候，请不要以为你“教会”了别人什么东西，不要以为自己可以当老师了。以老师自居，使用一些像“跟我学”一类的语言，其实是一种居高临下，不尊重人的行为。&lt;/p&gt;&lt;p&gt;人们很喜欢在获得了信息的时候用“学习”这个词，然而我觉得这个词被滥用了。我们应该分清两种情况：“学习”和“了解”。前者指你通过别人的指点和自己的理解，获得了精髓的，不能轻易制造出来的知识。后者只是指你“了解”了原来不知道的一些事情。举个例子，如果有人把一件物品放在了某个你不知道的地方，你找不到，问他，然后他告诉你了。这种信息的获取，显然不叫“学习”，这种信息也不叫做“知识”。&lt;/p&gt;&lt;p&gt;然而，IT行业很多时候所谓的“学习”，就是类似这种情况。比如，有人写了一些代码，设计了一些框架模块。有人不知道怎么用，然后有人告诉他了。很多人把这种情况称为“学习”，这其实是对人的不尊重。这跟有人告诉你他把东西放在哪里了，是同样性质的。这样的代码和设计，我也可以做，甚至做得更好，凭什么你说我在向你学习呢？我只是了解了一下而已。&lt;/p&gt;&lt;p&gt;所谓学习，必须是更加高级的知识和技能，必须有一种“有收获”，“有提高”的感觉。简单的信息获取不能叫做“学习”，只能叫做“了解”。分清“了解”和“学习”，不以老师自居，是尊重人的一个重要表现。&lt;/p&gt;&lt;h3&gt;明确自己的要求，不要使用指责的语气&lt;/h3&gt;&lt;p&gt;有些人很怪异，他根本没告诉过你他想要什么，有什么特别的要求，可他潜意识里假设已经告诉你了。到了后来，他发现你的作法不符合要求，于是严厉指责你没有按照他“心目中的要求”办事。这种现象不止限于程序员，而且包括日常生活中的普通人。举个例子，我妈就是这种人的典型，所以我以前在家生活经常很辛苦。她心目中有一套“正确”的做事方式，如果你没猜出来就会挨骂。你为了避免挨骂，干脆什么事都不要做，然后她又会说你懒，所以你就左右不是人 :)&lt;/p&gt;&lt;p&gt;IT公司里面也有挺多这样的人，他们假设有些信息他已经告诉你了，而其实根本没告诉你。到了后来，他们开始指责你没有按照要求做事。有些极其奇葩的公司，里面的程序员不但喜欢以老师自居，而且他们“传授”你“知识”的主要方式是指责。他们事先不告诉你任何规则，然后只在你违反的时候来责备你。我曾经在这样一个公司待过，名字就不提了。&lt;/p&gt;&lt;p&gt;现在举一个具体的场景例子：&lt;/p&gt;&lt;p&gt;A: 你push到master了？&lt;br&gt;
B: 是啊？怎么了？&lt;br&gt;
A: 不准push到master！只能用pull request！&lt;br&gt;
B: 可是你们之前没告诉过我啊……&lt;br&gt;
A: 现在你知道了？！&lt;/p&gt;&lt;p&gt;注意到了吗？这不是一个技术问题，而是一个礼节（etiquette）问题。你没有事先告诉别人一些规则，就不该用怪罪的语气来对人说话，况且你的规则还不一定总是对的。所以我现在提醒各位IT公司，在技术上的某些特殊要求必须事先提出来，确保程序员知道并且理解。如果没有事先提出，就不要怪别人没按要求做，因为这是非常伤害人自尊的作法。其实，在任何时候都不应该使用指责的语气，它不但对解决问题没有任何正面作用，而且会恶化人际关系，最终导致更加严重的后果。&lt;/p&gt;&lt;h3&gt;程序员的工作量不可用时间衡量&lt;/h3&gt;&lt;p&gt;很多IT公司管理层不懂得如何估算程序员的工作量，所以用他们坐在自己位置上工作的时间来估算。如果你能力很强，在很短的时间内把最困难的问题解决了，接下来他们不会让你闲着，而会让你做另外一些很低级的活。这是很不合理的作法。打个比方，能力强的员工就像一辆F1赛车，马力和速度是其他人的几十倍。当然，普通人需要很长时间才能解决，甚至根本没法解决的问题，到他手里很快就化解掉了。这就像一辆F1赛车，眨眼工夫就跑完了别人需要很久的路程。如果你用时间来衡量工作量，那么这辆赛车跑完全程只需要很短时间，所以你算出来的工作量就比普通车子小很多。你能因此说赛车工作不够努力，要他快马再加鞭吗？这显然是不对的。&lt;/p&gt;&lt;p&gt;物理定律是这样：能量 = 功率 x 时间。工作量也应该是同样的计算方法。英明的，真正理解程序员的公司，就不会指望高水平的程序员不停地工作。高水平程序员由于经常能够另辟蹊径，一个就可以抵好几个甚至几十个普通程序员。他们处理的问题比常人的困难很多，费脑力多很多，当然他们需要更好的休息，保养，娱乐，…… 如果你让高水平的程序员太忙了，一刻都不停着，有趣有挑战性的事情做完了就让他们做一些低级无聊的事情，他们悟出这个道理之后，就会故意放慢速度，有时候明明很快做完了也会说没做完。与其这样，不如只期望他们工作短一点的时间，把事情做完就可以。&lt;/p&gt;&lt;p&gt;当然这并不是说初级的程序员就应该过量工作。编程是一项艰苦的脑力活动，超时超量的工作再加上压力，只会带来效率的低下，质量的降低。&lt;/p&gt;&lt;h3&gt;不要让其他人修补自己的BUG&lt;/h3&gt;&lt;p&gt;这个我已经在一篇专门的&lt;a href=&quot;http://www.yinwang.org/blog-cn/2015/02/20/other-peoples-bug&quot;&gt;文章&lt;/a&gt;里讨论过。让一个程序员修补另外一个程序员的BUG，不但是效率低下，而且是不尊重程序员个人价值的作法，应该尽量避免。&lt;/p&gt;&lt;p&gt;在软件行业，经常看到有的公司管理让一个人修补另一个人代码里的BUG。有时候有人写了一段代码，扔出来不管了，然后公司管理让其他工程师来修复它。我想告诉你们，这种方法会很失败。&lt;/p&gt;&lt;p&gt;首先，让一个人修复另一个人的BUG，是不尊重工程师个人技术的表现。久而久之会降低工程师的工作积极性，以至于失去有价值的员工。代码是人用心写出来的作品，就像艺术家的作品一样，它的质量牵挂着一个人的人格和尊严。如果一个人A写了代码，自己都不想修复里面的BUG，那说明A自己都认为他自己的代码是垃圾，不可救药。如果让另一个人B来修复A代码里的BUG，就相当于是让B来收拾其他人丢下的垃圾。可想而知，B在公司的眼里是什么样的地位，受到什么样的尊重。&lt;/p&gt;&lt;p&gt;其次，让一个人修复另一个人的BUG，是效率非常低下的作法。每个人都有自己写代码的风格和技巧，代码里面包含了一个人的思维方式。人很难不经解释理解别人的思想，所以不管这两人的编程技术高下，都会比较难理解。不能理解别人的代码，不能说明这人编程技术的任何方面。所以让一个人修补另一个人的BUG，无论这人技术多么高明，都会导致效率低下。有时候技术越是高的人，修补别人的BUG效率越是低，因为这人根本就写不出来如此糟糕的代码，所以他无法理解，觉得还不如推翻重写一遍。&lt;/p&gt;&lt;p&gt;当我在大学里做程序设计课程助教的时候，我发现如果学生的代码出了问题，你基本是没法简单的帮他们修复的。我的水平显然比学生的高出许多，然而我却经常根本看不懂，也不想看他们的代码，更不要说修复里面的BUG。就像上面提到的，有些人自己根本不知道自己在写什么，做出一堆垃圾来。看这样的代码跟吃屎的感觉差不多。对于这样的代码，你只能跟他们说这是不正确的。至于为什么不正确，你只能让他们自己去改，或者建议他们推翻重写。也许你能指出大致的方向和思路，然而深入到具体的细节却是不可能的，而且不应该是你的职责。这就是我的教授告诉我的做法：如果代码不能运行，直接打一个叉，不用解释，不用推敲，等他们自己把程序改好，或者实在没办法，来office hours找你，向你解释他们的思想。&lt;/p&gt;&lt;p&gt;如果你明白我在说什么，从今天起就对自己的代码负起责任来，不要再让其它人修补自己的BUG，不要再修补其他人的BUG。如果有人离开公司，必须要有人修补他遗留下来的BUG，那么说话应该特别特别的小心。你必须指出需要他帮忙的特殊原因，强调这件事本来不是他的错，本来是不应该他来做的，但是有人走了，没有办法，并且诚恳的为此类事情的发生表示歉意。只有这样，程序员才会心甘情愿的在这种特殊关头，修补另外一个人的BUG。&lt;/p&gt;&lt;h3&gt;不要嚷着要别人写测试&lt;/h3&gt;&lt;p&gt;在很多程序员的脑子里，所谓的“流程”和“测试”，比真正解决问题的代码还重要。他们跟你说起这些，那真的叫正儿八经，义正言辞啊！所以有时候你很迷惑，这些人除了遵守这些按部就班的规矩，还知道些什么。大概没有能力的人都喜欢追究各种规矩吧，这样可以显得自己“没有功劳有苦劳”。这些人自己写的代码很平庸，不知道如何简单有效地解决困难的问题，却喜欢在别人提交代码让他review的时候叫喊：“测试很重要！覆盖很重要！你要再加一些测试才能通过我的review！”&lt;/p&gt;&lt;p&gt;本来code review是让他们帮忙发现可能存在的问题，有些人却仿佛把它作为了评判（judge）其他人能力，经验，甚至智商的机会。他们根本不明白别人代码的实质价值，就知道以一些表面现象来判断。我在Google实习，最后提交了质量和难度都非常高的代码，然而一些完全没能力写出这样代码的人，不但没表示出最基本的肯定，反而发出沉闷的咆哮：“快——写——测——试！” 你觉得我会高兴吗？&lt;/p&gt;&lt;p&gt;我并不否认测试的用处，然而很多人提起这些事情时候，语气和态度是非常不尊重，让人反感的。这些人不但没有为解决问题作出任何实质贡献，当有人提交解决方案的时候，他们没有表达对真正做出贡献的人的尊重和肯定，反而指责别人没写测试。好像比他高明的人解决了问题，他反倒才是那个有发言权的，可以评判你的代码质量似的：“我管你代码写得多好，我完全没能力写出来，但你没写测试就是不够专业。你懂不懂测试的重要性啊，还做程序员！”&lt;/p&gt;&lt;p&gt;人际交往的问题经常不在于你说了什么，而在于你是怎么说的。所以我的意思并不是说你不该建议写测试，然而建议就该有建议的语气和态度。因为你没有做实际的工作，所以一些礼貌用语，比如“请”，“可不可以”……是必须的。经常有人说话不注意语气和态度，让人反感，却以自己是工程师，不善于跟人说话为借口。永远要记住，你没有做事，说话就应该委婉，切不可使用光秃秃的祈使句，说得好像这事别人非做不可，不做就是不懂规矩一样。&lt;/p&gt;&lt;p&gt;礼貌的语言，跟人的职业完全没有关系。身为工程师，完全不能作为说话不礼貌的借口。&lt;/p&gt;&lt;h3&gt;关于Git的礼节&lt;/h3&gt;&lt;p&gt;Git是现在最流行的代码版本控制工具。用外行话说，Git就是一个代码的“仓库”或者“保管”，这样很多人修改了代码之后，可以知道是谁改了哪一块。其实不管什么工具，不管是编辑器，程序语言，还是版本控制工具，比起程序员的核心思想来，都是次要的东西，都是起辅助作用的。可是Git这工具似乎特别惹人恼火。&lt;/p&gt;&lt;p&gt;Git并不像很多人吹嘘的那么好用，其中有明显的蹩脚设计。跟Unix的传统一脉相承，Git没有一个良好的包装，设计者把自己的内部实现细节无情地泄露给了用户，让用户需要琢磨者设计者内部到底怎么实现的，否则很多时候不知道该怎么办。用户被迫需要记住挺多稀奇古怪的命令，而且命令行的设计也不怎么合理，有时候你需要加-f之类的参数，各个参数的位置可能不一致，而且加了还不一定能起到你期望的效果。各种奇怪的现象，比如&quot;head detached&quot;，都强迫用户去了解它内部是怎么设计的。随着Git版本的更新，新的功能和命令不断地增加，后来你终于看到命令行里出现了foreach，才发现它的命令行就快变成一个（劣质的）程序语言。如果你了解&lt;a href=&quot;https://github.com/yinwang0/ydiff&quot;&gt;ydiff&lt;/a&gt;的设计思想，就会发现Git之类基于文本的版本控制工具，其实属于古代的东西。然而很多人把Git奉为神圣，就因为它是Linus Torvalds设计的。&lt;/p&gt;&lt;p&gt;Git最让人恼火的地方并不是它用起来麻烦，而是它的“资深用户”们居高临下的态度给你造成的心理阴影。好些人因为自己“精通Git”就以为高人一等，摆出一副专家的态度。随着用户的增加，Git最初的设计越来越被发现不够用，所以一些约定俗成的规则似乎越来越多，可以写成一本书！跟Unix的传统一脉相承，Git给你很多可以把自己套牢的“机制”，到时候出了问题就怪你自己不知道。所以你就经常听有人煞有介事的说：“并不是Git允许你这么做，你就可以这么做的！Unix的哲学是不阻止傻人做傻事……” 如果你提交代码时不知道Git用户一些约定俗成的规则，就会有人嚷嚷：“rebase了再提交！” “不要push到master！” “不要merge！” “squash commits！” 如果你不会用git submodule之类的东西，有人可能还会鄙视你，说：“你应该知道这些！”&lt;/p&gt;&lt;p&gt;打个比方，这样的嚷嚷给人的感觉是，你得了奥运会金牌之后，把练习用的器材还回到器材保管科，结果管理员对你大吼：“这个放这边！那个放那边！懂不懂规矩啊你？” 看出来问题了吗？程序员提交了有高价值的代码（奥运金牌），结果被一些自认为Git用的很熟的人（器材保管员）厉声呵斥。&lt;/p&gt;&lt;p&gt;一个尊重程序员的公司文化，就应该把程序员作为运动健将，把程序员的代码放在尊贵的地位。其它的工具，都应该像器材保管科一样。我们尊重这些器材保管员，然而如果运动员们不懂你制定的器材摆放规矩，也应该表示出尊重和理解，说话应该和气有礼貌，不应该骑到他们头上。所以，对于Git的一些命令和用法，我建议大家向新手介绍时，这样开场：“你本来不该知道这些的，可是现在我们没有更好的工具，所以得这样弄一下……”&lt;/p&gt;</description>
<author>yinwang0</author>
<guid isPermaLink="false">how-to-respect-a-programmer</guid>
<pubDate>Tue, 03 Mar 2015 00:00:00 +0800</pubDate>
</item>
<item>
<title>所谓“人为错误”</title>
<link>http://yinwang.org/blog-cn/2015/02/24/human-errors</link>
<description>&lt;p&gt;
  　　
  &lt;/p&gt;&lt;h2&gt;所谓“人为错误”&lt;/h2&gt;&lt;p&gt;昨天是一个让人悲哀的日子。旧金山湾区主要的上下班交通工具Caltrain，在24小时之内发生三次事故，撞死三人。其中一次事故发生在Menlo Park，一辆汽车被困在铁轨上，因为被前后的车辆堵塞而无法逃避，终于被飞驰而来的列车撞成一堆废铁。开车人被消防队员从残骸里切割出来，送往医院后不久死亡。(&lt;a href=&quot;http://www.ktvu.com/story/28193228/driver-killed-in-menlo-park-caltrain-accident-was-trapped-on-tracks&quot;&gt;新闻视频&lt;/a&gt;）&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://www.ktvu.com/story/28193228/driver-killed-in-menlo-park-caltrain-accident-was-trapped-on-tracks&quot;&gt;
&lt;img src=&quot;http://www.yinwang.org/images/caltrain-accident1.jpg&quot; width=&quot;90%&quot;&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;http://www.yinwang.org/images/caltrain-accident2.jpg&quot; width=&quot;90%&quot;&gt;&lt;/p&gt;&lt;p&gt;我为生命的殒灭而悲哀，然而让我更加悲哀的是，每当这样的事故发生，总有人指责说是“人为错误”。比如，Twitter上有人说这事故是因为死者没有遵守交通规则，才导致自己的汽车被困在铁轨之上，所以她死的活该。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;http://www.yinwang.org/images/caltrain-accident-twitter.png&quot; width=&quot;90%&quot;&gt;&lt;/p&gt;&lt;p&gt;真的是因为她不遵守交通规则吗？真的有人愿意把车停在铁轨上等死吗？也许是这规则没法遵守，或者设计得让人很容易“违反”呢？&lt;/p&gt;&lt;p&gt;首先，规则必须要让人理解，切实可行，才能叫做规则。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;http://www.yinwang.org/images/ravenswood-ave-crossing.png&quot; width=&quot;90%&quot;&gt;&lt;/p&gt;&lt;p&gt;但是请看看铁轨交叉路口上的指令：“不要停在铁轨上（DO NOT STOP ON TRACKS）”，“保持路口畅通（KEEP CLEAR）”。我也不想停在铁轨上啊，可是我刚开到铁轨上，前面的车就停下来了，过不去怎么办？另外什么叫做clear？一定要等到路口里面完全没有车才可以进去吗？如果路口里面虽然有车，然而它们都以每小时30英里的速度行驶？这时我还该停下来吗？如果前面车的速度不到每小时5英里呢？如果前面车辆貌似很快，结果我一进路口它就慢下来了怎么办？&lt;/p&gt;&lt;p&gt;如果“不要停在铁轨上”的指令我想遵守都不可能，如果连clear这个单词都定义不清楚，这还叫什么“交通规则”呢？既然规则都不清楚，又怎么能责怪别人不遵守？我要有多么高的预知未来的能力，才能猜得到前面的车会不会正好在我开到铁轨上的时候停下来，把我堵在铁轨上呢？也许你已经看出来了，这其实不是开车人的错误，它最多算一个“判断失误”。每个人都有可能在那种模棱两可的情况下发生判断失误，因为你没法知道前面的车会怎样运动。记者在现场采访的几个开车人都说：“过那个路口要极度小心，因为你不知道前面的车会怎么样走。”&lt;/p&gt;&lt;p&gt;如果你仔细看看卫星图，就会发现铁轨前方的道路狭窄，而且不远处有一个红绿灯。如果这个红绿灯变红，那么就有可能把直到铁轨处的车辆全都叫停。如果你熟悉湾区的道路，就知道红灯处是82号公路（El Camino Real），上那条路的红灯经常等很久。也就是说，可能有很多车在那里等红灯，一直到铁轨的地方！&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;http://www.yinwang.org/images/ravenswood-ave.png&quot; width=&quot;90%&quot;&gt;&lt;/p&gt;&lt;p&gt;如果你再仔细一点，用Google Map的street view去实地看一下那个路口，就会发现，地面上的&quot;KEEP CLEAR&quot;字样，其实是用来给被堵在铁轨上的车预留后路的。然后你就发现，如果后面的车不遵守KEEP CLEAR的指令，那么它们就会断掉铁轨上的车的退路。所以，其实不是铁轨上的车自己等死，而是后面那些不遵守KEEP CLEAR指令的车，把它逼上了绝路。然而就像我之前提到的，想要遵守KEEP CLEAR又是很模棱两可的事情，后面的车有可能以为你过得去，所以才跟上的。所以你死了，不能怪火车，不能怪你自己，不能怪前面的车，还不能怪后面的车！怪谁呢？只能怪路口的“设计”！&lt;/p&gt;&lt;p&gt;这种路口交通规则还有一个致命的特征，那就是后果的严重性不明显，人不会敏锐的感觉到犯了错误的结果是车毁人亡。一般人都不闯红灯，因为很显然，如果你红灯不停就会被另一个方向的车撞上。可是违反这铁道路口的规则，后果不是立显的，有可能什么事也没有，也有可能呆在那里几分钟之后才出事，到时候想逃都逃不了。这就像把活青蛙放进冷水里，然后慢火加热一样，它不会立即被烫得跳出来，而会死在里面！等你慢慢的开到铁轨上，才发现前面的车不走了（因为更前面路口亮了红灯），后面的车又抵上来。过一会儿，当当当，栏杆放下来，火车来了…… 你这是在设陷阱诱捕野生动物吗？&lt;/p&gt;&lt;p&gt;如此容易出现的失误（甚至不叫做自己的失误），真的值得一个人用生命来偿还吗？按照这样的逻辑，我就可以把地雷埋在大街上，插上标志牌说：“下面有地雷，不要踩！”如果你踩了，那我就可以怪你没遵守规则，自己找死！&lt;/p&gt;&lt;p&gt;如果你回头看看历史就会发现，Caltrain几乎每个月都会撞上至少一辆汽车，所以这次的事故绝不是偶然，它有更深层的原因。上一个月，我乘坐的一列Caltrain，就因为前面一趟列车撞上了汽车而延误了好几个小时。当时我就在Twitter上看到有人责备开车的人，说他脑子秀逗了，不该把车停在铁轨上。当时我就在Twitter上警告@Caltrain，说你们应该仔细分析一下这个交叉路口的设计，也许是因为设计有问题。没有人回应我。这次出了三条人命，交叉路口的设计问题才终于受到了重视。&lt;/p&gt;&lt;p&gt;出了人命的大事故，也许能唤醒人们一点理智，认识到所谓的“人为错误”，其实在很多时候是设计错误。在这个例子中，交叉路口的设计是不合理的。一旦你因为判断失误把车开进去了，就有可能出现无路可逃，车毁人亡的局面。然而很多生活中的设计失误所引发的“人为错误”都是不致命的，有点像慢性毒药。这种貌似无关痛痒的设计错误，更加容易被忽视，它们就潜伏在我们的身边。&lt;/p&gt;&lt;p&gt;在我所在的软件行业里，就有很多这样的设计错误。在我看来，整个软件行业基本就是建立在一堆堆的设计失误之上。做程序员如此困难和辛苦，大部分原因就是因为软件系统里面积累了大量前人的设计失误，所以我们需要做大量的工作来弥补或者绕过。举个例子，Unix/Linux操作系统就是一个重大的设计失误。Unix系统的命令行，系统API，各种工具程序，编辑器，程序语言（C，C++等），设计其实都很糟糕。很多工具程序似乎故意设计得晦涩难用，让人摸不着头脑，需要大量时间学习，而且容易出错。出错之后难以发现，难以弥补。&lt;/p&gt;&lt;p&gt;然而一般程序员都没有意识到这里面的设计错误，知道了也不敢指出来，他们反而喜欢显示自己死记硬背得住这些稀奇古怪的规则。这就导致了软件行业的“皇帝的新装现象”——没有人敢说工具的设计有毛病，因为如果你说出来，别人就会认为你在抱怨，那你不是经验不足，就是能力不行。这就像你不敢说皇帝没穿衣服，否则别人就会认为你就是白痴或者不称职的人！Unix系统的同盟者和后裔们（Linux，C语言，Go语言），俨然形成了这样一种霸权，他们鄙视觉得它们难用，质疑它们的设计的人。他们嘲笑这些用户为失败者，即使其实有些“用户”水平比Unix的设计者还要高。久而久之，他们封住了人们的嘴，让人误以为难用的东西就是好的。&lt;/p&gt;&lt;p&gt;我体会很深的一个例子就是Git版本控制工具。有人很把这种东西当回事，引以为豪记得住如何用一些稀奇古怪的Git命令（比如git rebase, git submodule之类）。好像自己知道了这些就真的是某种专家一样，每当遇到不会用这些命令的人，都在心底默默地鄙视他们。作为一个比Git的作者还要高明的程序员，我却发现自己永远无法记住那些命令。在我看来，这些命令晦涩难懂，很有可能是因为没设计好造成的。因为如果一个东西设计好了，以我的能力是不可能不理解的。可是Linus Torvalds的名气之大，威望之高，有谁敢说：“我就是不会用你设计的破玩意儿！你把我怎么着？”&lt;/p&gt;</description>
<author>yinwang0</author>
<guid isPermaLink="false">human-errors</guid>
<pubDate>Tue, 24 Feb 2015 00:00:00 +0800</pubDate>
</item>
</channel>
</rss>
