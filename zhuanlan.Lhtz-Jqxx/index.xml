<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:media="http://search.yahoo.com/mrss/">
<channel>
<title>【量化投资与机器学习】微信公众号</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/</link>
<description>公众号主要介绍关于量化投资和机器学习的知识和应用。通过研报，论坛，博客，程序等途径全面的为大家带来知识食粮。版块语言分为：Python、Matlab、R，涉及领域有：量化投资、机器学习、深度学习、综合应用、干货分享等。</description>
<language>zh-cn</language>
<lastBuildDate>Mon, 22 Jan 2018 13:47:19 +0800</lastBuildDate>
<item>
<title>2018年「金融平均脸」出炉，是不是要长这样才能做期货？</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/2018-01-21-33153911.htm</link>
<description>&lt;meta charset=&quot;utf-8&quot;&gt;
&lt;title&gt;2018年「金融平均脸」出炉，是不是要长这样才能做期货？&lt;/title&gt;
&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/33153911&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-5fd69dc772d93269f198ba59246108a9_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;对比各个金融领域公司的“平均样貌”，你眼中的“颜值代表”是哪家？&lt;/p&gt;&lt;p&gt;期货脸长啥样？&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;金融行业&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;行业平均脸&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;最受热捧的脸&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-35476841a978e223078a0dcaa83d559f_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;900&quot; data-rawheight=&quot;419&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;高盛&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-9971fe42cfe51be34b8be0efeb234ac1_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;65&quot; data-rawheight=&quot;45&quot;&gt;&lt;p&gt;世界上最可怕的事&lt;/p&gt;&lt;p&gt;是比你美的人还比你有脑子&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-5c898afd266deaea0cbe92ac4f5fd6c8_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;900&quot; data-rawheight=&quot;419&quot;&gt;&lt;p&gt;&lt;b&gt;摩根大通&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-a9973210c31280929e25313a96e49612_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;178&quot; data-rawheight=&quot;75&quot;&gt;&lt;p&gt;听说很多人叫我爸爸&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-9134a4e26e1083d862d646850bc027ae_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;900&quot; data-rawheight=&quot;419&quot;&gt;&lt;p&gt;&lt;b&gt;中金&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-6c837db1ea6b924e0885de5e9a7f96b3_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;149&quot; data-rawheight=&quot;102&quot;&gt;&lt;p&gt;我们有很多个小目标 &lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-c1250204b41f23ce7971882c804e6709_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;900&quot; data-rawheight=&quot;419&quot;&gt;&lt;p&gt;&lt;b&gt;中信&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-55a2452a23ff0e371dca78ef63eb1112_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;175&quot; data-rawheight=&quot;50&quot;&gt;&lt;p&gt;几十个亿，真的好难花&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-2ae16cc8f64bd1023f438395c9ea8502_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;900&quot; data-rawheight=&quot;419&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;四大会计师事务所&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;行业平均脸 &lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;风彩与黑眼圈并存&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-aecb9c29dd345b1b1e64d5ac6eae597b_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;900&quot; data-rawheight=&quot;419&quot;&gt;&lt;p&gt;&lt;b&gt;安永&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-9ecb3dce198252ac82d8382a16704f67_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;121&quot; data-rawheight=&quot;75&quot;&gt;&lt;p&gt;压力大又怎样，软妹多就好&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-990e351a3e9d217e174da157f08e479b_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;900&quot; data-rawheight=&quot;419&quot;&gt;&lt;p&gt;&lt;b&gt;普华永道&lt;/b&gt;&lt;/p&gt;&lt;b&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-56f748ddacb95f4f63f1d870eb249e7f_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;119&quot; data-rawheight=&quot;75&quot;&gt;&lt;/b&gt;&lt;p&gt;来自行业老大的微笑&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-86640f3058a89ac9a5ea17a80c0b78a9_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;900&quot; data-rawheight=&quot;419&quot;&gt;&lt;p&gt;&lt;b&gt;毕马威&lt;/b&gt;&lt;/p&gt;&lt;p&gt;有一种脸叫数据脸&lt;/p&gt;&lt;p&gt;&lt;b&gt;德勤&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-5433aaa9de1e9c1230cb8244d28c7b68_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;121&quot; data-rawheight=&quot;91&quot;&gt;&lt;p&gt;金丝镜框后，是美图也盖不住的细纹&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-3a93d9be2dc5ba5dc97b476ed913683b_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;900&quot; data-rawheight=&quot;419&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;咨询行业&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;行业平均脸&lt;br&gt;做PPT，我们是最认真的&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-567c4905d9fd12c102af578b3388673a_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;900&quot; data-rawheight=&quot;419&quot;&gt;&lt;p&gt;&lt;b&gt;麦肯锡&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-b1a52a74c8dfe09d2f8ce93a51a3f1f5_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;300&quot; data-rawheight=&quot;154&quot;&gt;&lt;p&gt;盛名在外的，除了专业还有颜值&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-7f5ac1f8e0bcd9c7b009bb437df32c6d_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;900&quot; data-rawheight=&quot;419&quot;&gt;&lt;p&gt;&lt;b&gt;波士顿&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-a7fc2b65f764767d6b75dba88352c70e_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;225&quot; data-rawheight=&quot;104&quot;&gt;&lt;p&gt;没有用数字解释不了的事儿 &lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-312aabaeb2efa2cd8d306a8427bef24d_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;900&quot; data-rawheight=&quot;419&quot;&gt;&lt;p&gt;&lt;b&gt;贝恩&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-077b7999edd5e4dec5268847c209b9ba_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;225&quot; data-rawheight=&quot;169&quot;&gt;&lt;p&gt;好不好看不重要&lt;/p&gt;&lt;p&gt;会汇（bi）报（bi）最重要&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-4e96e9da41ebca94ea46c0588752af10_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;900&quot; data-rawheight=&quot;419&quot;&gt;&lt;p&gt;&lt;b&gt;罗兰贝格&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-fe72ebb22890c6adae89f32732a73999_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;224&quot; data-rawheight=&quot;120&quot;&gt;&lt;p&gt;150一天的餐补，果然不是盖的。&lt;/p&gt;&lt;p&gt;有图为证！&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-75304830a152a09ab90fc9b4dc35258e_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;640&quot; data-rawheight=&quot;298&quot;&gt;&lt;h2&gt;&lt;b&gt;银行&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;行业平均脸&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;算起账来，我们是最无辜的&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-5a369cc08cb10fe4523fe12ce233b155_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;640&quot; data-rawheight=&quot;298&quot;&gt;&lt;p&gt;&lt;b&gt;招商银行&lt;/b&gt;&lt;/p&gt;&lt;p&gt;招行的人一定要苗条漂亮，&lt;/p&gt;&lt;p&gt;如果太胖了请去对面的工行。&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-1a3b278e5d14cef8ce6d2c4034a79fd2_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;640&quot; data-rawheight=&quot;298&quot;&gt;&lt;p&gt;&lt;b&gt;工商银行&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-d9c3047e1c0e889495b0c9f22c9ac08c_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;466&quot; data-rawheight=&quot;113&quot;&gt;&lt;p&gt;我们宇宙行家大业大，实力雄厚。&lt;/p&gt;&lt;p&gt;楼上就是矫情！&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-5a369cc08cb10fe4523fe12ce233b155_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;640&quot; data-rawheight=&quot;298&quot;&gt;&lt;p&gt;&lt;b&gt;建设银行&lt;/b&gt;&lt;/p&gt;&lt;b&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-c5f44fda1e311917046628090a8c44f9_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;454&quot; data-rawheight=&quot;106&quot;&gt;&lt;/b&gt;&lt;p&gt;我就静静看着楼上不说话，&lt;/p&gt;&lt;p&gt;反正我也没空说话。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-11f3df96d86dcc363093bb498b3b760b_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;640&quot; data-rawheight=&quot;298&quot;&gt;&lt;p&gt;&lt;b&gt;中国银行&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-449e5df1bdf3e70d36adc4102f81ad39_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;232&quot; data-rawheight=&quot;90&quot;&gt;&lt;p&gt;谢谢，这是四大行里最international的脸&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-16f1c79b4419f6a9bd1015713008b04e_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;640&quot; data-rawheight=&quot;298&quot;&gt;&lt;h2&gt;&lt;b&gt;期货行业&lt;/b&gt;&lt;/h2&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-b7ae886a05284c6f583d9077432339d3_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;657&quot; data-rawheight=&quot;430&quot;&gt;&lt;p&gt;咳咳...金融民工中的战斗机...灰头土脸...&lt;/p&gt;&lt;p&gt;本文纯属搞笑，如有雷同，纯属巧合&lt;/p&gt;&lt;p&gt;素材来源：UniCareer、瞭望智库&lt;/p&gt;&lt;b&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-17c3d959b2db4c1b3a90a54e5af7f4ee_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1096&quot; data-rawheight=&quot;372&quot;&gt;&lt;/b&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>量化投资机器学习</author>
<guid isPermaLink="false">2018-01-21-33153911</guid>
<pubDate>Sun, 21 Jan 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>模式识别在金融时间序列中的应用【系列五十】</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/2018-01-21-33153489.htm</link>
<description>&lt;meta charset=&quot;utf-8&quot;&gt;
&lt;title&gt;模式识别在金融时间序列中的应用【系列五十】&lt;/title&gt;
&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/33153489&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-42b57b00a759ca6bdc258a8f967aa0b2_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;今天编辑部诶大家带来Kathryn Dover的一篇文章。主要讲模式识别在股票数据中的应用。&lt;/p&gt;&lt;p&gt;&lt;b&gt;来源：&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653287106&amp;amp;idx=1&amp;amp;sn=93e506237376f0540ff5ede81f1d2343&amp;amp;chksm=802e30d7b759b9c1d9716776fade282acebae0e55a1f6fcd3897f183736c24b14fd23f08ce27#rd&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-2e9f227f80bf31b439e47581a6b43f2b&quot; data-image-width=&quot;640&quot; data-image-height=&quot;640&quot; data-image-size=&quot;ipico&quot;&gt;模式识别在金融时间序列中的应用【系列五十】&lt;/a&gt;&lt;p&gt;Finding patterns in high dimensional data can be difficult because it cannot be easily visualized. Many different machine learning methods are able to fit this high dimensional data in order to predict and classify future data but there is typically a large expense on having the machine learn the fit for a certain part of the dataset. This thesis proposes a geometric way of defining different patterns in data that is invariant under size and rotation so it is not so dependent on the input data. Using a Gaussian Process, the pattern is found within stock market data and predictions are made from it.&lt;/p&gt;&lt;p&gt;主要是用一种不依赖于输入数据的方法。 使用高斯过程，在股票市场数据中发现该模式，并从中进行预测。&lt;/p&gt;&lt;p&gt;前面几部分都是背景介绍，大家可以自己去查阅，我们主要把后面几部分解读一下。&lt;/p&gt;&lt;p&gt;作者定义了三种大家常见的指标形态：W底、M顶、头肩形。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1. Standard Double-Bottom Pattern (A Standard W)&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-c020094c0c4880baf508c0d835417f7a_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;249&quot; data-rawheight=&quot;186&quot;&gt;&lt;p&gt;&lt;b&gt;2. Standard Double-Top Pattern (A Standard M)&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-82f9bed60e9e9bf18b159f75b2186f1f_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;277&quot; data-rawheight=&quot;207&quot;&gt;&lt;p&gt;&lt;b&gt;3. Standard Head and Shoulder Pattern&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-d40dea7c724458261ce2e642a8bf61e3_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;270&quot; data-rawheight=&quot;207&quot;&gt;&lt;p&gt;When we actually apply our definition of shapes to the data, we will find that there is no perfect match to our standard definition. Therefore, we will want to define a “fuzzy shape” that will be considered an approximate form of the standard shape. These fuzzy shapes will not only allow us to approximate how far away a shape is from the standard shape but also how far fuzzy shapes are from each other. The way that we define our fuzzy shapes is discussed below. &lt;/p&gt;&lt;p&gt;用数据去定义这些指标形态时，我们会发现与我们定义的形态并不是完全匹配。 作者做了一个“模糊形状”的处理。将被视为标准形状的近似形式。 &lt;/p&gt;&lt;p&gt;&lt;b&gt;1. The Fuzzy W&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-41f9aac637fb0e8b131d3deed4a8f237_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;392&quot; data-rawheight=&quot;264&quot;&gt;&lt;p&gt;&lt;b&gt;2. The Fuzzy M&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-7d7a46b860d17b137f5ae32b6e318670_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;122&quot; data-rawheight=&quot;29&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-741fbac4dff294df4eb9fb2718b060b1_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;387&quot; data-rawheight=&quot;221&quot;&gt;&lt;p&gt;&lt;b&gt;3. The Fuzzy Head and Shoulder&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-29da43f4bd4cced4f3a8eb0c80aad1a2_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;436&quot; data-rawheight=&quot;270&quot;&gt;&lt;p&gt;Now that we have defined both our standard and fuzzy shapes, we want to be able to define some actions we can take on these shapes. We discuss the change of basis and flipping a shape and how they can be used to compare different shapes to each other. We also discuss how the slopes and lengths of the vectors can be used to categorize the shape that can be used for prediction.&lt;/p&gt;&lt;p&gt;接下来作者对这三种形态的一些变化进行了讨论。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1. Change of Basis&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. Flipping a Shape&lt;/b&gt;&lt;/p&gt;&lt;b&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-6f0bef6b065aa075213159f8630772bf_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;265&quot; data-rawheight=&quot;203&quot;&gt;&lt;/b&gt;&lt;p&gt;&lt;b&gt;3. Symmetric Representation&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-134d9ad67dd4bab3f595d961c6b8a9d9_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;255&quot; data-rawheight=&quot;175&quot;&gt;&lt;p&gt;Once the shapes have been found in the data, we can categorize these shapes using their slopes and lengths. By categorizing certain shapes by their slopes and lengths, we can group similar shapes together and run traditional machine learning techniques on these specific groups. This is to determine indicators of these types of shapes, which can ultimately be used for predictive purposes. We discuss how shapes can be categorized using slopes and lengths and how we can use those categories to do rough predictions of stock market data.&lt;/p&gt;&lt;p&gt;开始对图形的斜率和长度进行定量的分析结合机器学习方法，然后开始预测。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Saving the Slopes and Lengths&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-cd6ee3a8a3a0a8c9734743d863aaa136_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;486&quot; data-rawheight=&quot;37&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-91d72b91259f958235e5e1119fb0407b_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;486&quot; data-rawheight=&quot;548&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-efcb1788e41e6f795bc849b750afd3d9_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;370&quot; data-rawheight=&quot;199&quot;&gt;&lt;p&gt;两种预测方法：&lt;/p&gt;&lt;p&gt;&lt;b&gt;1. Predictions with Slopes and Lengths&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. Prediction with Direction&lt;/b&gt;&lt;/p&gt;&lt;p&gt;创建一个算法来寻找模式：&lt;/p&gt;&lt;p&gt;Now that we have defined what our shapes are and how to approximate shapes, we will be discussing the algorithm that finds these shapes and how it handles the data. I used a Gaussian Process to fit the data with a function that the local minimums and maximums could be extracted from. These local extrema are used to define the vectors of the shapes and subsequently the algorithm checks these vectors using the definitions from the previous sections.&lt;/p&gt;&lt;p&gt;关键词：&lt;b&gt;Gaussian Process&lt;/b&gt;&lt;/p&gt;&lt;p&gt;最终过程如下：&lt;/p&gt;&lt;p&gt;1. Given a certain variance, a Gaussian Process is used to fit the data.&lt;/p&gt;&lt;p&gt;2. From this fit of the data, the algorithm identifies all the local extrema in the data.&lt;/p&gt;&lt;p&gt;3. The algorithm goes through the local extrema in the data sequentially and identifies sequences that could possibly be a predefined shape.For example, a W has a following sequence of minium and maximum:max/min/max/min/max.&lt;/p&gt;&lt;p&gt;4. If a sequence is valid for a certain shape, then the algorithm uses the definitions of the shapes defined earlier in order to figure out if they qualify.&lt;/p&gt;&lt;p&gt;5. If a shape is found, the lengths and slopes of the vectors in the shape are stored (as well as the following segment) for predictive purposes.&lt;/p&gt;&lt;p&gt;6. The categorizing constants k` i and ks i are calculated and stored.&lt;/p&gt;&lt;p&gt;7. Once the algorithm has gone through all the extrema, it uses the stored slopes, lengths, and categorizing constants and uses them to calculate the rough prediction from the weighted average.&lt;/p&gt;&lt;p&gt;在下图中，我们可以看到算法的运行步骤。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-42b53e58ffb2513c0e272d052520200f_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;508&quot; data-rawheight=&quot;219&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-7424028c97ee1280007620a30e1a50af_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;507&quot; data-rawheight=&quot;223&quot;&gt;&lt;p&gt;实际的应用：&lt;br&gt;In this section, I describe the results of the algorithm I developed in the last chapter when it was run on stock price history for different companies. The algorithm was run for variances 0.001 and 4 over the entire stock price history for the companies Apple, Disney, Microsoft, Walmart and the NASDAQ index. For testing the prediction part of the algorithm, the entire stock price history for the Dow Jones index was used. The reason for choosing this dataset was twofold. First, there was between 20-30 years worth of data for each company, which would mean I could potentially find a variety of shapes in different sizes. Second, I chose these specific companies by looking through the stock history of companies and choose those that had many turbulent years that offer enough fluctuation that can create the shapes I am looking for. All stock data was found at yahoo.finance.com. The below subsections discuss how I fit my data, found the shapes, and calculated predictions for different shapes. &lt;/p&gt;&lt;p&gt;用了两种方法&lt;/p&gt;&lt;p&gt;&lt;b&gt;1. Finding Different Sized Shapes&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-00c7953c6f670571bd80a643ecb1e04a_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;509&quot; data-rawheight=&quot;468&quot;&gt;&lt;p&gt;The original data (upper le), the fit of the data when the variance is 0.001 (upper right), the algorithm identifying the local extrema (lower le), the W found by the algorithm (lower right)&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. Finding Multiple Shapes&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-c4eb07203dd4c838e90704938b5ac68a_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;509&quot; data-rawheight=&quot;471&quot;&gt;&lt;p&gt;is 4 (upper right), the algorithm identifying the local extrema (lower le), the W found by the algorithm (lower right)&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-930ee1fcbe7bb54af07ef173075902a4_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;505&quot; data-rawheight=&quot;235&quot;&gt;&lt;p&gt;A fit with variance = 0.001 where multiple Ws were found&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-930ee1fcbe7bb54af07ef173075902a4_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;505&quot; data-rawheight=&quot;235&quot;&gt;&lt;p&gt;A fit with variance = 0.001 where multiple Ms were found&lt;/p&gt;&lt;p&gt;&lt;b&gt;预测&lt;/b&gt;&lt;/p&gt;&lt;p&gt;I used the data from Apple, Disney, Microsoft, Walmart and the NASDAQ as my learning set for both of my prediction methods and I used the Dow Jones index as my test data. The results from each of the methods is given below.&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-bb403d155288803c73adfd2aca4f62b5_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;620&quot; data-rawheight=&quot;268&quot;&gt;&lt;p&gt;&lt;b&gt;算法存在的问题：&lt;/b&gt;&lt;br&gt;While the algorithm was successful in finding the shapes and doing a rough prediction, there were still some issues with it. For example, finding a good tolerance for finding a shape was not exact. I had to run the algorithm many times to determine at what point the algorithm would find identify a pattern was not the correct shape. This usually happened because the tolerance was too high and the patterns found did not look like the shape. Likewise, when the tolerance was too low, it would identify no shape. Finding an appropriate tolerance (somewhere between 0.7 and 1) took a while and the tolerance for each shape was different as the Ws had a lower tolerance while the head and shoulder pattern needed a higher tolerance.&lt;/p&gt;&lt;p&gt;&lt;b&gt;总结：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Finally, the predictive step of the algorithm has a long way to go. Using the slopes and lengths method did not work well which might be explained by having no good way of measuring the difference between slopes. Addi tionally, the weighted average method might not be enough to get a good understanding unless I had a lot more data. I had about 55 shapes for each type of shape when I ran the algorithm on the data. If I want to get a better average, I would need to run my code on a lot more data in order for the average to not be swayed as heavily by outliers. The directional vector method worked much better than the slopes and lengths method, but there was still a error of about 20-28 degrees. We could potentially decrease this error by simply getting more data. This prediction also indicated that there actually might be a correlation between the shapes and the directional vector, so perhaps we could use more traditional machine learning methods on these directional vector values to see if we can learn anything extra from it.For example, is there some sort of correlation between the k values we found in the slope/lengths predictions and the directional vector we calculated? A question like this might be answered by using a neural network or a support vector machine.&lt;/p&gt;&lt;p&gt;同时作者还提出了一些改进模型的方法。我们认为此部分最为有探究性：&lt;/p&gt;&lt;p&gt;&lt;b&gt;1. Improving Shape Recognition&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. Extending into Multiple Dimensions&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;论文链接：&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;http://scholarship.claremont.edu/hmc_theses/105/&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot;&gt;&quot;Pattern Recognition in Stock Data&quot; by Kathryn Dover&lt;/a&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653287106&amp;amp;idx=1&amp;amp;sn=93e506237376f0540ff5ede81f1d2343&amp;amp;chksm=802e30d7b759b9c1d9716776fade282acebae0e55a1f6fcd3897f183736c24b14fd23f08ce27#rd&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-2e9f227f80bf31b439e47581a6b43f2b&quot; data-image-width=&quot;640&quot; data-image-height=&quot;640&quot; data-image-size=&quot;ipico&quot;&gt;模式识别在金融时间序列中的应用【系列五十】&lt;/a&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-ecd27e25f4fe3285f8f1fe63024bb19b_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1096&quot; data-rawheight=&quot;372&quot;&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>量化投资机器学习</author>
<guid isPermaLink="false">2018-01-21-33153489</guid>
<pubDate>Sun, 21 Jan 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>【代码+论文】通过ML、Time Series模型学习股价行为</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/2018-01-21-33152954.htm</link>
<description>&lt;meta charset=&quot;utf-8&quot;&gt;
&lt;title&gt;【代码+论文】通过ML、Time Series模型学习股价行为&lt;/title&gt;
&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/33152954&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-3ff2fee46d48ff47aed98c1cc9ce4c20_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;今天编辑部给大家带来的是来自Jeremy Jordan的论文，主要分析论文的建模步骤和方法，具体内容大家可以自行查看。&lt;/p&gt;&lt;p&gt;&lt;b&gt;（代码在文末下载。由于代码过长，有些代码部分略去了一些）&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;来源&lt;/h2&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653287197&amp;amp;idx=1&amp;amp;sn=9630389a52c7d0be4c1feaf3a534c2ce&amp;amp;chksm=802e3108b759b81ed11174f71b23fb73abe5c4ebad0f9d480b6efbd8f7e644de6b2232dc63fa#rd&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-2e9f227f80bf31b439e47581a6b43f2b&quot; data-image-width=&quot;640&quot; data-image-height=&quot;640&quot; data-image-size=&quot;ipico&quot;&gt;【代码+论文】通过ML、Time Series模型学习股价行为（第九期免费赠书活动来啦！）&lt;/a&gt;&lt;code lang=&quot;python&quot;&gt;# Standard imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
plt.style.use(&#39;seaborn-notebook&#39;)
import seaborn as sns
sns.set()
import matplotlib.cm as cm

# Enable logging
import logging
import sys
logging.basicConfig(level=logging.DEBUG, 
format=&#39;%(asctime)s - %(levelname)s - %(message)s&#39;) &lt;/code&gt;&lt;p&gt;&lt;b&gt;步骤1：下载数据&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;获取S＆P 500公司名单&lt;/b&gt;&lt;/p&gt;&lt;p&gt;从维基百科下载的S＆P 500公司信息。我们只使用股票代码列表，但GICS_sector和GICS_sub_industry等可能有用。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;column_names = [&#39;ticker&#39;, &#39;security&#39;, &#39;filings&#39;, 
&#39;GICS_sector&#39;, &#39;GICS_sub_industry&#39;, &#39;HQ_address&#39;, &#39;date_added&#39;, &#39;CIK&#39;]

sp500companies = pd.read_csv(&#39;data/S&amp;amp;P500.csv&#39;,
header = 0, names = column_names).drop([&#39;filings&#39;], axis=1)

sp500companies = sp500companies.set_index([&#39;ticker&#39;])
sp500companies.head(10) &lt;/code&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-00eeda76a8541e935271cc8dacdf72af_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1014&quot; data-rawheight=&quot;486&quot;&gt;&lt;p&gt;&lt;b&gt;下载价格数据&lt;/b&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;import pandas_datareader as pdr

start = pd.to_datetime(&#39;2009-01-01&#39;)
end = pd.to_datetime(&#39;2017-01-01&#39;)
source = &#39;google&#39;

for company in sp500companies.index:
   try:
       price_data = pdr.DataReader(company, source, start, end)
       price_data[&#39;Close&#39;].to_csv(&#39;data/company_prices/%s_adj_close.csv&#39; % company)
   except:
       logging.error(&quot;Oops! %s occured for %s. \nMoving on to next entry.&quot; % (sys.exc_info()[0], company))Transcripts are scraped from Seeking Alpha using the Python library Scrapy.&lt;/code&gt;&lt;p&gt;To fetch a company transcript, complete the following steps.&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;cd data/
scrapy crawl transcripts -a symbol=$SYM &lt;/code&gt;&lt;p&gt;This will download all of the posted earnings call transcripts for company SYM and store it as a JSON lines file in data/company_transcripts/SYM.json.&lt;/p&gt;&lt;p&gt;&lt;b&gt;步骤2：加载数据&lt;/b&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;# Read in a collection of company transcripts
def load_company_transcripts(company):
   &#39;&#39;&#39;
   Reads a company&#39;s transcripts into a Pandas DataFrame. 
   
   Args:
       company: The ticker symbol for the desired company. 
   &#39;&#39;&#39;
   logging.debug(&#39;Reading company transcripts for {}&#39;.format(company))
   
   # Read file
   text_data = pd.read_json(&#39;data/company_transcripts/{}.json&#39;.format(company), lines=True)
    
   # Drop events that don&#39;t have an associated date
   text_data = text_data.dropna(axis = 0, subset= [&#39;date&#39;])
   
   # Reindex according to the date
   text_data = text_data.set_index(&#39;date&#39;).sort_index()
   
   # Check for possible duplicate entries from scraping
   if sum(text_data.duplicated([&#39;title&#39;])) &amp;gt; 0:
       logging.warning(&#39;{} duplicates removed from file&#39;.format(sum(text_data.duplicated([&#39;title&#39;]))))
        text_data = text_data.drop_duplicates([&#39;title&#39;])
              
   # Concatenate body into single body of text
   text_data[&#39;body&#39;] = text_data[&#39;body&#39;].apply(lambda x: &#39; &#39;.join(x))
   
   # Add a column to each transcript for word count, plot histogram
   text_data[&#39;count&#39;] = text_data[&#39;body&#39;].apply(lambda x: len(x.split()))
   
   # Check for empty transcripts
   if len(text_data[text_data[&#39;count&#39;] == 0]) &amp;gt; 0:
       logging.warning(&#39;{} empty transcripts removed from file&#39;.format(len(text_data[text_data[&#39;count&#39;] == 0])))
    text_data = text_data[text_data[&#39;count&#39;] != 0]
   
   return text_data&lt;/code&gt;&lt;p&gt;&lt;b&gt;加载价格数据&lt;/b&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;def load_company_price_history(companies, normalize = False, fillna = True, dropna=True):
    &#39;&#39;&#39; 
   Builds a DataFrame where each column contains one company&#39;s adjusted closing price history.
    
   Args:
       companies: A list of company ticker symbols to load.
       normalize: Boolean flag to calculate the log-returns from the raw price data. 
       fillna: Boolean flag to fill null values. Limited fill up to 5 days forward and 1 day backward,
        for companies with long periods of null values, this prevents from creating a stagnant time series.
        Instead, those companies should be dropped using `dropna=true`.
       dropna: Boolean flag to drop companies that don&#39;t have a full price history. 
        
   &#39;&#39;&#39;
   prices = []
   for company in companies:
       logging.debug(&#39;Reading company prices for {}&#39;.format(company))
       price_history = pd.read_csv(&#39;data/company_prices/{}_adj_close.csv&#39;.format(company), 
                                    names=[company], index_col=0)
       prices.append(price_history)
       
   df = pd.concat(prices, axis=1)
   df.index = pd.to_datetime(df.index)
   df = df.asfreq(&#39;B&#39;, method=&#39;ffill&#39;)
  
   if normalize:
       df = np.log(df).diff() # Calculate the log-return, first value becomes null
       df.iloc[0] = df.iloc[1] # Forward fill the null value
       
   if fillna:
       df = df.fillna(method = &#39;ffill&#39;, limit=5) # First pass fill NAs as previous day price
        df = df.fillna(method = &#39;bfill&#39;, limit=1) # For NAs with no prev value (ie. first day), fill NA as next day price
    
   if dropna:
       # Validate quality of data (null values, etc)
       # Drop any companies that don&#39;t have a full 8-year history
       df = df.dropna(axis=1, how=&#39;any&#39;)
       assert df.isnull().values.any() == 0
       logging.debug(&#39;Null values found after cleaning: {}&#39;.format(df.isnull().values.any()))
        
   
   return df&lt;/code&gt;&lt;p&gt;&lt;b&gt;读取文件目录并加载可用数据&lt;/b&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;# Go to data folder and get list of all companies that have data
import glob
import re

# All files for price history
price_files = glob.glob(&#39;data/company_prices/*_adj_close.csv&#39;)
company_prices = [re.search(r&#39;(?&amp;lt;=data\/company_prices\/)(.*)(?=_adj_close.csv)&#39;, f).group(0) 
                  for f in price_files]
logging.info(&#39;{} company price histories available.&#39;.format(len(company_prices)))

# All files for transcripts
transcript_files = glob.glob(&#39;data/company_transcripts/*.json&#39;)
company_transcripts = [re.search(r&#39;(?&amp;lt;=data\/company_transcripts\/)(.*)(?=.json)&#39;, f).group(0) 
                       for f in transcript_files]
logging.info(&#39;{} company transcripts available.&#39;.format(len(company_transcripts)))

# Intersection of two datasets
company_both = list(set(company_prices) &amp;amp; set(company_transcripts))
logging.info(&#39;{} companies have both transcripts and price history available.&#39;.format(len(company_both)))
2017-10-04 09:52:44,215 - INFO - 502 company price histories available.
2017-10-04 09:52:44,219 - INFO - 173 company transcripts available.
2017-10-04 09:52:44,220 - INFO - 170 companies have both transcripts and price history available.&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;# Load all pricing data into memory
company_price_df = load_company_price_history(company_both, normalize=True)
2017-10-04 09:52:45,680 - DEBUG - Reading company prices for AAL
2017-10-04 09:52:45,683 - DEBUG - Reading company prices for XRAY
2017-10-04 09:52:45,686 - DEBUG - Reading company prices for CVX
2017-10-04 09:52:45,689 - DEBUG - Reading company prices for ACN
2017-10-04 09:52:45,692 - DEBUG - Reading company prices for COF
2017-10-04 09:52:45,696 - DEBUG - Reading company prices for DISCK
2017-10-04 09:52:45,699 - DEBUG - Reading company prices for AXP
2017-10-04 09:52:45,702 - DEBUG - Reading company prices for CVS
2017-10-04 09:52:45,705 - DEBUG - Reading company prices for AMGN
```&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;# Load all transcript data into memory
company_transcripts_dict = {}
failures= []

for company in company_transcripts:
   try:
       company_transcripts_dict[company] = load_company_transcripts(company)
   except:
       logging.error(&quot;Oops! {} occured for {}. \nMoving on to next entry.&quot;.format(sys.exc_info()[0], company))
        failures.append(company)
2017-10-04 09:52:51,910 - DEBUG - Reading company transcripts for A
2017-10-04 09:52:51,979 - DEBUG - Reading company transcripts for AAL
2017-10-04 09:52:52,080 - DEBUG - Reading company transcripts for AAP
2017-10-04 09:52:52,150 - DEBUG - Reading company transcripts for AAPL
2017-10-04 09:52:52,213 - DEBUG - Reading company transcripts for ABBV
2017-10-04 09:52:52,251 - DEBUG - Reading company transcripts for ABC
2017-10-04 09:52:52,314 - DEBUG - Reading company transcripts for ABT
2017-10-04 09:52:52,345 - WARNING - 1 duplicates removed from file
2017-10-04 09:52:52,422 - DEBUG - Reading company transcripts for ACN
2017-10-04 09:52:52,483 - DEBUG - Reading company transcripts for ADBE
2017-10-04 09:52:52,546 - DEBUG - Reading company transcripts for ADI
2017-10-04 09:52:52,548 - ERROR - Oops! &amp;lt;class &#39;KeyError&#39;&amp;gt; occured for ADI. 
Moving on to next entry.
2017-10-04 09:52:52,549 - DEBUG - Reading company transcripts for ADM
```&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;failures 
[&#39;ADI&#39;, &#39;BHF&#39;, &#39;CTHS&#39;, &#39;ED&#39;, &#39;ETN&#39;]&lt;/code&gt;&lt;p&gt;都是空的文件。&lt;/p&gt;&lt;p&gt;&lt;b&gt;抽样检查&lt;/b&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;# Select companies to load and inspect
google_transcripts = company_transcripts_dict[&#39;GOOG&#39;]
amazon_transcripts = company_transcripts_dict[&#39;AMZN&#39;]
adobe_transcripts = company_transcripts_dict[&#39;ADBE&#39;]
apple_transcripts = company_transcripts_dict[&#39;AAPL&#39;]

transcript_samples = [google_transcripts, amazon_transcripts, adobe_transcripts, apple_transcripts]

google_prices = company_price_df[&#39;GOOG&#39;]
amazon_prices = company_price_df[&#39;AMZN&#39;]
adobe_prices = company_price_df[&#39;ADBE&#39;]
apple_prices = company_price_df[&#39;AAPL&#39;]

price_samples = load_company_price_history([&#39;GOOG&#39;, &#39;AMZN&#39;, &#39;ADBE&#39;, &#39;AAPL&#39;], normalize=True)

google_transcripts &lt;/code&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-25d25c9f2426acd772f519e1bb69834e_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1011&quot; data-rawheight=&quot;469&quot;&gt;&lt;p&gt;&lt;b&gt;步骤3：分析数据&lt;/b&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;# Calculate average word count across all company transcripts
companies_avg_word_count = []

for company in company_transcripts_dict:
   company_avg = company_transcripts_dict[company][&#39;count&#39;].mean()
   companies_avg_word_count.append(company_avg)
   
print(&#39;Average word count in transcripts: {}&#39;.format(np.mean(np.array(companies_avg_word_count))))&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;Average word count in transcripts: 9449.255561736534&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;# Plot histogram of word counts for company transcripts
def visualize_word_count(transcripts):
   &#39;&#39;&#39;
   Plots a histogram of a company&#39;s transcript word counts.
   
   Args:
       transcripts: A Pandas DataFrame containing a company&#39;s history of earnings calls.
    &#39;&#39;&#39;
   
   company = transcripts[&#39;company&#39;][0]
   fig, ax = plt.subplots(figsize=(10,5))
   ax.hist(transcripts[&#39;count&#39;])
   plt.title(&quot;Word count distribution for {}&quot;.format(company))
   ax.set_xlabel(&#39;Word count&#39;)
   ax.set_ylabel(&#39;Number of occurrences&#39;)

for company in transcript_samples:
   visualize_word_count(company) &lt;/code&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-3a13f91bf87ce1dd41cf4aa7a193197d_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;594&quot; data-rawheight=&quot;670&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-b50533d8c3f312ecc0a14a4b4ef6387b_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;602&quot; data-rawheight=&quot;669&quot;&gt;&lt;code lang=&quot;python&quot;&gt;# Visualize transcript dates
def visualize_dates(transcripts):
   &#39;&#39;&#39;
   Plots the dates of a company&#39;s earning calls.
   
   Args:
       transcripts: A Pandas DataFrame containing a company&#39;s history of earnings calls.
    &#39;&#39;&#39;
   
   company = transcripts[&#39;company&#39;][0]
   fig, ax = plt.subplots(figsize=(22,5))
   ax.scatter(transcripts.index, np.ones(len(transcripts.index)), marker = &#39;s&#39;, alpha=0.6, s=100)
    fig.autofmt_xdate()
   ax.set_title(&quot;Dates of earnings calls for {}&quot;.format(company))
   ax.set_yticklabels([])

for company in transcript_samples:
   visualize_dates(company) &lt;/code&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-f96f7ae28dabc0b84864c0e9b0f7d109_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1014&quot; data-rawheight=&quot;748&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-a385201c4c6b4cbe725e2b0e988d6fa9_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1008&quot; data-rawheight=&quot;243&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;# Average value of all S&amp;amp;P 500 companies
all_companies = load_company_price_history(company_both, normalize=False)
all_companies.mean(axis=1).plot()
plt.xlabel(&#39;Date&#39;)
plt.ylabel(&#39;Adjusted Closing Price&#39;)
plt.title(&#39;Average Daily Stock Price of all S&amp;amp;P 500 companies&#39;)&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;google_true_prices = load_company_price_history([&#39;GOOG&#39;])
google_true_prices.plot()
plt.xlabel(&#39;Date&#39;)
plt.ylabel(&#39;Adjusted Closing Price&#39;)
plt.title(&#39;Google Stock Price&#39;) &lt;/code&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-0dbce695eec0d9b30e524d5d5b100b54_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;498&quot; data-rawheight=&quot;357&quot;&gt;&lt;code lang=&quot;python&quot;&gt;google_prices.plot()
plt.xlabel(&#39;Date&#39;)
plt.ylabel(&#39;Daily Log Return&#39;)
plt.title(&#39;Google Daily Log Returns&#39;) &lt;/code&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-3d9769eae850775e2f8cf6c2aea1fe9f_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;507&quot; data-rawheight=&quot;357&quot;&gt;&lt;code lang=&quot;python&quot;&gt;price_samples.head(10) &lt;/code&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-9d00fd121f691308ae2be26022c3922d_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;387&quot; data-rawheight=&quot;327&quot;&gt;&lt;code lang=&quot;python&quot;&gt;cum_returns = price_samples.cumsum()
cum_returns.plot()
plt.title(&#39;Cumulative log-returns&#39;) &lt;/code&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-a21031327837ddbe8d597ba505d6ae38_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;487&quot; data-rawheight=&quot;343&quot;&gt;&lt;code lang=&quot;python&quot;&gt;tot_rel_returns = 100*(np.exp(price_samples.cumsum()) - 1)
tot_rel_returns.plot()
plt.title(&#39;Total relative returns&#39;) &lt;/code&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-bf7c34bcfca837e733fea0f34fdc623c_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;490&quot; data-rawheight=&quot;343&quot;&gt;&lt;code lang=&quot;python&quot;&gt;google_price_sample = load_company_price_history([&#39;GOOG&#39;])[&#39;2012&#39;:&#39;2015&#39;]
google_returns_sample = load_company_price_history([&#39;GOOG&#39;], normalize=True)[&#39;2012&#39;:&#39;2015&#39;]
google_transcript_sample = load_company_transcripts(&#39;GOOG&#39;)[&#39;2012&#39;:&#39;2015&#39;]
2017-09-19 20:26:57,937 - DEBUG - Reading company prices for GOOG
2017-09-19 20:26:57,996 - INFO - Null values found after cleaning: False
2017-09-19 20:26:57,998 - DEBUG - Reading company prices for GOOG
2017-09-19 20:26:58,050 - INFO - Null values found after cleaning: False
2017-09-19 20:26:58,052 - DEBUG - Reading company transcripts for GOOG
2017-09-19 20:26:58,093 - WARNING - 1 duplicates removed from file&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Comparing price data with earnings call events&lt;/b&gt;&lt;/h2&gt;&lt;code lang=&quot;python&quot;&gt;def plot_price_and_text(prices, transcripts):
   &#39;&#39;&#39;
   Plots the dates of a company&#39;s earning calls on top of a chart of the company&#39;s stock price.
    
   Args:
       prices: A Pandas DataFrame containing a company&#39;s price history. 
       transcripts: A Pandas DataFrame containing a company&#39;s history of earnings calls.
    &#39;&#39;&#39;
   # Plot the transcript events below the price, 10% offset from min price
   event_level = int(prices.min()*0.9) 
   fig, ax = plt.subplots(figsize=(22,8))
   ax.scatter(transcripts.index, event_level*np.ones(len(transcripts.index)), marker = &#39;s&#39;, alpha=0.4, s=100)
    ax.plot(prices.index, prices)
   fig.autofmt_xdate()
   ax.set_title(&#39;Analyzing the connection between earnings call events and price movement&#39;)
    
plot_price_and_text(google_price_sample, google_transcript_sample) &lt;/code&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-c963935a1f0f8be6e152fa321ec13a55_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1270&quot; data-rawheight=&quot;453&quot;&gt;&lt;h2&gt;&lt;b&gt;Explore connection between text events and returns&lt;/b&gt;&lt;/h2&gt;&lt;code lang=&quot;python&quot;&gt;def plot_returns_and_text(returns, transcripts):
   # Plot the transcript events below the price, 10% offset from min price
   event_level = int(returns.min()*0.9) 
   fig, ax = plt.subplots(figsize=(22,8))
   #ax.scatter(transcripts.index, 0.1*np.ones(len(transcripts.index)), marker = &#39;s&#39;, alpha=0.4, s=100)
    ax.plot(returns.index, returns)
   for date in transcripts.index:
       ax.axvspan(date - pd.to_timedelta(&#39;1 days&#39;), date + pd.to_timedelta(&#39;6 days&#39;), color=&#39;green&#39;, alpha=0.15)
    fig.autofmt_xdate()
   ax.set_title(&#39;Analyzing the connection between earnings call events and price movement&#39;)
    ax.set_ylabel(&#39;Daily return&#39;)

plot_returns_and_text(google_returns_sample, google_transcript_sample) &lt;/code&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-01746fce48b6c1f4714190a8d89fc62e_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1288&quot; data-rawheight=&quot;453&quot;&gt;&lt;code lang=&quot;python&quot;&gt;plot_returns_and_text(load_company_price_history([&#39;CMG&#39;], normalize=True)[&#39;2012&#39;:&#39;2015&#39;],
                      load_company_transcripts(&#39;CMG&#39;)[&#39;2012&#39;:&#39;2015&#39;]) &lt;/code&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-28506d896c3c00dade2cdeab8df3c2d9_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1274&quot; data-rawheight=&quot;453&quot;&gt;&lt;p&gt;Clearly, we can see some examples of large price movements surrounding the time of quarterly earnings calls. The goal of this project is to develop an algorithm capable of learning the price movements associated with the content of an earnings call.&lt;/p&gt;&lt;p&gt;&lt;b&gt;步骤4：将文本转换为文字嵌入&lt;/b&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;with open(&#39;glove.6B/glove.6B.50d.txt&#39;) as words:
   w2v = {word.split()[0]: np.vectorize(lambda x: float(x))(word.split()[1:]) for word in words}

logging.info(&#39;{} words in word2vec dictionary.&#39;.format(len(w2v)))

# We&#39;ll later reduce the dimensionality from 50 to 2, let&#39;s go ahead and fit the entire corpus
# I&#39;ve opted to use PCA over t-SNE given that we can fit the transformer once and have deterministic results
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
reduced_embeddings = pca.fit_transform(list(w2v.values()))
w2v_reduced = dict(zip(list(w2v.keys()), reduced_embeddings.tolist()))&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;2017-10-04 09:55:50,937 - INFO - 400000 words in word2vec dictionary.&lt;/code&gt;&lt;p&gt;我们创建了一个字典，其中的关键字代表了我们词汇表中的单词，值是给定单词的向量表示。可以通过查询字典来访问单词的50维向量。并将50维矢量投影到二维中，以便于观察。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;w2v[&#39;example&#39;]
array([ 0.51564  ,  0.56912  , -0.19759  ,  0.0080456,  0.41697  ,
        0.59502  , -0.053312 , -0.83222  , -0.21715  ,  0.31045  ,
        0.09352  ,  0.35323  ,  0.28151  , -0.35308  ,  0.23496  ,
        0.04429  ,  0.017109 ,  0.0063749, -0.01662  , -0.69576  ,
        0.019819 , -0.52746  , -0.14011  ,  0.21962  ,  0.13692  ,
       -1.2683   , -0.89416  , -0.1831   ,  0.23343  , -0.058254 ,
        3.2481   , -0.48794  , -0.01207  , -0.81645  ,  0.21182  ,
       -0.17837  , -0.02874  ,  0.099358 , -0.14944  ,  0.2601   ,
        0.18919  ,  0.15022  ,  0.18278  ,  0.50052  , -0.025532 ,
        0.24671  ,  0.10596  ,  0.13612  ,  0.0090427,  0.39962  ])



w2v_reduced[&#39;example&#39;]
[4.092878121172412, 1.785939893037579]&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;# Sample transcripts from collection
sample_text_google = google_transcripts[&#39;body&#39;][5]
sample_text_amazon = amazon_transcripts[&#39;body&#39;][5]
sample_text_adobe = adobe_transcripts[&#39;body&#39;][5]&lt;/code&gt;&lt;p&gt;Let&#39;s see what words were ignored when we translate the transcripts to word embeddings.&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;from keras.preprocessing.text import text_to_word_sequence

not_in_vocab = set([word for word in text_to_word_sequence(sample_text_google) if word not in w2v])
print(&#39;  --  &#39;.join(not_in_vocab))
motofone  --  segment&#39;s  --  we&#39;re  --  ml910  --  here&#39;s  --  we&#39;ve  --  doesn&#39;t  --  didn&#39;t  --  ed&#39;s  --  z6  --  asp&#39;s  --  vhub  --  p2k  --  weren&#39;t  --  you&#39;re  --  3gq  --  i&#39;ll  --  ray&#39;s  --  wasn&#39;t  --  they&#39;re  --  what&#39;s  --  i&#39;d  --  motowi4  --  world&#39;s  --  embracement  --  downish  --  broadbus  --  hereon  --  devices&#39;  --  shippable  --  w355  --  motorola&#39;s  --  w205  --  that’s  --  isn&#39;t  --  morning&#39;s  --  mw810  --  wimax&#39;s  --  that&#39;s  --  wouldn&#39;t  --  ounjian  --  let&#39;s  --  w215  --  dan&#39;s  --  motoming  --  organization&#39;s  --  krzr  --  reprioritizing  --  w510  --  mc50  --  greg&#39;s  --  today&#39;s  --  mc70  --  terry&#39;s  --  we&#39;ll  --  company&#39;s  --  don&#39;t  --  5ish  --  haven&#39;t  --  kvaal  --  you&#39;ve  --  you&#39;ll  --  can&#39;t  --  nottenburg  --  motorokr  --  what’s  --  mc35  --  i&#39;ve  --  metlitsky  --  there&#39;s  --  july&#39;s  --  w370  --  i&#39;m  --  it&#39;s  --  motorizr&lt;/code&gt;&lt;p&gt;词嵌入字典不支持连词。 然而，这应该是正确的，因为大多数会被视为停用词。&lt;br&gt;A note on stopwords, these are words that are very commonly used and their presence does little to convey a unique signature of a body of text. They&#39;re useful in everyday conversations, but when you&#39;re identifying text based on the frequency of words used, they&#39;re next to useless.&lt;/p&gt;&lt;p&gt;&lt;b&gt;TF-IDF权重&lt;/b&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer

def get_tfidf_values(documents, norm=None):
   count_vec = CountVectorizer()
   counts = count_vec.fit_transform(documents)
   words = np.array(count_vec.get_feature_names())
   
   transformer = TfidfTransformer(norm=norm)
   tfidf = transformer.fit_transform(counts)
   tfidf_arr = tfidf.toarray()
   
   tfidf_documents = []
   for i in range(len(documents)):
       tfidf_doc = {}
       for word, tfidf in zip(words[np.nonzero(tfidf_arr[i, :])], tfidf_arr[i, :][np.nonzero(tfidf_arr[i, :])]):
            tfidf_doc[word] = tfidf
       tfidf_documents.append(tfidf_doc)
   return tfidf_documents

def docs_to_3D(tfidf_documents, w2v_reduced):
   text_docs_3D = []
   
   for i, doc in enumerate(tfidf_documents): # list of documents with word:tfidf
       data = []
       for k, v in tfidf_documents[i].items():
           try:
               item = w2v_reduced[k][:] # Copy values from reduced embedding dictionary
               item.append(v) # Append the TFIDF score
               item.append(k) # Append the word
               data.append(item) # Add [dim1, dim2, tfidf, word] to collection
           except: # If word not in embeddings dictionary
               continue 

       df = pd.DataFrame(data, columns=[&#39;dim1&#39;, &#39;dim2&#39;, &#39;tfidf&#39;, &#39;word&#39;])
       df = df.set_index([&#39;word&#39;])

       text_docs_3D.append(df)
       
   return text_docs_3D

from mpl_toolkits.mplot3d import Axes3D
``` &lt;/code&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-5b36b5e161decc74ddf1cba44c2fe957_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;572&quot; data-rawheight=&quot;558&quot;&gt;&lt;code lang=&quot;python&quot;&gt;plt.hist(text_docs_3D[0][&#39;tfidf&#39;].apply(np.sqrt), range=(0,20))
(array([ 911.,  679.,   88.,   15.,   12.,    3.,    2.,    1.,    2.,    2.]),
 array([  0.,   2.,   4.,   6.,   8.,  10.,  12.,  14.,  16.,  18.,  20.]),

&lt;/code&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-eb461245551568b76e3c7eea101adcec_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;484&quot; data-rawheight=&quot;329&quot;&gt;&lt;p&gt;&lt;b&gt;Evolution of company transcripts over time&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Enhancement: rather than providing a list of word embedding vectors to plot, pass a dictionary of word:vector pairs so that user can hover mouse over points to see what words are.&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;from matplotlib import rc
# equivalent to rcParams[&#39;animation.html&#39;] = &#39;html5&#39;
rc(&#39;animation&#39;, html=&#39;html5&#39;)

from matplotlib.animation import FuncAnimation
from IPython.display import HTML

import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D


def animate_company_transcripts_3D(vis_docs):
   fig = plt.figure()
   ax = fig.add_subplot(111, projection=&#39;3d&#39;)
   ax.set_xlim([-3, 6])
   ax.set_ylim([-3, 6])
   ax.set_zlim([0, 20])
   ax.set_xlabel(&#39;Word Embedding&#39;)
   ax.set_ylabel(&#39;Word Embedding&#39;)
   ax.set_zlabel(&#39;Word Importance&#39;)

   text = vis_docs[0]
   scatter = ax.scatter(text[&#39;dim1&#39;], text[&#39;dim2&#39;], text[&#39;tfidf&#39;], alpha=0.1, 
                        zdir=&#39;z&#39;, s=20, c=None, depthshade=True, animated=True)

   def update(frame_number):
       text = vis_docs[frame_number]
       scatter._offsets3d = (text[&#39;dim1&#39;], text[&#39;dim2&#39;], text[&#39;tfidf&#39;].apply(np.sqrt))
       return scatter

   return FuncAnimation(fig, update, frames=len(vis_docs), interval=300, repeat=True)


tfidf_docs = get_tfidf_values(google_transcripts[&#39;body&#39;])
text_docs_3D = docs_to_3D(tfidf_docs, w2v_reduced)
animate_company_transcripts_3D(text_docs_3D) &lt;/code&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-edf33f4a1c3f6f3262f7c4b3e5096660_r.gif&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;640&quot; data-rawheight=&quot;440&quot; data-thumbnail=&quot;https://pic4.zhimg.com/v2-edf33f4a1c3f6f3262f7c4b3e5096660_b.jpg&quot;&gt;&lt;p&gt;&lt;b&gt;Digitize input space for ConvNet&lt;/b&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;def digitize_embedding_space(text_docs_3D, index, bins=250):
   binned_docs = []
   for frame, data in enumerate(text_docs_3D):
       doc = text_docs_3D[frame]

       # Sort collection of word embeddings in continous vector space to a 2D array of bins. Take square root of 
        # TF-IDF score as a means of scaling values to prevent a small number of terms from being too dominant.
        hist = np.histogram2d(doc[&#39;dim1&#39;], doc[&#39;dim2&#39;], weights=doc[&#39;tfidf&#39;].apply(np.sqrt), bins=bins)[0]
        binned_docs.append(hist)

   # Technically, you shouldn&#39;t store numpy arrays as a Series
   # Somehow, I was able to hack my way around that, but when you try to reindex the Series it throws an error
    # It was convenient to use the Series groupby function, though
   # NOTE: This should be revisited at some point using xarray or some other more suitable data store
    text_3D = pd.Series(binned_docs, index=index)

   # Combine same-day events
   if text_3D.index.duplicated().sum() &amp;gt; 0:
       logging.info(&#39;{} same-day events combined.&#39;.format(text_3D.index.duplicated().sum()))
    text_3D = text_3D.groupby(text_3D.index).apply(np.mean)
   
   # Now I&#39;ll convert the Series of numpy 2d arrays into a list of numpy 2d array (losing the date index)
    # and create another Series that ties the date to the list index of text_docs 
   text_docs = text_3D.values.tolist()
   lookup = pd.Series(range(len(text_docs)), index = text_3D.index)
   
   return text_docs, lookup&lt;/code&gt;&lt;p&gt;&lt;b&gt;Develop full text processing pipeline&lt;/b&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;def process_text_for_input(documents, w2v_reduced, norm=None):
   index = documents.index
   tfidf_docs = get_tfidf_values(documents, norm=norm)
   text_docs_3D = docs_to_3D(tfidf_docs, w2v_reduced)
   text_docs, lookup = digitize_embedding_space(text_docs_3D, index)
   return text_docs, lookup

# Test out pre-processing pipeline
text_docs, lookup = process_text_for_input(google_transcripts[&#39;body&#39;], w2v_reduced)
2017-09-19 17:28:52,774 - INFO - 2 same-day events combined.&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;步骤五：ARIMA 模型&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;探索数据的统计属性&lt;/b&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;# Note: this cell was copied from source as cited. 

# TSA from Statsmodels
import statsmodels.api as sm
import statsmodels.formula.api as smf
import statsmodels.tsa.api as smt
from statsmodels.graphics.api import qqplot

def tsplot(y, lags=None, title=&#39;&#39;, figsize=(14, 8)):
   &#39;&#39;&#39;Examine the patterns of ACF and PACF, along with the time series plot and histogram.
    Original source: https://tomaugspurger.github.io/modern-7-timeseries.html
   &#39;&#39;&#39;
```&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;# Load a few companies for inspection
company_price_ARIMA = load_company_price_history([&#39;GOOG&#39;, &#39;AAPL&#39;, &#39;AMZN&#39;, &#39;CA&#39;, &#39;MMM&#39;])

# Select a company and sample a two year time period, reindexing to have a uniform frequency
google_price_ARIMA = company_price_ARIMA[&#39;GOOG&#39;][&#39;2012&#39;:&#39;2013&#39;]

apple_price_ARIMA = company_price_ARIMA[&#39;AAPL&#39;][&#39;2012&#39;:&#39;2013&#39;]

logging.info(&quot;Index frequency: {}&quot;.format(google_price_ARIMA.index.freq))&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;tsplot(google_price_ARIMA[&#39;2013&#39;], title=&#39;Google price for 2013&#39;, lags =40) &lt;/code&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-932b8d1c0da650a5ac7f0a28c431c465_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1004&quot; data-rawheight=&quot;568&quot;&gt;&lt;code lang=&quot;python&quot;&gt;tsplot(apple_price_ARIMA[&#39;2013&#39;], title=&#39;Apple price for 2013&#39;, lags = 40) &lt;/code&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-a5b8936a9df01f421711cae9015d82bb_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1004&quot; data-rawheight=&quot;559&quot;&gt;&lt;p&gt;看看自相关图，时间序列数据高度依赖于它的历史。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;google_returns_2013 = np.log(google_price_ARIMA[&#39;2013&#39;]).diff()[1:]
apple_returns_2013 = np.log(apple_price_ARIMA[&#39;2013&#39;]).diff()[1:]

tsplot(google_returns_2013, title=&#39;Google 2013 log returns&#39;, lags = 40) &lt;/code&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-36e2f54e9f057ab0871cee0d5707e72a_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;989&quot; data-rawheight=&quot;563&quot;&gt;&lt;code lang=&quot;python&quot;&gt;tsplot(apple_returns_2013, title=&#39;Apple 2013 log returns&#39;, lags = 40) &lt;/code&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-c1804ace2c71592c9f7a5d4a48f7ffc0_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;995&quot; data-rawheight=&quot;567&quot;&gt;&lt;p&gt;注意，一阶差分产生时间序列的平稳性。 因此，我们应该执行d = 1。&lt;/p&gt;&lt;p&gt;返回大概遵循随机游走，其中当前时间步与先前的任何时间步不相关。 这表明一个ARIMA（0,1,0）模型，其中最好的预测可以使它成为一个常数值。&lt;/p&gt;&lt;p&gt;&lt;b&gt;网格搜索最佳的ARIMA参数&lt;/b&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;import itertools

import warnings
warnings.filterwarnings(&quot;ignore&quot;) # Ignore convergence warnings

def grid_search_SARIMA(y, pdq_min, pdq_max, seasonal_period):
   p = d = q = range(pdq_min, pdq_max+1)
   pdq = list(itertools.product(p, d, q))
   seasonal_pdq = [(x[0], x[1], x[2], seasonal_period) for x in list(itertools.product(p, d, q))]
    
   best_params = []
   best_seasonal_params = []
   score = 1000000000000 # this is a bit of a hack
   
   for param in pdq:
       for param_seasonal in seasonal_pdq:
           try:
               mod = sm.tsa.statespace.SARIMAX(y,
                                               order=param,
                                               seasonal_order=param_seasonal,
                                               enforce_stationarity=False,
                                               enforce_invertibility=False)

               results = mod.fit()
               logging.info(&#39;ARIMA{}x{}12 - AIC:{}&#39;.format(param, param_seasonal, results.aic))
                if results.aic &amp;lt; score:
                   best_params = param
                   best_seasonal_params = param_seasonal
                   score = results.aic
           except:
               continue
   logging.info(&#39;\n\nBest ARIMA{}x{}12 - AIC:{}&#39;.format(best_params, best_seasonal_params, score))
    return best_params, best_seasonal_params, score


params, seasonal_params, score = grid_search_SARIMA(google_price_ARIMA, 0, 2, 12)&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;ARIMA(0, 0, 0)x(0, 0, 1, 12)12 - AIC:6898.362293047701
ARIMA(0, 0, 0)x(0, 0, 2, 12)12 - AIC:6200.392463920813
ARIMA(0, 0, 0)x(0, 1, 1, 12)12 - AIC:4377.816871987012
ARIMA(0, 0, 0)x(0, 1, 2, 12)12 - AIC:4278.05126125979
```&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;mod = sm.tsa.statespace.SARIMAX(google_price_ARIMA,
                               order=(0, 1, 2),
                               seasonal_order=(0, 1, 2, 12),
                               enforce_stationarity=False,
                               enforce_invertibility=False)

results = mod.fit()

print(results.summary().tables[1]) &lt;/code&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-59d5dd7eac6f21e6010f5344f1b436b3_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;560&quot; data-rawheight=&quot;151&quot;&gt;&lt;code lang=&quot;python&quot;&gt;pred = results.get_prediction(start = pd.to_datetime(&#39;2013-10-1&#39;), end = pd.to_datetime(&#39;2013-12-31&#39;), dynamic=False)
pred_ci = pred.conf_int()

fig, ax = plt.subplots(figsize=(20,10))
ax.plot(google_price_ARIMA.index, google_price_ARIMA, 
       label=&#39;Observed stock price&#39;)
ax.plot(pred.predicted_mean.index, pred.predicted_mean, 
       label=&#39;One-step ahead forecast&#39;, alpha=.7)
fig.autofmt_xdate()
ax.fill_between(pred_ci.index,
               pred_ci.iloc[:, 0],
               pred_ci.iloc[:, 1], color=&#39;k&#39;, alpha=.2)
ax.set_xlabel(&#39;Date&#39;)
ax.set_ylabel(&#39;Stock price&#39;)
ax.set_title(&#39;In sample one-step prediction&#39;)
plt.legend() &lt;/code&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-453b961c9b83ccc29ca1e9485e6afa1d_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1168&quot; data-rawheight=&quot;565&quot;&gt;&lt;code lang=&quot;python&quot;&gt;pred_dynamic = results.get_prediction(start = pd.to_datetime(&#39;2013-11-1&#39;), dynamic=True, full_results=True)
pred_ci_dynamic = pred_dynamic.conf_int()

fig, ax = plt.subplots(figsize=(20,10))
ax.plot(google_price_ARIMA.index, google_price_ARIMA, 
       label=&#39;Observed stock price&#39;)
ax.plot(pred_dynamic.predicted_mean.index, pred_dynamic.predicted_mean, 
       label=&#39;Dynamic forecast&#39;, alpha=.7)
fig.autofmt_xdate()
ax.fill_between(pred_ci_dynamic.index,
               pred_ci_dynamic.iloc[:, 0],
               pred_ci_dynamic.iloc[:, 1], color=&#39;k&#39;, alpha=.2)
ax.set_xlabel(&#39;Date&#39;)
ax.set_ylabel(&#39;Google stock price&#39;)
ax.set_title(&#39;In sample dynamic prediction&#39;)
ax.set_ylim(200,600)
plt.legend() &lt;/code&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-41aabc55f2ce74227232c04ffc4326e4_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1168&quot; data-rawheight=&quot;565&quot;&gt;&lt;code lang=&quot;python&quot;&gt;# Get forecast 500 steps ahead in future
pred_future = results.get_forecast(steps=100)

# Get confidence intervals of forecasts
pred_future_ci = pred_future.conf_int()

fig, ax = plt.subplots(figsize=(20,10))
ax.plot(company_price_ARIMA[&#39;GOOG&#39;][&#39;2013&#39;:&#39;2014-05-01&#39;].index, company_price_ARIMA[&#39;GOOG&#39;][&#39;2013&#39;:&#39;2014-05-01&#39;], 
        label=&#39;Observed stock price&#39;)
ax.plot(pd.to_datetime(pred_future.predicted_mean.index), pred_future.predicted_mean, 
       label=&#39;Dynamic forecast&#39;, alpha=.7)
fig.autofmt_xdate()
ax.fill_between(pred_future_ci.index,
               pred_future_ci.iloc[:, 0],
               pred_future_ci.iloc[:, 1], color=&#39;k&#39;, alpha=.2)
ax.set_xlabel(&#39;Date&#39;)
ax.set_ylabel(&#39;Google stock price 2013&#39;)
ax.set_title(&#39;Out of sample prediction&#39;)
ax.set_ylim(200,800)
plt.legend() &lt;/code&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-79a740c4c456f5b333fd0f40e9ec5174_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1168&quot; data-rawheight=&quot;565&quot;&gt;&lt;p&gt;&lt;b&gt;训练&lt;/b&gt;&lt;/p&gt;&lt;p&gt;除非时间序列数据是由同一过程生成的，否则ARIMA模型通常不会在时间序列上进行训练。 对于股票价格的情况，对个别公司的影响是独立的，对所有公司的影响并不一致。 因此，5种不同的ARIMA模型将针对不同的公司进行训练，分别评估每个模型并平均每个结果。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;google_arima_train = company_price_df[&#39;GOOG&#39;][&#39;2009&#39;:&#39;2012&#39;]
amazon_arima_train = company_price_df[&#39;AMZN&#39;][&#39;2009&#39;:&#39;2012&#39;]
mmm_arima_train = company_price_df[&#39;MMM&#39;][&#39;2009&#39;:&#39;2012&#39;]
chipotle_arima_train = company_price_df[&#39;CMG&#39;][&#39;2009&#39;:&#39;2012&#39;]
duke_arima_train = company_price_df[&#39;DUK&#39;][&#39;2009&#39;:&#39;2012&#39;]

companies_train = [google_arima_train, amazon_arima_train, mmm_arima_train, chipotle_arima_train, duke_arima_train]

google_arima_test = company_price_df[&#39;GOOG&#39;][&#39;2013&#39;:&#39;2014&#39;]
amazon_arima_test = company_price_df[&#39;AMZN&#39;][&#39;2013&#39;:&#39;2014&#39;]
mmm_arima_test = company_price_df[&#39;MMM&#39;][&#39;2013&#39;:&#39;2014&#39;]
chipotle_arima_test = company_price_df[&#39;CMG&#39;][&#39;2013&#39;:&#39;2014&#39;]
duke_arima_test = company_price_df[&#39;DUK&#39;][&#39;2013&#39;:&#39;2014&#39;]

companies_test = [google_arima_test, amazon_arima_test, mmm_arima_test, chipotle_arima_test, duke_arima_test]

company_results = []
for company_price in companies_train:
   model = sm.tsa.statespace.SARIMAX(company_price,
                               order=(0, 1, 2),
                               seasonal_order=(0, 1, 2, 12),
                               enforce_stationarity=False,
                               enforce_invertibility=False)

   results = model.fit()
   company_results.append(results)&lt;/code&gt;&lt;p&gt;&lt;b&gt;评估&lt;/b&gt;&lt;/p&gt;&lt;p&gt;我们将在三个时间尺度上评估每个模型：5天预测，20天预测和100天预测。 长期预测非常困难，特别是对于随机过程；因此，短期预测提供了一个更合理的业绩衡量标准。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;forecast_5_day = []
forecast_20_day = []
forecast_100_day = []

for result in company_results:
   forecast_5_day.append(result.get_forecast(steps=5))
   forecast_20_day.append(result.get_forecast(steps=20))
   forecast_100_day.append(result.get_forecast(steps=100))

from sklearn.metrics import mean_absolute_error

forecast_5_day_mae = []
for true, pred in zip(companies_test, forecast_5_day):
   forecast_5_day_mae.append(mean_absolute_error(true.iloc[0:5], pred.predicted_mean))
   
forecast_20_day_mae = []
for true, pred in zip(companies_test, forecast_20_day):
   forecast_20_day_mae.append(mean_absolute_error(true.iloc[0:20], pred.predicted_mean))
    
forecast_100_day_mae = []
for true, pred in zip(companies_test, forecast_100_day):
   forecast_100_day_mae.append(mean_absolute_error(true.iloc[0:100], pred.predicted_mean))
    
   
print(&#39;Average MAE across companies (5 day): {:.6f}&#39;.format(np.mean(forecast_5_day_mae)))
print(&#39;Average MAE across companies (20 day): {:.6f}&#39;.format(np.mean(forecast_20_day_mae)))
print(&#39;Average MAE across companies (100 day): {:.6f}&#39;.format(np.mean(forecast_100_day_mae)))
Average MAE across companies (5 day): 0.009500
Average MAE across companies (20 day): 0.009690
Average MAE across companies (100 day): 0.009965&lt;/code&gt;&lt;p&gt;让我们来看看长期预测是怎样的。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;fig, ax = plt.subplots(figsize=(20,10))
ax.plot(google_arima_test.iloc[0:100], label=&#39;True&#39;)
ax.plot(forecast_100_day[0].predicted_mean, label=&#39;Pred&#39;)
ax.set_title(&#39;ARIMA: 100 Day Forecast of Google (starting Jan 1, 2013)&#39;)
plt.legend() &lt;/code&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-b8f283b03c930910c7fd64443d954384_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1162&quot; data-rawheight=&quot;588&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;步骤六：LSTM 模型&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;数据准备&lt;/b&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;# Load a test company for inspection
LSTM_company_prices = load_company_price_history([&#39;GOOG&#39;, &#39;AMZN&#39;, &#39;MMM&#39;, &#39;CMG&#39;, &#39;DUK&#39;], normalize = True)

LSTM_prices_train = LSTM_company_prices[&#39;2009&#39;:&#39;2012&#39;]
LSTM_prices_test = LSTM_company_prices[&#39;2013&#39;:&#39;2014&#39;]
LSTM_prices_val = LSTM_company_prices[&#39;2015&#39;:&#39;2016&#39;]

def price_generator(data, window=180, batch_size=128):
   
   timesteps = data.shape[0]
   companies = data.shape[1]
   
   if window + batch_size &amp;gt; timesteps:
       logging.warning(&#39;Not enough data to fill a batch, forcing smaller batch size.&#39;)
       batch_size = timesteps - window
   
   # Index to keep track of place within price timeseries, corresponds with the last day of output
    i = window
   
   # Index to keep track of which company to query the data from
   j = 0
   
   while True:
       # If there aren&#39;t enough sequential days to fill a batch, go to next company
       if i + batch_size &amp;gt;= timesteps:
           i = window
           
           # If end of companies has been reached, start back at first company
           if j+1 &amp;gt;= companies:
               j=0
           else:
               j+=1
           
       
       samples = np.arange(i, i + batch_size)
       i += len(samples)
       
       # 代码太多略去&lt;/code&gt;&lt;p&gt;&lt;b&gt;建立模型&lt;/b&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;from keras.models import Sequential
from keras.layers import Dense, TimeDistributed, LSTM


# define LSTM configuration
n_features = 1 # only price
window = 180 # look back 50 days
batch_size = 128

# create LSTM
price_only_model = Sequential()
price_only_model.add(LSTM(20, input_shape=(window, n_features), return_sequences=True))
price_only_model.add(LSTM(60, return_sequences=True))
price_only_model.add(TimeDistributed(Dense(1)))
print(price_only_model.summary()) &lt;/code&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-63e452f69beaf301507bc9a4022137b7_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;478&quot; data-rawheight=&quot;235&quot;&gt;&lt;code lang=&quot;python&quot;&gt;from keras.callbacks import ModelCheckpoint 

price_only_model.compile(loss=&#39;mean_absolute_error&#39;, optimizer=&#39;adam&#39;)

checkpointer = ModelCheckpoint(filepath=&#39;saved_models/weights.best.price_only.hdf5&#39;, 
                              verbose=1, save_best_only=True)

# Train LSTM
history = price_only_model.fit_generator(train_gen, steps_per_epoch=train_steps, epochs=3, callbacks=[checkpointer], 
                                         validation_data=val_gen, validation_steps=val_steps) &lt;/code&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-09df422176fc1c3eb56e20ec0082fde5_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1014&quot; data-rawheight=&quot;191&quot;&gt;&lt;code lang=&quot;python&quot;&gt;# Load weights from previous training
price_only_model.load_weights(&#39;saved_models/weights.best.price_only.hdf5&#39;)

def validation_curve(history):
   loss = history.history[&#39;loss&#39;]
   val_loss = history.history[&#39;val_loss&#39;]
   epochs = range(len(loss))

   plt.figure()
   plt.plot(epochs, loss, &#39;g&#39;, label=&#39;Training loss&#39;)
   plt.plot(epochs, val_loss, &#39;b&#39;, label=&#39;Validation loss&#39;)
   plt.title(&#39;Training and validation loss&#39;)
   plt.legend()
   
   validation_curve(history) &lt;/code&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-059297d13a55ee14632f06b099bec6f0_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;504&quot; data-rawheight=&quot;343&quot;&gt;&lt;p&gt;&lt;b&gt;评估模型&lt;/b&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;def forecast(seed_data, forecast_steps, model):
   &#39;&#39;&#39;
   Forecast future returns by making day-by-day predictions.
   
   Args:
       seed_data: Initial input sequence.
       forecast_steps: Defines how many steps into the future to predict.
       model: Trained LSTM prediction model. 
   &#39;&#39;&#39;
   
   future = []
   
   for timestep in range(forecast_steps):
       pred = model.predict(seed_data)[0][-1][0]
       future.append(pred)
       seed_data = np.append(seed_data[0][1:], [pred]).reshape(1, seed_data.shape[1], 1)

   return future

    # 代码太多略去
&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;fig, ax = plt.subplots(figsize=(20,10))
ax.plot(company_prices_test.iloc[0:100, 0].values, label=&#39;True&#39;)
ax.plot(forecast_100_day[0], label=&#39;Pred&#39;)
ax.set_title(&#39;LSTM: 100 Day Forecast of Google (starting Jan 1, 2013)&#39;)
plt.legend() &lt;/code&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-27581638674e73013b7cd3ec65bad300_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1162&quot; data-rawheight=&quot;588&quot;&gt;&lt;p&gt;&lt;b&gt;测试评估&lt;/b&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;mae = price_only_model.evaluate_generator(test_gen, steps=test_steps)
print(&#39;Mean absolute error on test data: {:.6f}&#39;.format(mae))
Mean absolute error on test data: 0.009176&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;步骤七：ConvNet特征提取&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;在本节中，我们将为ConvNet模型准备数据，开发一个能够根据文本信息预测价格变动的模型，并从网络的最后一层提取特征，以便在卷积网络中输入。&lt;/p&gt;&lt;p&gt;只贴出核心代码&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;def text_generator(price_data, text_data, w2v_reduced, window=5, batch_size=1):
   companies = len(text_data)
   
   # Start with first transcript
   i = 0
   
   # Start with first company
   j = 0
   
   text_docs, lookup = process_text_for_input(text_data[j][&#39;body&#39;], w2v_reduced, norm=&#39;l2&#39;)
    
   while True:
       # If end of transcripts reached, go to next company
       if i &amp;gt;= len(lookup):
           i = 0
           if j+1 &amp;gt;= companies:
               j=0
           else:
               j+=1
               
           text_docs, lookup = process_text_for_input(text_data[j][&#39;body&#39;], w2v_reduced, norm=&#39;l2&#39;)
        
       
       event = lookup.index[i]
       text = text_docs[i].reshape(1, text_docs[i].shape[0], text_docs[i].shape[1], 1)
       price_target = company_prices[event : event + pd.to_timedelta(&#39;{} days&#39;.format(window+1))].iloc[:,j].sum()
        
       yield text, np.array(price_target).reshape(1)
       
       i+= 1
``` &lt;/code&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-8cad8fd8a0868cf6ab7395b8f771634f_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;493&quot; data-rawheight=&quot;343&quot;&gt;&lt;code lang=&quot;python&quot;&gt;# Run this cell to inspect a new example
text, price = next(test_gen)

print(&#39;Predicted return:\t{}&#39;.format(text_model.predict(text)[0][0]))
print(&#39;True return:\t\t{}&#39;.format(price[0]))
Predicted return:	0.01737634837627411
True return:		0.06355231462064292&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;mae = text_model.evaluate_generator(test_gen, steps=test_steps)
print(&#39;Mean absolute error on test data: {}&#39;.forma
Mean absolute error on test data: 0.057885023705180616&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;步骤八：LSTM价格+测试模型&lt;/b&gt;&lt;/h2&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-dcc8a7970645f6ceda1a03d8b8995e8a_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;735&quot; data-rawheight=&quot;479&quot;&gt;&lt;code lang=&quot;python&quot;&gt;companies = [&#39;GOOG&#39;, &#39;AMZN&#39;, &#39;MMM&#39;, &#39;CMG&#39;, &#39;DUK&#39;]

company_transcripts_train = [load_company_transcripts(company)[&#39;2009&#39;:&#39;2012&#39;] for company in companies]
company_prices_train = load_company_price_history(companies, normalize=True)[&#39;2009&#39;:&#39;2012&#39;]

```
window_size = 180
batch_size = 128
text_features = 10

train_gen = price_text_generator(company_prices_train, company_transcripts_train, w2v_reduced, extract_features, 
                                 text_features=text_features, window=window_size, batch_size=batch_size)
train_steps = (company_prices_train.shape[0] // batch_size)*company_prices_train.shape[1]

```

def price_text_generator(price_data, text_data, w2v_reduced, extract_features_model, text_features, 
                         window=180, batch_size=128):
```

# Train LSTM
history = price_text_model.fit_generator(train_gen, steps_per_epoch=train_steps, epochs=3, callbacks=[checkpointer], 
                                         validation_data=val_gen, validation_steps=val_steps)

# Load weights from previous training
price_text_model.load_weights(&#39;saved_models/weights.best.price_with_text_features.hdf5&#39;)

validation_curve(history)

mae = price_text_model.evaluate_generator(test_gen, steps=test_steps)
print(&#39;Mean absolute error on test data: {:.6f}&#39;.format(mae)) &lt;/code&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-46a85f3ee5bb9e9895a6157344783e00_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;498&quot; data-rawheight=&quot;343&quot;&gt;&lt;code lang=&quot;python&quot;&gt;2017-09-22 14:29:52,468 - INFO - 1 same-day events combined.
2017-09-22 14:29:55,605 - INFO - 1 same-day events combined.
Mean absolute error on test data: 0.008930&lt;/code&gt;&lt;p&gt;链接: https://pan.baidu.com/s/1pMLdP1L &lt;/p&gt;&lt;p&gt;密码: x6mj&lt;/p&gt;&lt;p&gt;谢谢大家对公众号的一直以来厚爱！&lt;/p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653287197&amp;amp;idx=1&amp;amp;sn=9630389a52c7d0be4c1feaf3a534c2ce&amp;amp;chksm=802e3108b759b81ed11174f71b23fb73abe5c4ebad0f9d480b6efbd8f7e644de6b2232dc63fa#rd&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-2e9f227f80bf31b439e47581a6b43f2b&quot; data-image-width=&quot;640&quot; data-image-height=&quot;640&quot; data-image-size=&quot;ipico&quot;&gt;【代码+论文】通过ML、Time Series模型学习股价行为（第九期免费赠书活动来啦！）&lt;/a&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-2593c949ae5b55159619364a33f21f8e_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1096&quot; data-rawheight=&quot;372&quot;&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>量化投资机器学习</author>
<guid isPermaLink="false">2018-01-21-33152954</guid>
<pubDate>Sun, 21 Jan 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>机器学习应用区块链系列（一）——如何开发一套自己的智能合约系统</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/2018-01-21-33152738.htm</link>
<description>&lt;meta charset=&quot;utf-8&quot;&gt;
&lt;title&gt;机器学习应用区块链系列（一）——如何开发一套自己的智能合约系统&lt;/title&gt;
&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/33152738&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-5627788616a10bf169cd136fc01f8e65_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;&lt;b&gt;从今天开始&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;编辑部将带来机器学习应用区块链系列&lt;/b&gt;&lt;/p&gt;&lt;p&gt;由于是第一期，我们想解读一些国外已有的文献和研究。故带来了START-Summit-2017-Blockchain-Machine-Learning-Workshop的演讲稿和示例代码，希望能够给大家带来一些启迪。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;来源&lt;/b&gt;&lt;/h2&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653287256&amp;amp;idx=2&amp;amp;sn=483e71c66e863aec424ff0c1936411c6&amp;amp;chksm=802e314db759b85b505f7fd1ed0cb0cacf10ed4bbbf86cc50c7bbc9d07d4ac0fed3b783b2278##&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-2e9f227f80bf31b439e47581a6b43f2b&quot; data-image-width=&quot;640&quot; data-image-height=&quot;640&quot; data-image-size=&quot;ipico&quot;&gt;机器学习应用区块链系列（一）--如何开发一套自己的智能合约系统&lt;/a&gt;&lt;h2&gt;&lt;b&gt;介绍&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;代码的目的是用一个简单的例子来演示如何把区块链技术，智能合约和机器学习结合在一起。 &lt;b&gt;（代码在文末下载）&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;代码文件&lt;/b&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;- runTestnet.sh: launches a local development Blockchain for easy testing
- contract.sol: contains the smart contract code in solidity language
- installContract.py: Python script for sending our contract to the Blockchain
- user.py: Python script that contains the actual chat client
- classify.py: Python script to classifiy an image file &lt;/code&gt;&lt;p&gt;&lt;b&gt;流程&lt;/b&gt;&lt;/p&gt;&lt;p&gt;推荐使用Linux系统。 安装将集中在Ubuntu Linux 16.04 LTS上，但对于其他发行版应该是类似的。&lt;/p&gt;&lt;p&gt;&lt;b&gt;我们需要先安装Python 2.7，pip，curl和git：&lt;/b&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;open a Linux Terminal and enter the following command:
sudo apt-get install python2.7 python-pip curl git build-essential libssl-dev &lt;/code&gt;&lt;p&gt;&lt;b&gt;安装当前nodejs版本：&lt;/b&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;curl -sL https://deb.nodesource.com/setup_7.x | sudo -E bash -
sudo apt-get install -y nodejs &lt;/code&gt;&lt;p&gt;&lt;b&gt;安装区块链测试环境：&lt;/b&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;sudo npm install -g ethereumjs-testrpc solc &lt;/code&gt;&lt;p&gt;&lt;b&gt;安装其他Python包：&lt;/b&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;sudo pip install -U pip
sudo pip install -U numpy keras==2.0.0 tensorflow ethjsonrpc h5py Pillow scipy &lt;/code&gt;&lt;p&gt;&lt;b&gt;克隆workshop代码：&lt;/b&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;cd
git clone https://github.com/thoschm/START-Summit-2017-Blockchain-Machine-Learning-Workshop.git workshop_code &lt;/code&gt;&lt;p&gt;&lt;b&gt;克隆现成的深度学习模型：&lt;/b&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;cd
git clone https://github.com/fchollet/deep-learning-models.git
cp deep-learning-models/resnet50.py workshop_code/code/
cp deep-learning-models/imagenet_utils.py workshop_code/code/ &lt;/code&gt;&lt;p&gt;&lt;b&gt;简单的图像分类示例&lt;/b&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;cd
cd workshop_code/code
[get some image file in there]
python classify.py image.jpg &lt;/code&gt;&lt;p&gt;当您第一次运行它时，它会从互联网下载预先训练的网络权重。 不需要自己训练。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;Using TensorFlow backend.
content:

Downloading data from https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json
&amp;gt;  tree_frog  &amp;lt; &lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;PPT展示&lt;/b&gt;&lt;/h2&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-9553ce4cd6ddf45121cb09b8928926f0_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-4a66954e4b5191556dd53003475639fa_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-a18be2bc915f4cd672ff0c65cac58ff8_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-2cdba7dc48e60dd3cf13857f18bb1841_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-eb1e6b5979731993475b18d5e5a3722e_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-b44e2a489704bc45616954846f651d4c_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-7e2d553ec4188126abf85e28f8af58e8_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-6726013442b8dff8728900b3bd993e86_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-cccc6288f0f41e48bfee42d8b3e0927f_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-5d61ef34da06495ce41e2fed1a75cf18_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-2c604e5a811289f7c164aaed1f473f2f_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-e91b95db74e92d8fbf366f2afd2a7d6e_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-d934053a159a1219e856bb9d57575b3e_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-7efad18817c8d68d3c55db27a8d67c5b_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-7bd65405ef1a92e2e312c6983eda9a39_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-aa8c91696ab177e2fbe45e5108286610_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-06299f07baedd00070b1fc451c6ed6aa_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-64c52c167ae10b21b47224532770d4d9_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-f3f0f280b9468853573b885ad5393a1f_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-b519b1f09183f20a84060828af2a59c0_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-41b2f56f78c42716f059e396d820b6d9_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-22a71c4175f77f6c6669cc1ba18a45e7_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-bc224e010d93ff7af782e81b47e7dc9b_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-1577966f716f5a6c7b6a5296c404d6df_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-009e6c99b633554b6a5c26558bf69c63_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-b3b657e89a99bcc9220253ddd8accb98_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-3c762149ec52d81aaac91b870ad39337_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-5f888cf0c5ef24df32cf399479906c8c_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-b7b2649e9a4f6205386910f82aebdfe1_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-3c65b615ad5962d99faf3beff9da5730_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-b40d386932966cd50008dc3e2ed7d981_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-54e64e234265287ca24237eef498511f_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-4309cff7fbd215f8c9e21cd5ad0a61d6_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-1957d87efb17e050a30e88eb69086209_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-7ff555c6cf4d3db258e467b5d8baf174_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-1d78dde1a6c7b2d610ab24846a4ed673_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-17195cae25e59965f4e9a7f1c7c48357_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-ee684c83aeb183a1f4e5f7ecdf13c5e7_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-eb9679b1b88cb2897745bd2fb2070118_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;p&gt;链接: https://pan.baidu.com/s/1o9Zteiu &lt;/p&gt;&lt;p&gt;密码: 95qd&lt;/p&gt;&lt;p&gt;谢谢大家对公众号的一直以来厚爱！&lt;/p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653287256&amp;amp;idx=2&amp;amp;sn=483e71c66e863aec424ff0c1936411c6&amp;amp;chksm=802e314db759b85b505f7fd1ed0cb0cacf10ed4bbbf86cc50c7bbc9d07d4ac0fed3b783b2278##&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-2e9f227f80bf31b439e47581a6b43f2b&quot; data-image-width=&quot;640&quot; data-image-height=&quot;640&quot; data-image-size=&quot;ipico&quot;&gt;机器学习应用区块链系列（一）--如何开发一套自己的智能合约系统&lt;/a&gt;&lt;b&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-ecd27e25f4fe3285f8f1fe63024bb19b_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1096&quot; data-rawheight=&quot;372&quot;&gt;&lt;/b&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>量化投资机器学习</author>
<guid isPermaLink="false">2018-01-21-33152738</guid>
<pubDate>Sun, 21 Jan 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>【拥抱量化】当AI遇上智能投顾——公众号受邀参加万得3C中国财经会议</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/2018-01-10-32821542.htm</link>
<description>&lt;meta charset=&quot;utf-8&quot;&gt;
&lt;title&gt;【拥抱量化】当AI遇上智能投顾——公众号受邀参加万得3C中国财经会议&lt;/title&gt;
&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/32821542&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-c9134639a91c9f3da75369d6232e8dc4_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;&lt;b&gt;收听会议&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;传送门：&lt;a href=&quot;http://3c-share.wind.com.cn/3CWeb/3CMobile/html/sharedMeeting.html?meetingId=52235&quot;&gt;会议详情&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-1192a9ff8a55aece3650f2362f87427f_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;755&quot; data-rawheight=&quot;1355&quot;&gt;&lt;p&gt;量化投资与机器学习公众号成立2年多来，一直秉持着无偿分享，共同成长的精神。努力为中国的量化投资事业贡献一份自己微薄的力量。&lt;/p&gt;&lt;p&gt;2年来从0到1，公众号成长很多，但是在大家的鼓励和支持下，我们不忘初心，赢得了众多媒体和机构的认可。&lt;/p&gt;&lt;p&gt;近期，公众号作为全网受 &lt;b&gt;Wind（万得）&lt;/b&gt;邀请的自媒体平台&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-6f1f30251631ad520d49ad3cb18bd606_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;738&quot; data-rawheight=&quot;201&quot;&gt;&lt;p&gt;参加了&lt;b&gt;中国最专业的机构路演平台&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-11fe70780bc01cff5e51ae37f032ad2a_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;640&quot; data-rawheight=&quot;180&quot;&gt;&lt;p&gt;我们很荣幸，能够和各大金融机构、金融大咖在此平台上分享我们的研究心得。这既是对公众号的认可，也是对公众号编辑部所有人的肯定。&lt;/p&gt;&lt;p&gt;在未来，公众号会继续秉持知识分享、坚持原创的精神。为更多的量化爱好者提供学习交流的平台。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;下面，大家快来看看我们这次线上路演的内容吧！&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-8a2e0a9f24c66c5d70d57138b04b4560_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1113&quot; data-rawheight=&quot;157&quot;&gt;&lt;p&gt;&lt;b&gt;部分PPT内容&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-61278fbbcb2079559d11f69c324c3bb0_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1378&quot; data-rawheight=&quot;770&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-c1d6604b67a8087dfdffaf1ec6d374d1_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1363&quot; data-rawheight=&quot;767&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;如何参加？&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;非万得用户&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;传送门：&lt;a href=&quot;http://3c-share.wind.com.cn/3CWeb/3CMobile/html/sharedMeeting.html?meetingId=52235&quot;&gt;会议详情&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;万得用户&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;PC端&lt;/b&gt;&lt;/p&gt;&lt;p&gt;1、点击Wind金融终端首页右侧的&lt;b&gt;“3C会议”&lt;/b&gt;或在右下角按键精灵&lt;b&gt;输入“3C”&lt;/b&gt;，进入3C会议模块。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-fe6ca35155ce6f20c49389e67278c636_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;640&quot; data-rawheight=&quot;771&quot;&gt;&lt;p&gt;2、选中一场会议，点击&lt;b&gt;“立即报名”&lt;/b&gt;。 &lt;/p&gt;&lt;p&gt;3、会议开始后，进入会议详情页，点击&lt;b&gt;“立即参会”&lt;/b&gt;，无需拨打电话，立即在线收听。您还可以在互动处留言，交流、提问。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;移动端&lt;/b&gt;&lt;/p&gt;&lt;p&gt;没有Wind金融终端的客户，请扫面下方二维码下载。&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-b8d3084c36db5e9bc4b14a70d80bf6bc_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;640&quot; data-rawheight=&quot;790&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-ca6de73d4a23d38fc1079568cb067c3d_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;640&quot; data-rawheight=&quot;1003&quot;&gt;&lt;p&gt;1、进入APP后，先点击右下角&lt;b&gt;“发现”&lt;/b&gt;，再进入&lt;b&gt;“3C会议平台”&lt;/b&gt;。&lt;br&gt;&lt;/p&gt;&lt;p&gt;2、报名及参会步骤，与PC端相同。&lt;/p&gt;&lt;p&gt;&lt;b&gt;注意：&lt;/b&gt;请确认APP已升级至最新版。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;收听会议&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;传送门：&lt;a href=&quot;http://3c-share.wind.com.cn/3CWeb/3CMobile/html/sharedMeeting.html?meetingId=52235&quot;&gt;会议详情&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-a3cf7ad8b603ed25923764c15c446691_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1715&quot; data-rawheight=&quot;444&quot;&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>量化投资机器学习</author>
<guid isPermaLink="false">2018-01-10-32821542</guid>
<pubDate>Wed, 10 Jan 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>【精心解读】关于Jupyter Notebook的28个技巧</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/2018-01-03-32600329.htm</link>
<description>&lt;meta charset=&quot;utf-8&quot;&gt;
&lt;title&gt;【精心解读】关于Jupyter Notebook的28个技巧&lt;/title&gt;
&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/32600329&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-8099359813f6c354ef60706a41d136cb_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;Jupyter具有很强的可扩展性，支持许多编程语言，可以很容易地托管在计算机上或几乎所有的服务器上，只需要拥有ssh或http访问权限。 最重要的是，它是完全免费的。 &lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-0b54a408ef565513c084fb7e7c2d5150_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1194&quot; data-rawheight=&quot;826&quot;&gt;&lt;p&gt;在Jupyter中使用Python时，使用了IPython内核，这使得我们可以在Jupyter笔记本中轻松访问IPython功能（后面会介绍更多内容！）&lt;/p&gt;&lt;h2&gt;&lt;b&gt;1、键盘快捷键&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;正如任何用户所知，键盘快捷键会为您节省大量的时间。 Jupyter在顶部的菜单下面保存一keybord快捷键列表：Help &amp;gt; Keyboard Shortcuts，或者在命令模式下按H键。 每次更新Jupyter都值得检查一下，因为所有的时候都会添加更多的快捷方式。&lt;/p&gt;&lt;p&gt;另一种访问键盘快捷方式的方法，以及学习它们的方便方法是使用：Cmd + Shift + P（或者在Linux和Windows上使用Ctrl + Shift + P）。 此对话框可帮助你按名称运行任何命令 - 如果你不知道某个操作的键盘快捷方式，或者您想要执行的操作没有键盘快捷键，则可以使用该对话框。 这个功能类似于Mac上的Spotlight搜索，一旦你开始使用它，你会会知道你的生活从此不能没有它！&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-912626bd5f76c0daef0b6726d2a1a13f_r.gif&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;776&quot; data-rawheight=&quot;367&quot; data-thumbnail=&quot;https://pic1.zhimg.com/v2-912626bd5f76c0daef0b6726d2a1a13f_b.jpg&quot;&gt;&lt;p&gt;&lt;b&gt;The command palette&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Esc将带你进入命令模式，你可以使用箭头键在笔记本上导航。&lt;/li&gt;&lt;li&gt;在命令模式下：&lt;/li&gt;&lt;ul&gt;&lt;li&gt;A在当前单元格上方插入一个新单元格，B在下面插入一个新单元格。&lt;/li&gt;&lt;li&gt;M将当前单元格更改为Markdown，Y将其更改回代码&lt;/li&gt;&lt;li&gt;D + D（按键两次）删除当前单元格&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;Enter将把你从命令模式转换回给定单元格的编辑模式。&lt;/li&gt;&lt;li&gt;Shift + Tab会显示刚刚在代码单元中输入的对象的文档字符串（文档） - 你可以继续按下此快捷键以循环使用几种文档模式。&lt;/li&gt;&lt;li&gt;Ctrl + Shift + - 会将当前单元格从光标所在的位置分成两部分。&lt;/li&gt;&lt;li&gt;Esc + F查找并替换你的代码，而不是输出。&lt;/li&gt;&lt;li&gt;Esc + O切换单元格输出。&lt;/li&gt;&lt;li&gt;选择多个单元格：&lt;/li&gt;&lt;ul&gt;&lt;li&gt;Shift + J或Shift + Down选择向下的下一个sell。你也可以使用Shift + K或Shift + Up选择向上的sell。&lt;/li&gt;&lt;li&gt;选中单元格后，可以删除/复制/剪切/粘贴/批处理。当你需要移动笔记本的某些部分时，这非常有用。&lt;/li&gt;&lt;li&gt;你也可以使用Shift + M来合并多个单元格。&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-2d1598cf4eab2a06758c9b55f13f792e_r.gif&quot; data-caption=&quot;合并多个单元格&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;765&quot; data-rawheight=&quot;162&quot; data-thumbnail=&quot;https://pic2.zhimg.com/v2-2d1598cf4eab2a06758c9b55f13f792e_b.jpg&quot;&gt;&lt;h2&gt;&lt;b&gt;2、完美的显示变量&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;第一部分是广为人知的。 通过完成Jupyter单元格的变量名称或未指定的语句输出，Jupyter将显示该变量，而不需要打印语句。 这在处理Pandas DataFrames时特别有用，因为输出整齐地格式化为表格。&lt;/p&gt;&lt;p&gt;但是很少人知道，你可以修改ast_note_interactivity内核选项来使jupyter对它自己的行上的任何变量或语句执行此操作，所以你可以同时看到多个语句的值。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = &quot;all&quot; 
from pydataset import data
quakes = data(&#39;quakes&#39;)
quakes.head()
quakes.tail() &lt;/code&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-856c378dc277b2e8880ba44d2d7398dd_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;845&quot; data-rawheight=&quot;310&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-bf839f8d24bce72f6507a35272217876_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;832&quot; data-rawheight=&quot;309&quot;&gt;&lt;p&gt;如果要为Jupyter（Notebook和Console）的所有实例设置此行为，只需使用下面的行创建〜/ .ipython / profile_default / ipython_config.py文件即可。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;c = get_config()

# Run all nodes interactively
c.InteractiveShell.ast_node_interactivity = &quot;all&quot;&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;3、易于链接到文档&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;在“帮助”菜单中，您可以找到包含NumPy，Pandas，SciPy和Matplotlib等通用库的在线文档的便捷链接。&lt;/p&gt;&lt;p&gt;另外不要忘记，通过在库中添加库，方法或变量。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;?str.replace()&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;Docstring:
S.replace(old, new[, count]) -&amp;gt; str

Return a copy of S with all occurrences of substring
old replaced by new.  If the optional argument count is
given, only the first count occurrences are replaced.
Type:      method_descriptor&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;4、绘图&lt;/b&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;matplotlib，用％matplotlib inline激活 。&lt;b&gt;(https://matplotlib.org/)&lt;/b&gt;&lt;/li&gt;&lt;li&gt;％matplotlib提供交互性，但可能有点慢，因为渲染是在服务器端完成的。&lt;/li&gt;&lt;li&gt;Seaborn建立在Matplotlib之上， 只需通过导入Seaborn，你的matplotlib就会变得更漂亮，不需要修改任何代码。&lt;i&gt;&lt;b&gt;(http://seaborn.pydata.org/)&lt;/b&gt;&lt;/i&gt;&lt;/li&gt;&lt;li&gt;mpld3为matplotlib代码提供了替代渲染器（使用d3）。 相当不错，虽然不完整。&lt;i&gt;&lt;b&gt;(https://github.com/mpld3/mpld3)&lt;/b&gt;&lt;/i&gt;&lt;/li&gt;&lt;li&gt;bokeh是建立交互式更好选择。&lt;i&gt;&lt;b&gt;(http://bokeh.pydata.org/en/latest/)&lt;/b&gt;&lt;/i&gt;&lt;/li&gt;&lt;li&gt;plot.ly可以提供更好的plots - 这曾经只是一个付费服务，但最近是开源le 。&lt;b&gt;&lt;i&gt;(https://plot.ly/)&lt;/i&gt;&lt;/b&gt;&lt;/li&gt;&lt;li&gt;Altair是一个相对较新的用Python的可视化库。 使用起来非常简单，并且可以制作出非常好看的图，但是定制这些图的能力并不像Matplotlib那样强大。&lt;b&gt;&lt;i&gt;(https://github.com/altair-viz/altair)&lt;/i&gt;&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-42911711dc9ec802818e10379c066b83_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;384&quot; data-rawheight=&quot;389&quot;&gt;&lt;h2&gt;&lt;b&gt;5、IPython Magic命令&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;&lt;br&gt;上面看到的％matplotlib是一个IPython Magic命令的例子。 基于IPython内核，Jupyter可以从IPython内核访问所有的Magics，它可以让你的工作变得更容易！&lt;/b&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;# This will list all magic commands
%lsmagic&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;Available line magics:
%alias  %alias_magic  %autocall  %automagic  %autosave  %bookmark  %cat  %cd  %clear  %colors  %config  %connect_info  %cp  %debug  %dhist  %dirs  %doctest_mode  %ed  %edit  %env  %gui  %hist  %history  %killbgscripts  %ldir  %less  %lf  %lk  %ll  %load  %load_ext  %loadpy  %logoff  %logon  %logstart  %logstate  %logstop  %ls  %lsmagic  %lx  %macro  %magic  %man  %matplotlib  %mkdir  %more  %mv  %notebook  %page  %pastebin  %pdb  %pdef  %pdoc  %pfile  %pinfo  %pinfo2  %popd  %pprint  %precision  %profile  %prun  %psearch  %psource  %pushd  %pwd  %pycat  %pylab  %qtconsole  %quickref  %recall  %rehashx  %reload_ext  %rep  %rerun  %reset  %reset_selective  %rm  %rmdir  %run  %save  %sc  %set_env  %store  %sx  %system  %tb  %time  %timeit  %unalias  %unload_ext  %who  %who_ls  %whos  %xdel  %xmode

Available cell magics:
%%!  %%HTML  %%SVG  %%bash  %%capture  %%debug  %%file  %%html  %%javascript  %%js  %%latex  %%perl  %%prun  %%pypy  %%python  %%python2  %%python3  %%ruby  %%script  %%sh  %%svg  %%sx  %%system  %%time  %%timeit  %%writefile

Automagic is ON, % prefix IS NOT needed for line magics.&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;建议浏览所有IPython Magic命令的文档，因为你会发现一些适合你的工具。&lt;i&gt;&lt;b&gt;(http://ipython.readthedocs.io/en/stable/interactive/magics.html)&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;6、IPython Magic - ％env：设置环境变量&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;你可以管理Jupyter的环境变量，而无需重新启动Jupyter服务器进程。 有些库（如theano）使用环境变量来控制行为，％env是最方便的方法。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;# Running %env without any arguments
# lists all environment variables

# The line below sets the environment
# variable OMP_NUM_THREADS
%env OMP_NUM_THREADS=4&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;env: OMP_NUM_THREADS=4&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;7、IPython Magic - ％run：执行python代码&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;％run可以从.py文件中执行python代码，鲜为人知的是，它也可以执行其他jupyter notebooks，相当有用。&lt;/p&gt;&lt;p&gt;请注意，使用％run与导入python模块不同。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;# this will execute and show the output from
# all code cells of the specified notebook
%run ./two-histograms.ipynb &lt;/code&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-544e14e48bd1eabcff6108a9149ec296_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;379&quot; data-rawheight=&quot;256&quot;&gt;&lt;h2&gt;&lt;b&gt;8、IPython Magic - ％load：从外部脚本插入代码&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;这将用外部脚本替换单元格的内容。 你可以使用计算机上的文件作为源，也可以使用URL。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;# Before Running
%load ./hello_world &lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;# After Running
# %load ./hello_world.py
if __name__ == &quot;__main__&quot;:
 print(&quot;Hello World!&quot;)&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;Hello World!&lt;/code&gt;&lt;h2&gt;&lt;b&gt;9、IPython Magic - ％store：在笔记本之间传递变量&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;％store命令可以让你在两个不同的文件之间传递变量。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;data = &#39;this is the string I want to pass to different notebook&#39;
%store data
del data # This has deleted the variable&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;Stored &#39;data&#39; (str) &lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;new&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;%store -r data
print(data) &lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;this is the string I want to pass to different notebook &lt;/code&gt;&lt;h2&gt;&lt;b&gt;10、IPython Magic - ％who：列出全局范围的所有变量&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;没有任何参数的％who命令将列出全局范围中存在的所有变量。 传递像str这样的参数将仅列出该类型的变量。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;one = &quot;for the money&quot;
two = &quot;for the show&quot;
three = &quot;to get ready now go cat go&quot; 
%who str&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;one   three   two &lt;/code&gt;&lt;h2&gt;&lt;b&gt;11、IPython Magic - 时间&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;有两个IPython Magic命令对时间有效 - %%time和％timeit。 &lt;/p&gt;&lt;p&gt;%%time会给你关于单元中的代码的单一运行的信息。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;%%time
import time
for _ in range(1000):
   time.sleep(0.01)# sleep for 0.01 seconds&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;CPU times: user 21.5 ms, sys: 14.8 ms, total: 36.3 ms
Wall time: 11.6 s&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;import numpy
%timeit numpy.random.normal(size=100) &lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;The slowest run took 7.29 times longer than the fastest. This could mean that an intermediate result is being cached.
100000 loops, best of 3: 5.5 µs per loop &lt;/code&gt;&lt;h2&gt;&lt;b&gt;12、IPython Magic - %% writefile和％pycat：导出单元格的内容/显示外部脚本的内容&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;使用%% writefile magic将该单元格的内容保存到外部文件中。 ％pycat会做相反的处理，并显示（在弹出窗口中）外部文件高亮内容。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;%%writefile pythoncode.py

import numpy
def append_if_not_exists(arr, x):
   if x not in arr:
       arr.append(x)

def some_useless_slow_function():
   arr = list()
   for i in range(10000):
       x = numpy.random.randint(0, 10000)
       append_if_not_exists(arr, x)&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;Writing pythoncode.py

%pycat pythoncode.py

```python
import numpy
def append_if_not_exists(arr, x):
   if x not in arr:
       arr.append(x)

def some_useless_slow_function():
   arr = list()
   for i in range(10000):
       x = numpy.random.randint(0, 10000)
       append_if_not_exists(arr, x)

### 13. IPython Magic - %prun: Show how much time your program spent in each function.

Using `%prun statement_name` will give you an ordered table showing you the number of times each internal function was called within the statement, the time each call took as well as the cumulative time of all runs of the function.


```python
%prun some_useless_slow_function()

26324 function calls in 0.556 seconds

Ordered by: internal time

ncalls  tottime  percall  cumtime  percall filename:lineno(function)
10000    0.527    0.000    0.528    0.000 &amp;lt;ipython-input-46-b52343f1a2d5&amp;gt;:2(append_if_not_exists)
10000    0.022    0.000    0.022    0.000 {method &#39;randint&#39; of &#39;mtrand.RandomState&#39; objects}
     1    0.006    0.006    0.556    0.556 &amp;lt;ipython-input-46-b52343f1a2d5&amp;gt;:6(some_useless_slow_function)
  6320    0.001    0.000    0.001    0.000 {method &#39;append&#39; of &#39;list&#39; objects}
    1    0.000    0.000    0.556    0.556 &amp;lt;string&amp;gt;:1(&amp;lt;module&amp;gt;)
    1    0.000    0.000    0.556    0.556 {built-in method exec}
    1    0.000    0.000    0.000    0.000 {method &#39;disable&#39; of &#39;_lsprof.Profiler&#39; objects}&lt;/code&gt;&lt;h2&gt;&lt;b&gt;14.、IPython Magic - 用％pdb进行调试&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Jupyter拥有自己的Python Debugger（pdb）接口。 这使得调试成为可能。&lt;/p&gt;&lt;p&gt;你可以在这里查看a list of accepted commands for pdb here.&lt;i&gt;&lt;b&gt;(https://docs.python.org/3.5/library/pdb.html#debugger-commands)&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;%pdb

def pick_and_take():
   picked = numpy.random.randint(0, 1000)
   raise NotImplementedError()

pick_and_take()

Automatic pdb calling has been turned ON&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;---------------------------------------------------------------------------
NotImplementedError                       Traceback (most recent call last)
&amp;lt;ipython-input-24-0f6b26649b2e&amp;gt; in &amp;lt;module&amp;gt;()
     5     raise NotImplementedError()
     6 
----&amp;gt; 7 pick_and_take()

&amp;lt;ipython-input-24-0f6b26649b2e&amp;gt; in pick_and_take()
     3 def pick_and_take():
     4     picked = numpy.random.randint(0, 1000)
----&amp;gt; 5     raise NotImplementedError()
     6 
     7 pick_and_take()

NotImplementedError:&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;&amp;gt; &amp;lt;ipython-input-24-0f6b26649b2e&amp;gt;(5)pick_and_take()
     3 def pick_and_take():
     4     picked = numpy.random.randint(0, 1000)
----&amp;gt; 5     raise NotImplementedError()
     6 
     7 pick_and_take()&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;ipdb&amp;gt;&lt;/code&gt;&lt;h2&gt;&lt;b&gt;15、IPython Magic - 输出Retina notebooks的高分辨率绘图&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;一行神奇的IPython会给你的Retina屏幕输出双分辨率绘图像，比如Macbook。 注意：下面的例子不会在非视网膜屏幕上渲染。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;x = range(1000)
y = [i ** 2 for i in x]
plt.plot(x,y)
plt.show();

%config InlineBackend.figure_format = &#39;retina&#39;
plt.plot(x,y)
plt.show(); &lt;/code&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-8e68a75ceadfa0b4631d36e66c866110_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;569&quot; data-rawheight=&quot;356&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-a5f97f203ac5038fbba2f50451a58fe0_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1140&quot; data-rawheight=&quot;713&quot;&gt;&lt;h2&gt;&lt;b&gt;16、阻止最终函数的输出&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;有时候在最后一行阻止函数的输出是很方便的，例如绘图时。 要做到这一点，你只需在最后添加一个分号。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;%matplotlib inline
from matplotlib import pyplot as plt
import numpy
x = numpy.linspace(0, 1, 1000)**1.5
# Here you get the output of the function
plt.hist(x)
(array([ 216.,  126.,  106.,   95.,   87.,   81.,   77.,   73.,   71.,   68.]),
array([ 0. ,  0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9,  1. ]),
&amp;lt;a list of 10 Patch objects&amp;gt;)
# By adding a semicolon at the end, the output is suppressed.
plt.hist(x); &lt;/code&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-2f352ecf6157c687061d5a8511335bef_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;377&quot; data-rawheight=&quot;256&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-2f352ecf6157c687061d5a8511335bef_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;377&quot; data-rawheight=&quot;256&quot;&gt;&lt;h2&gt;&lt;b&gt;17、执行Shell命令&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;从 notebook内部执行shell命令很容易。 你可以使用它来检查工作文件夹中可用的数据集：&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;!ls *.csv
nba_2016.csv             titanic.csv
pixar_movies.csv         whitehouse_employees.csv&lt;/code&gt;&lt;code lang=&quot;python&quot;&gt;!pip install numpy
!pip list | grep pandas
Requirement already satisfied (use --upgrade to upgrade): numpy in /Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages
pandas (0.18.1)&lt;/code&gt;&lt;h2&gt;&lt;b&gt;18、使用LaTeX的公式&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;当你在Markdown单元格中编写LaTeX时，使用MathJax将其渲染为公式。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;$$ P(A \mid B) = \frac{P(B \mid A) \, P(A)}{P(B)} $$&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-1b37ad452e3ba895d05224b312684818_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;334&quot; data-rawheight=&quot;94&quot;&gt;&lt;h2&gt;&lt;b&gt;19、运行代码从其他内核在notebook中&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;如果你喜欢，你可以将来自多个内核的代码组合到一个notebook中。&lt;/p&gt;&lt;p&gt;只需在每个单元的开始处使用IPython Magics以及你的内核的名称就可以使用该内核：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;%%bash&lt;/li&gt;&lt;li&gt;%%HTML&lt;/li&gt;&lt;li&gt;%%python2&lt;/li&gt;&lt;li&gt;%%python3&lt;/li&gt;&lt;li&gt;%%ruby&lt;/li&gt;&lt;li&gt;%%perl&lt;/li&gt;&lt;/ul&gt;&lt;code lang=&quot;python&quot;&gt;%%bash
for i in {1..5}
do
  echo &quot;i is $i&quot;
done
i is 1
i is 2
i is 3
i is 4
i is 5&lt;/code&gt;&lt;h2&gt;&lt;b&gt;20、安装Jupyter的其他内核&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Jupyter的一个很好的功能是能够运行不同语言的内核。 举个例子，这里是如何获取R内核运行。&lt;/p&gt;&lt;p&gt;简单选项：使用Anaconda安装R内核&lt;/p&gt;&lt;p&gt;如果你使用Anaconda来设置你的环境，那么让R工作非常容易。 在终端上运行下面的代码：&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;conda install -c r r-essentials &lt;/code&gt;&lt;p&gt;较不容易的选项：手动安装R内核&lt;/p&gt;&lt;p&gt;如果你不使用Anaconda，这个过程会更复杂一些。 首先，如果您尚未安装R，则需要安装R。&lt;/p&gt;&lt;p&gt;完成之后，启动R控制台并运行以下命令：&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;install.packages(c(&#39;repr&#39;, &#39;IRdisplay&#39;, &#39;crayon&#39;, &#39;pbdZMQ&#39;, &#39;devtools&#39;))
devtools::install_github(&#39;IRkernel/IRkernel&#39;)
IRkernel::installspec()  # to register the kernel in the current R installation&lt;/code&gt;&lt;h2&gt;&lt;b&gt;21、在同一个 notebook中运行R和Python&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;最好的解决方案是安装rpy2（需要一个R的工作版本），可以很容易地用pip完成：&lt;i&gt;&lt;b&gt;(https://bitbucket.org/)&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;pip install rpy2

%load_ext rpy2.ipython

%R require(ggplot2)

array([1], dtype=int32)

import pandas as pd
df = pd.DataFrame({
       &#39;Letter&#39;: [&#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;c&#39;, &#39;c&#39;, &#39;c&#39;],
       &#39;X&#39;: [4, 3, 5, 2, 1, 7, 7, 5, 9],
       &#39;Y&#39;: [0, 4, 3, 6, 7, 10, 11, 9, 13],
       &#39;Z&#39;: [1, 2, 3, 1, 2, 3, 1, 2, 3]
   })
   
%%R -i df
ggplot(data = df) + geom_point(aes(x = X, y= Y, color = Letter, size = Z)) &lt;/code&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-f877934afbfc4ab2673cc6f7dd8f82aa_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;480&quot; data-rawheight=&quot;480&quot;&gt;&lt;h2&gt;&lt;b&gt;22、用其他语言编写函数&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;有时numpy的速度是不够的，我们需要写一些速度较快的代码。&lt;/p&gt;&lt;p&gt;原则上，你可以在动态库中编译函数并编写Python包装器...&lt;/p&gt;&lt;p&gt;但是，这个无聊的部分应该你做吗？&lt;/p&gt;&lt;p&gt;你可以用cython或者fortran编写函数，直接从python代码中使用。&lt;/p&gt;&lt;p&gt;首先你需要安装：&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;!pip install cython fortran-magic 

%load_ext Cython

%%cython
def myltiply_by_2(float x):
   return 2.0 * x
   
myltiply_by_2(23.)&lt;/code&gt;&lt;p&gt;个人更喜欢使用fortran，这对于编写数字运算函数非常方便。 更多的使用细节可以在这里找到。&lt;i&gt;&lt;b&gt;(http://arogozhnikov.github.io/2015/11/29/using-fortran-from-python.html)&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;%load_ext fortranmagic

%%fortran
subroutine compute_fortran(x, y, z)
   real, intent(in) :: x(:), y(:)
   real, intent(out) :: z(size(x, 1))

   z = sin(x + y)

end subroutine compute_fortran

compute_fortran([1, 2, 3], [4, 5, 6])&lt;/code&gt;&lt;h2&gt;&lt;b&gt;23、Multicursor支持&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Jupyter支持多个Multicursor，类似于Sublime Text。 只需按住Alt键并拖动鼠标即可。&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-aa676a4da8a6d4e0d2af249921cd6110_r.gif&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;651&quot; data-rawheight=&quot;572&quot; data-thumbnail=&quot;https://pic4.zhimg.com/v2-aa676a4da8a6d4e0d2af249921cd6110_b.jpg&quot;&gt;&lt;h2&gt;&lt;b&gt;24.、Jupyter-contrib扩展&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;包括jupyter拼写检查器和代码格式化等。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;!pip install https://github.com/ipython-contrib/jupyter_contrib_nbextensions/tarball/master
!pip install jupyter_nbextensions_configurator
!jupyter contrib nbextension install --user
!jupyter nbextensions_configurator enable --user &lt;/code&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-70fe8942112e211bfc0b558109e04786_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1145&quot; data-rawheight=&quot;382&quot;&gt;&lt;h2&gt;&lt;b&gt;25、从Jupyter中创建一个演示文稿。&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Damian Avila&#39;s的RISE允许你从现有的笔记本中创建一个PPT风格的演示文稿。&lt;i&gt;&lt;b&gt;(https://github.com/damianavila/RISE)&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;你可以使用conda安装RISE：&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;conda install -c damianavila82 rise
# Or alternatively pip:

pip install RISE
# And then run the following code to install and enable the extension:

jupyter-nbextension install rise --py --sys-prefix
jupyter-nbextension enable rise --py --sys-prefix&lt;/code&gt;&lt;h2&gt;&lt;b&gt;26、Jupyter输出系统&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;笔记本显示为HTML，单元格输出可以是HTML，因此你可以返回几乎任何内容：视频/音频/图像。&lt;/p&gt;&lt;p&gt;在这个例子中，我用我的资料库中的图像扫描文件夹，并显示其缩略图：&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;import os
from IPython.display import display, Image
names = [f for f in os.listdir(&#39;../images/ml_demonstrations/&#39;) if f.endswith(&#39;.png&#39;)]
for name in names[:5]:
   display(Image(&#39;../images/ml_demonstrations/&#39; + name, width=100)) &lt;/code&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-b923ff938bf09791b1d7192bf5cade89_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;223&quot; data-rawheight=&quot;829&quot;&gt;&lt;p&gt;我们可以用bash命令创建相同的列表&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;names = !ls ../images/ml_demonstrations/*.png
names[:5]
[&#39;../images/ml_demonstrations/colah_embeddings.png&#39;,
&#39;../images/ml_demonstrations/convnetjs.png&#39;,
&#39;../images/ml_demonstrations/decision_tree.png&#39;,
&#39;../images/ml_demonstrations/decision_tree_in_course.png&#39;,
&#39;../images/ml_demonstrations/dream_mnist.png&#39;]&lt;/code&gt;&lt;h2&gt;&lt;b&gt;27、“大数据”分析&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;查询/处理大数据样本有多种解决方案：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;ipyparallel（以前的ipython集群）是python中简单的map-reduce的一个好选择。我们用它来并行训练许多机器学习模型&lt;i&gt;&lt;b&gt;(https://github.com/ipython/ipyparallel)&lt;/b&gt;&lt;/i&gt;&lt;/li&gt;&lt;li&gt;pyspark&lt;i&gt;&lt;b&gt;(https://www.cloudera.com/documentation/enterprise/5-5-x/topics/spark_ipython.html)&lt;/b&gt;&lt;/i&gt;&lt;/li&gt;&lt;li&gt;spark-sql magic %% sql&lt;i&gt;&lt;b&gt;(https://github.com/jupyter-incubator/sparkmagic)&lt;/b&gt;&lt;/i&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;28、共享notebook&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;共享笔记本最简单的方法就是使用笔记本文件（.ipynb），但对于那些不使用Jupyter的用户，有几个选择：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Convert notebooks to html file using the &lt;code class=&quot;inline&quot;&gt;File &amp;gt; Download as &amp;gt; HTML&lt;/code&gt; Menu option.&lt;/li&gt;&lt;li&gt;Share your notebook file with gists or on github, both of which render the notebooks. See this example.&lt;/li&gt;&lt;ul&gt;&lt;li&gt;If you upload your notebook to a github repository, you can use the handy mybinder service to allow someone half an hour of interactive Jupyter access to your repository.&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;Setup your own system with jupyterhub, this is very handy when you organize mini-course or workshop and don&#39;t have time to care about students machines.&lt;/li&gt;&lt;li&gt;Store your notebook e.g. in dropbox and put the link to nbviewer. nbviewer will render the notebook from whichever source you host it.&lt;/li&gt;&lt;li&gt;Use the &lt;code class=&quot;inline&quot;&gt;File &amp;gt; Download as &amp;gt; PDF&lt;/code&gt; menu to save your notebook as a PDF. If you&#39;re going this route, I highly recommend reading Julius Schulz&#39;s excellent article Making publication ready Python notebooks.&lt;/li&gt;&lt;li&gt;Create a blog using Pelican from your Jupyter notebooks.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;原文：&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653286936&amp;amp;idx=1&amp;amp;sn=17ebfb48f8541243953041b0c857ae5d&amp;amp;chksm=802e300db759b91bd4028d0cebe7f01ceb844c44a68067fbbf5312945b5b6f2b44830e606516#rd&quot;&gt;【精心解读】关于Jupyter Notebook的28个技巧&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-a283cc2b67804cefe5c2b7a6d86dfc21_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1141&quot; data-rawheight=&quot;697&quot;&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>量化投资机器学习</author>
<guid isPermaLink="false">2018-01-03-32600329</guid>
<pubDate>Wed, 03 Jan 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>【精选干货】2017年12月份机器学习排名前10名文章（论文+代码）</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/2018-01-03-32600161.htm</link>
<description>&lt;meta charset=&quot;utf-8&quot;&gt;
&lt;title&gt;【精选干货】2017年12月份机器学习排名前10名文章（论文+代码）&lt;/title&gt;
&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/32600161&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-b289a3e99a7eb7790128da739bb4de4e_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;这是一份很有竞争力的榜单&lt;/p&gt;&lt;p&gt;排名综合了很多因素&lt;/p&gt;&lt;p&gt;你将看到最前沿的内容&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;文中部分没有下载链接的论文&lt;/p&gt;&lt;p&gt;可在&lt;b&gt;文末下载&lt;/b&gt;&lt;/p&gt;&lt;p&gt;我们已经打包好了！&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;第一名&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Alpha Zero：Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm. Courtesy of Demis Hassabis and others at Deepmind&lt;/p&gt;&lt;p&gt;&lt;b&gt;论文&lt;/b&gt;&lt;/p&gt;&lt;p&gt;《Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm》&lt;/p&gt;&lt;p&gt;&lt;b&gt;（论文在文末下载）&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-e4dd3421c70690f6c6d7631621bb971c_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;618&quot; data-rawheight=&quot;795&quot;&gt;&lt;p&gt;&lt;b&gt;视频地址&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;https://youtu.be/7-MborNxYWE&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-91a34c1a4d9abe36e46923d9921d6d5f_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1153&quot; data-rawheight=&quot;605&quot;&gt;&lt;h2&gt;&lt;b&gt;第二名&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs。由加利福尼亚大学伯克利分校和Nvidia Research的Ming-Yu Liu等人。&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-500b0367fdc2f66a567be45bd3428dc3_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1015&quot; data-rawheight=&quot;571&quot;&gt;&lt;p&gt;&lt;b&gt;论文&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;https://arxiv.org/pdf/1711.11585.pdf&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;代码&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;https://github.com/NVIDIA/pix2pixHD&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;第三名&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Capsule Networks Tutorial — Aurelien Geron (Geob Hinton on this video: “This is an amazingly good video. I wish I could explain capsules that well”). Courtesy of Aurélien Géron&lt;/p&gt;&lt;p&gt;&lt;b&gt;视频地址&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;https://www.youtube.com/watch?v=pPN8d0E3900&amp;amp;feature=youtu.be&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-321be8d1860d691a37cc8cf1f2cb4a1c_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1114&quot; data-rawheight=&quot;605&quot;&gt;&lt;h2&gt;&lt;b&gt;第四名&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Artwork Personalization at Neteix&lt;b&gt;（论文在文末下载）&lt;/b&gt;&lt;/p&gt;&lt;p&gt;[Part I： Courtesy of Neteix Technology Blog]&lt;/p&gt;&lt;p&gt;[Part 2：Innovating Faster on Personalization Algorithms at Neteix Using Interleaving]&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-03adea488d3b20c89f66be4859706539_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;922&quot; data-rawheight=&quot;559&quot;&gt;&lt;h2&gt;&lt;b&gt;第五名&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;A Year in Computer Vision. Courtesy of Ben Duby and Daniel Flynn&lt;b&gt;（论文在文末下载）&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-99ce018168a54e5be4edf99a9159b8d3_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1649&quot; data-rawheight=&quot;925&quot;&gt;&lt;h2&gt;&lt;b&gt;第六名&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Sequence Modeling with CTC. Courtesy of Distill&lt;/p&gt;&lt;p&gt;&lt;b&gt;网址&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;https://distill.pub/2017/ctc/?utm_source=mybridge&amp;amp;utm_medium=blog&amp;amp;utm_campaign=read_more&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-3b1dd5e33ed17a6681d1d09ac0fc5557_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1103&quot; data-rawheight=&quot;843&quot;&gt;&lt;h2&gt;&lt;b&gt;第七名&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;吴恩达最新研究成果&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Improving Palliative Care with Deep Learning. Courtesy of Andrew Ng and others at Stanford ML Group&lt;/p&gt;&lt;p&gt;&lt;b&gt;论文&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;https://arxiv.org/pdf/1711.06402.pdf&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-6648704925146e85954395b75645e985_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;765&quot; data-rawheight=&quot;323&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-690509b243fa420408e0ee5cc1f737d7_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;640&quot; data-rawheight=&quot;480&quot;&gt;&lt;h2&gt;&lt;b&gt;第八名&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Evolving Stable Strategies. Courtesy of hard maru at Google Brain&lt;/p&gt;&lt;h2&gt;&lt;b&gt;第九名&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Some Deep Learning with Python, TensorFlow and Keras&lt;/p&gt;&lt;p&gt;&lt;b&gt;地址&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;https://sandipanweb.wordpress.com/2017/11/25/some-deep-learning-with-python-tensorflow-and-keras/?utm_source=mybridge&amp;amp;utm_medium=blog&amp;amp;utm_campaign=read_more&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-57922b5b4685e1e5394aa246e776c306_r.gif&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;432&quot; data-rawheight=&quot;288&quot; data-thumbnail=&quot;https://pic2.zhimg.com/v2-57922b5b4685e1e5394aa246e776c306_b.jpg&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-666cd29d5eb196e19ae36c2d53b29854_r.gif&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;384&quot; data-rawheight=&quot;288&quot; data-thumbnail=&quot;https://pic4.zhimg.com/v2-666cd29d5eb196e19ae36c2d53b29854_b.jpg&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-586ec178f8826a030559cfc49c890a78_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;532&quot; data-rawheight=&quot;391&quot;&gt;&lt;h2&gt;&lt;b&gt;第十名&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Introduction To Neural Networks &lt;/p&gt;&lt;p&gt;[Part I]. Courtesy of Ben Gorman at GormAnalysis&lt;/p&gt;&lt;p&gt;&lt;b&gt;地址&lt;/b&gt;&lt;/p&gt;&lt;p&gt;http://blog.kaggle.com/2017/11/27/introduction-to-neural-networks/&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-637c489d8718a3d3d0d0a37bdb58827b_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1895&quot; data-rawheight=&quot;560&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-2c82281e57702021ee0a9113059c3530_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1000&quot; data-rawheight=&quot;564&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-7db3ed1935b934fccc97dba09a321713_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;2750&quot; data-rawheight=&quot;562&quot;&gt;&lt;p&gt;[Part II]. A Worked Example ]&lt;/p&gt;&lt;p&gt;&lt;b&gt;地址&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;http://blog.kaggle.com/2017/12/06/introduction-to-neural-networks-2/&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-21ee622aa0f46d99cf095dc44c3edbff_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1897&quot; data-rawheight=&quot;580&quot;&gt;&lt;b&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-4c497e536f811b8a921f227b33d0593b_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1000&quot; data-rawheight=&quot;527&quot;&gt;&lt;/b&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;原文：&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653286968&amp;amp;idx=1&amp;amp;sn=ba12dc60d694e916eef0c830ad9016b6&amp;amp;chksm=802e302db759b93babf20638a4bf8d0c5f2c7c8fa05207f0beed198891cfab4e78b1e30ceea5#rd&quot;&gt;【精选干货】2017年12月份机器学习排名前10名文章（论文+代码）&lt;/a&gt;&lt;/p&gt;&lt;p&gt;来源：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://medium.mybridge.co/machine-learning-top-10-articles-for-the-past-month-v-dec-2017-82883b8062f5?source=userActivityShare-19d54032c7a-1513853387&quot;&gt;https://medium.mybridge.co/machine-learning-top-10-articles-for-the-past-month-v-dec-2017-82883b8062f5?source=userActivityShare-19d54032c7a-1513853387&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-a283cc2b67804cefe5c2b7a6d86dfc21_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1141&quot; data-rawheight=&quot;697&quot;&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>量化投资机器学习</author>
<guid isPermaLink="false">2018-01-03-32600161</guid>
<pubDate>Wed, 03 Jan 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>【重磅】2018年投资界十大猜想</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/2018-01-03-32599955.htm</link>
<description>&lt;meta charset=&quot;utf-8&quot;&gt;
&lt;title&gt;【重磅】2018年投资界十大猜想&lt;/title&gt;
&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/32599955&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-a39716a432272bbdac6d270466776ddf_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;今天，是2017年的最后一天。为此，编辑部特地整理了一份关于2018年投资界的十大猜想，希望在新的一年，能给大家带来好运！&lt;/p&gt;&lt;p&gt;我们综合了几大券商的研究报告，最终汇总出编辑部认为比较重要的十个猜想，供各位读者阅览。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;猜想一&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;债市方面&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;本轮熊市到底还会持续多久？&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;站在 17 年 10 月 21 日债熊一周年 之际往回看 ，10 年期国债收益率从 低点累计上行 107 bp ，看上去似乎利率调整的幅度已经足够大，但现实很骨感，债熊周年祭只不过是新一轮主跌浪的开始而已。截至目前从持续时间和幅度来看，本轮熊市似乎已经很难用历史上的任何一轮熊市进行简单类比。&lt;/p&gt;&lt;p&gt;09 年、11 年、13 年的三轮大熊市中，每一轮都有一个非常明确的主要利空因素，分别是经济、通胀和资金 。而在这一轮熊市中， 监管可能是影响债市权重较高的因素，但其他因素诸如海外、经济、资金等也都对债市产生过显著影响。&lt;/p&gt;&lt;p&gt;在 14 年到 16 年历史最长债券牛市中，几乎所有影响的因素均转为利多方向 ，在这三年里我们看到了美债利率累计下行 160 bp ，CPI 从最高 3% 回落到最低 1.3% ，GDP 从 7.7% 回落至 6.7% ，货币政策和监管不断放松。&lt;/p&gt;&lt;p&gt;而现在 ，所有因素都发生了逆转 ，美债收益率持续上行 ，GDP 企稳反弹，货币政策收紧 ，监管政策趋严，这一系列因素在边际转向为利空后 ，很有可能正在催生出史上最长熊市。现在唯一尚未对债市产生显著利空影响的因素就是通胀， 但是这个利空或许在明年会持续发酵。本轮熊市还多久 ，似乎谁都没法下一个明确的定论 。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;猜想二&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;2018 年的 A 股市场大概率是震荡上行的格局&lt;/b&gt;&lt;/p&gt;&lt;p&gt;站在 2017 年末的资本市场，对 2018 年的行情展望给予了较大的信心，中长期乐观因素在不断累积 幵且深化，2018 年权益市场遇见的是一个较为有利的市场起点。&lt;/p&gt;&lt;p&gt;2018 年权益市场向上最重要的逻辑在于存量流动性之争的伓势，也最受到市场资金预期的认可。在 整体金融监管趋严的基调下，权益资产作为资产定价区域合理的标准化金融资产在资产再配置和存 量流动性争夺方面更具有伓势。&lt;/p&gt;&lt;p&gt;其他方面的乐观因素也在给予市场 2018 年有敁的支撑。第一，金融体系风险得到有敁调整和监管， 房地产库存显著去化，金融市场、金融机极、金融产品的杠杄呈现下行;第事，权益市场觃范化建 设在进一步加强，改革収展和对外开放加速推进，养老金、社保基金、境外资金等长线资金在陆续 入场;第三，权益市场的局部高估值有所回落，市场资本定价及估值更趋合理;第四，宏观经济总需求在企稳反弹，上市公司企业盈利持续改善。&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-9115c4091b5cd5951d5146b93e8b1899_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;940&quot; data-rawheight=&quot;473&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-e4735f499cb6a4b63e6ee1c16f188dcc_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;886&quot; data-rawheight=&quot;473&quot;&gt;&lt;h2&gt;&lt;b&gt;猜想三&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;权益市场受益于存量流动性之争，机构定价权在增强&lt;/b&gt;&lt;/p&gt;&lt;p&gt;未来金融监管的进一步加强将推动金融体系加速存量调整，标准化金融资产在资产再配置和存量流动性争夺方面更具有优势，非标市场的调整显著且明确。我国权益市场的制度化规范化改革在加速，境外、养老金以及保险资金等长期配置资金在持续流入权益市场，给予市场强有效的长期支撑。因此，我们认为虽然2018 年整体资本市场的流动性影响较为负面，但是2018 年权益市场流动性因此而遭受的负面影响有限。&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-0b2000c94fbec0a9163ffe08fc932e48_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1287&quot; data-rawheight=&quot;592&quot;&gt;&lt;p&gt;权益市场受益于存量流动性之争是利好的一方面，更多的利好体现在机构规模特别是长线资金持续流入导致的机极定价权抬升，盈利驱动行情仍将延续。对比台湾股票市场的収展进程来看，台湾的机构成交占比从90 年代的8.1%抬升至近期的31.7%，特别是在20 世纪末加入MSCI 之后机构占比快速推进，境内机构交易也相应从6.9%上升至12.7%。根据上交所数据，近期A 股市场公募基金交易占比大致在10%左右，我国机构资金和成交金额仍有较大的提升空间，预计机构成交占比在未来3-5 年大概率有10%的上行幅度。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-417c16556f23a1b5925622eabb5154e0_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1367&quot; data-rawheight=&quot;571&quot;&gt;&lt;h2&gt;&lt;b&gt;猜想四&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;金融科技与证券行业结合更紧密&lt;/b&gt;&lt;/p&gt;&lt;p&gt;随着金融科技在金融领域的应用逐渐深入，证券行业作为整个金融产业价值链的核心环节，金融科技渗透率也将逐步提高。目前，我国证券行业对经纪业务依赖较高，佣金率下滑严重，需要向财富管理和全能型投行转型，而金融科技恰好能满足券商的业务重构需求。根据证券基金业协会数据，截止 2016 年底，我国资管业务总觃模超 50 万亿，若按照 10%的资管觃模年增长率，20%的金融科技渗透率计算，万亿应用空间待启。&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-7e9e3231f89868775f8eb2726ee4d5dd_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1533&quot; data-rawheight=&quot;736&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-c967febdc597f13dbcdc56ae4a9c897a_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1543&quot; data-rawheight=&quot;795&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-d1fa08caf66d7f240093011cefe10b6a_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1546&quot; data-rawheight=&quot;446&quot;&gt;&lt;p&gt;在金融科技的大潮中，券商纷纷迚行金融科技布局，研収投入不断增长，IT 队伍不断壮大。在移动终端建设方面，采取多元化移动互联网布局模式，収展微信公众号、微博、网站、展业平台等多种模式，幵不断迚行系统完善和升级，持续引流客户，提高客户活跃度；在大数据方面，建设数据平台，迚行大数据挖掘、分析和应用；在人工智能方面，通过自主研収或者合作开収机器人投顾等产品。&lt;/p&gt;&lt;p&gt;进入 2018 年，金融科技与证券行业会更加紧密结合，在某些领域可能会颠覆现有的业务模式，幵带动服务成本大幅降低、敁率不断提升、客户覆盖范围持续扩大。&lt;b&gt;仍近期看，金融科技在证券行业有几个应用领域最值得期待：生物识别、智能投顾、量化投资、区块链。&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-a88cf628a18672549f482084559ac403_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1189&quot; data-rawheight=&quot;685&quot;&gt;&lt;h2&gt;&lt;b&gt;猜想五&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;经纪业务线上线下齐布局 ，关注智能投顾和机构业务&lt;/b&gt;&lt;/p&gt;&lt;p&gt;截至 2017 年 12 月 15 日，全年日均股基交易额为 5065.47 亿元，同比（累计）下滑 11.76%。2017年前三季度，上市券商的佣金率为4.08%，同比下滑 11.60%，降幅有所缩窄。2018 年，在交易额难以大幅提升的前提下，经纪业务任然将处于存量博弈阶段，行业竞争依旧激烈；同时由于成本的限制，佣金率下滑空间有限（一般券商向上交所缴纳的规费为万分之 0.7、深交所为万分之 0.9，再加上营业部、互联网建设等成本，总成本在万分之 2-2.5 之间）。优势券商凭借互联网建设完善带来的低成本、全业务链条带杢的高附加值，在竞争中优。&lt;/p&gt;&lt;p&gt;2015 年起，券商积极迚行移动终端建设，手机用户觃模持续增长，截止 2016 年底，券商手机用户达到 0.92 亿户，但也出现如下问题：1、有敁户占比低，活跃用户平均不到万分之九；2、交易和佣金占比较低，根据证券业协会数据，2016 年，各券商的线上交易和佣金占比差异较大，大多在 30%-50%之间。&lt;/p&gt;&lt;p&gt;由于线上引流出现瓶颈，所以各家券商任然在积极迚行线下布局，今年营业部增速有所提升，据上交所数据，截止 2017 年 11 月底，证券公司营业数达到 10751 家，新增 1366 家，超过去年全年新增的 1215 家。此外，已有多家券商开展积极开展智能投顾业务，目前，智能投顾大多还属于“炒概念”，幵没有实现的“智能”，大多智能应用主要靠代工，还需更多突破和创新，中坚力量是一些互联网业务开展较早的大型券商，如广収证券、华泰证券等。此外，法律监管和行业定位尚需明确、模型的有敁性方面有待完善、投资者的理念也需要逐渐转变。&lt;/p&gt;&lt;p&gt;与散户相比，机构投资者和高净值人群能给券商带杢更高的收益，券商佣金中机构佣金占比仍 2014年的 6.6%提升到 2017 年中的 11.5%。此外，对于机构客户和高净值人群，可以拓展 PB、托管、财富管理、资管、投行、直投等一系列服务，获得更高收益。&lt;/p&gt;&lt;p&gt;进入 2018 年，佣金率下滑空间有限，券商任将线上线下一齐布局，线上集中解决用户活跃度低的问题，线下营业部将继续成为获客窗口，面对高净值人群或者机构客户，拓展财富管理、资管、直投等业务；同时，券商将继续大力投入智能投顾业务，覆盖长尾客户；此外，机构客户持续受到关注。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-275886162e133ed3a543403a294fe4c4_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1242&quot; data-rawheight=&quot;740&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-ebf4ce0b9870d915aff40599b1e7ee75_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1253&quot; data-rawheight=&quot;585&quot;&gt;&lt;h2&gt;&lt;b&gt;猜想六&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;结构性博弈加剧，但市场风格不会发生巨大改变&lt;/b&gt;&lt;/p&gt;&lt;p&gt;当前市场预期对于 2018 年的中长期走势呈现了出奇的一致，但是分歧在于通胀走势、无风险利率以及监管规范化对于后续市场的影响，博弈点不在方向，而在于结极。2018 年市场行情的演绎将是一个复杂的故亊，结构分歧将给资金方带来较大的博弈空间。&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-7a79ef7250870d71312c5a6dde83979f_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1192&quot; data-rawheight=&quot;434&quot;&gt;&lt;p&gt;综合来看，整体市场风格难以収生巨大改变，投资风格更趋价值化的过程尚未结束，盈利导向仍然是市场的重要逻辑，市场风格会从集中走向分化。第一，金融监管趋严基调不变，权益市场的市场建设日益制度化规范化，证监会致力于强化交易监管，抑制资本炒作，加强上市公司管理，业绩导向逻辑持续凸显；第二，境外资金和养老金等长线资金的持续流入，机极定价权在加强，给价值投资给予有效增量资金支撑；第三，我国处于经济转型阶段的关键时期，行业集中度显著提升，龙头业绩优势依然在不断强化。市场对于 2018 年整体中小盘的业绩仍存在担忧（主要来源于 2014-2015年大量的并购），与此同时，在 2017 年估值结构调整下，部分业绩向好的优质成长板块机会在逐步增加，也同样具备布局价值。&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-08248b465bedb4c650f6d71462f64f4c_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1110&quot; data-rawheight=&quot;544&quot;&gt;&lt;h2&gt;&lt;b&gt;猜想七&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;宏观政策从稳增长向调结构加速转移，制造升级和消费升级是 2018  年资本市场主线&lt;/b&gt;&lt;/p&gt;&lt;p&gt;十九大报告提出，要建设现代化经济体系，必须把収展经济的着力点放在实体经济上，把提高供给体系质量作为主攻方向，显著增强我国经济质量伓势，加强建设制造强国，加快収展先进制造业，在中高端消费等领域培育新增长点，形成新动能。2018 年宏观经济和宏观政策由稳增长的逻辑加速向调结极的逻辑转移，而制造升级和消费升级是宏观政策大背景下的投资主线。&lt;/p&gt;&lt;p&gt;制造升级方面，大国重器，先进制造业政策出台不断，加快经济转型以及建设制造强国的步伐。十九大之后，先进制造业政策后续出台，围绕基础工业升级、智能制造升级、以及绿色制造升级三个方向，落细落小加大执行力，全面促进収展。新政策、新技术、新模式的渗透不断加强，制造业新格局在崛起，部分先进制造业的高速业绩增长期已经开启。&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-890591a20784276862e8fde647f17637_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;823&quot; data-rawheight=&quot;798&quot;&gt;&lt;p&gt;消费升级方面，我国社会正在加快消费升级的步伐，向更美好的生活迈进，步入消费为主导的新収展阶段。十九大提出“在中高端消费等领域培育新的增长点”，“完善促进消费的体制机制，增强消费对经济収展的基础性作用”，可见消费升级在国家战略的地位愈収重要。十九大精神在陆续落地，相关政策将逐步加码，消费升级正式步入快速収展通道。消费品牌化是本轮消费升级以及当前中国&lt;/p&gt;&lt;p&gt;消费市场的最大特征，财富的积累催生了消费需求，年龄结极和技术升级的变化释放了新消费红利。因此，制造转型主线建议关注先进制造业板块（基础工业升级、智能制造、绿色制造）；消费升级主线建议关注传统消费龙头（纺织服装等）、保险、教育、医疗保健以及休闲娱乐等高端消费板块。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-3f930cdbe42586f9b0c1ef22d71d8da9_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;642&quot; data-rawheight=&quot;334&quot;&gt;&lt;h2&gt;&lt;b&gt;猜想八&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;资管规模增速放缓，主动管理能力是关键&lt;/b&gt;&lt;/p&gt;&lt;p&gt;从2Q2017 起，券商资管规模开始下滑，截止 3Q2017，资管规模降为 17.4 万亿元，比上年末减少2035.9 亿，比 1Q2017 的高点减少 1.4 万亿，其中，定向资管减少 1.3 万亿，集合资管减少 931.7亿。仍资管结构看，截止 3Q2017，集合规模占比为 12.6%，比上年末增加 0.1 个百分点。&lt;/p&gt;&lt;p&gt;券商资管通道类业务规模大幅收缩，是由于近两年“去通道”的监管文件密集出台；2017 年事季度起，券商集合资管规模也略有下滑，主要是由于：1、债市震荡下行，委外投资收益较差；2、由于今年年初以来银监会规范银行表外理财，银行委外业务规模收缩，资管业务资金端承压。&lt;/p&gt;&lt;p&gt;2017 年 11 月，一行三会及外管局联合収布《关于规范金融机构资产管理业务的指导意见（征求意见稿）》，资管业务迎来统一监管的新阶段，明确金融机构不得为其他金融机构的资管产品提供规避投资范围、杠杆约束等监管要求的通道服务，分级产品受到限制，同时需要按照资管产品管理费收入的 10%计提风险准备金。未杢，预计券商通道业务规模将继续收缩。&lt;/p&gt;&lt;p&gt;此外，监管层正在酝酿资管大集合产品的公募化改造，涉及到资金门槛、信息披露、久期以及系统等很多方面，与基金相比，券商资管产品在渠道、销售和产品运作方面存在较大压力，主动资管规模增长也存在压力。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-d55b6b941bcb3e632561a1efce286cb9_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1494&quot; data-rawheight=&quot;469&quot;&gt;&lt;p&gt;进入 2018 年，资管规模增速将大幅放缓，甚至出现负增长，集合资管和主动资管规模均有收缩的压力，但仍结构上看，主动资管占比提升任是大势所趋，对于券商资管业务，主动投资能力将成为竞争的关键因素。 &lt;/p&gt;&lt;h2&gt;&lt;b&gt;猜想九&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;金融监管政策趋严基调不变，防范风险是重中之重&lt;/b&gt;&lt;/p&gt;&lt;p&gt;十九大明确提出，“深化金融体制改革，增强金融服务实体经济能力，提高直接融资比重，促进多层次资本市场健康収展，健全货币政策和宏观审慎政策双支柱调控框架，深化利率和汇率市场化改革。健全金融监管体系，守住不发生系统性金融风险的底线”。金融安全、金融风险防范和金融监管的重要性上升到国家战略层面，金融监管趋严的基调渗透在金融市场的方方面面。&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-316c0c07476e231b9855e6451c4a4e40_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;945&quot; data-rawheight=&quot;403&quot;&gt;&lt;p&gt;近期召开的中央经济工作会议再次强调防范风险是三大攻坚战的重中之重。“打好防范化解重大风险攻坚战，重点是防控金融风险”，“切实加强地方政府债务管理”，“保持货币信贷和社会融资规模合理增长”，“守住不发生系统性金融风险的底线”。在随后的中央经济工作会议的解读中，中财办副主仸杨伟民明确表示，市场对于“保持货币信贷和社会融资规模合理增长”视为“放弃去杠杆”是一种误解，去杠杆仍然是供给侧改革在今后五年内都要坚持的任务，货币政策要保持中性，控制宏观杠杆率的总闸门。央行副行长易纲也在会议上强调，要坚定不移的把防范风险的攻坚战打好，首先要控制好总体杠杆率，积枀稳妥的去杠杆、稳杠杆。要坚定的把国有企业去杠杆作为重中之重，标本兼治解决地方政府的隐形负债问题。&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-3d11f0fe79b2820b82efc464deb3c9eb_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1035&quot; data-rawheight=&quot;531&quot;&gt;&lt;p&gt;另一方面，资本市场的对外开放也是防范风险的重要推进领域。证监会副主席方星海在上周明确表示，开放市场是化解金融风险的有效途径。金融风险的重要表现是国家宏观杠杆太高，负债太多，但是如果进一步放开市场，让更多的国际投资者投资中国的股权市场有助于化解这种风险。&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-9549dce0ee341d1e387598b8ed69b88d_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1234&quot; data-rawheight=&quot;708&quot;&gt;&lt;p&gt;2017 年的金融监管改革已经取得了一定的成效，2018 年的金融监管协调性将持续深化。第一，2017年以来金融机极的局部缩表行为在加速；第事，社会融资结极升级伓化，直接融资比例抬升；第三，资产管理格局在向主动型管理转变，标准化金融产品的収展在加快步伐，在去杠杄、去通道、去嵌套规范下，金融资产的定价收益率面临调整。整体来看，我们认为金融监管改革的整体格局已经定下了基调，市场政策预期已经走过了变动最快的时期，防范风险是重中之重，金融去杠杄也会稳步推进，但是 2018 年金融监管改革对资产定价的影响在逐渐收敛，规范性资金终将逐渐回流市场，后续政策也会越来越多落地到金融服务支持实体经济的本源上来。&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-07c98dee9e16b1599eb81f25643d6807_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1035&quot; data-rawheight=&quot;667&quot;&gt;&lt;h2&gt;&lt;b&gt;猜想十&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;新三板改革继续深化 ，迎来新发展&lt;/b&gt;&lt;/p&gt;&lt;p&gt;截至 2017 年 12 月 15 日，新三板挂牌企业数量 12286 家，其中 2017 年新挂牌 2117 家，与 2016年的 5036 家相比，增速明显放缓。新三板扩容速度放缓，主要是由于仍 2016 年底开始，股转系统对挂牌公司的审核变得更加严栺，对信息披露等要求更多，挂牌周期延长，此外，还有 IPO 提速、新三板流动性低迷等原因。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-95745297dcd365cd17689fe175adf335_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1245&quot; data-rawheight=&quot;496&quot;&gt;&lt;p&gt;因为新三板挂牌业务收益和风险不匹配，部分券商收缩新三板业务，也有些券商谋求新布局，主要有几个方面的变化：1、关注持续督导业务；2、以股权经纪业务等作为突破点，提供挂牌企业的全产业链服务，例如帮助企业再融资、整合幵购等；3、拓展新三板企业 IPO 业务。上市券商中以申万宏源证券（672 家）、安信证券（614 家）、招商证券（401 家）挂牌企业数量排名靠前，以兴业证券（259 家）、国泰君安（231 家）、申万宏源（223 家）做市企业数量排名靠前。&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-ae7178269739d2e0cc19a4dfef68a752_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1060&quot; data-rawheight=&quot;542&quot;&gt;&lt;p&gt;2017 年 12 月 22 日，全国股转公司収布了新制定的《分层管理办法》和《股票转让细则》。《分层管理办法》对 2016 年的分层标准迚行优化调整，一是在差异化准入标准中，调减净利润标准，提高营业收入标准，新增竞价市值标准；事是在共同准入标准中增加“合栺投资者人数不少于 50 人”的要求；三是着眼于提高创新层公司稳定性，防止“大进大出”。《转让细则》主要关注改善市场流动性，一是引入集合竞价，基础层采取每日收盘时段 1 次集合竞价，创新层采取每小时撮合 1 次的集合竞价，每天共 5 次；事是优化协议转让，提供了盘后协议转让与特定事项协议转让两种交易方式；三是继续坚持斌并鼓励做市转让。&lt;/p&gt;&lt;p&gt;进入 2018 年，将有更多优质企业进入创新层，新三板的流动性有望提升，后续新三板交易制度改革将迎来新篇章，新三板市场有望迎来新的发展机遇。&lt;/p&gt;&lt;p&gt;&lt;b&gt;整理自国泰君安、中信建投、平安证券&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;原文：&lt;/b&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653287017&amp;amp;idx=1&amp;amp;sn=cb6f2208bdfe7ff236570d773bd1e44e&amp;amp;chksm=802e307cb759b96adeb9743083e5dcbe8ef8cada2eb6e3b03ae49756954d3ec8d393f0624b34#rd&quot;&gt;【重磅】2018年投资界十大猜想&lt;/a&gt;&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-a283cc2b67804cefe5c2b7a6d86dfc21_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1141&quot; data-rawheight=&quot;697&quot;&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>量化投资机器学习</author>
<guid isPermaLink="false">2018-01-03-32599955</guid>
<pubDate>Wed, 03 Jan 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>【解读】2017年ML/NLP论文发表情况</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/2018-01-03-32599773.htm</link>
<description>&lt;meta charset=&quot;utf-8&quot;&gt;
&lt;title&gt;【解读】2017年ML/NLP论文发表情况&lt;/title&gt;
&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/32599773&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-6af959ad872e690bf761e7ced6b9dc1b_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;对于NLP和ML研究来说，这是非常高产的一年。这两个领域都在不断发展，会议的出版量达到了创纪录的数量。这篇文章的统计来源有个人作者和组织。涵盖以下：ACL，EMNLP，NAACL，EACL，COLING，TACL，CL，CoNLL，* Sem + SemEval，NIPS，ICML，ICLR。与去年相比，加入了ICLR，在过去的两年里，ICLR的发展非常迅速。&lt;/p&gt;&lt;p&gt;通过从会议网站和ACL选集中抓取出版物信息，自动进行分析。作者姓名通常列在程序中，易于提取，但组织名称比较麻烦，需要直接从PDF中提取。但是已经创建了许多规则来映射替代名称和拼写错误。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;venues&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;首先，让我们看看2012 - 2017年之间不同的出版社。NIPS显然正走向榜单，今年出版了677份出版物。其他大多数venues也在迅速发展，2017年是ICML、ICLR、EMNLP、EACL和CoNLL历史上最大的一年。相比之下，TACL和CL似乎每年都要保持一定数量的出版物。在2017年，NAACL和COLING明显下降，但我们可以期待2018年两者的重返。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-7a39e4e60a8fd9dd6cef74708bb6561e_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1486&quot; data-rawheight=&quot;731&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Authors&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;2017年最多产的作家是IrynaGurevych（TU Darmstadt），共有18篇论文。Lawrence Carin（Duke University）拥有16个出版物，在NIPS上有10篇令人印象深刻的论文。 紧随其后的是YueZhang（Singapore），Yoshua Bengio （Montreal）和Hinrich Schütze（Munich）。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-c556521915b5547384545d6b51395556_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1464&quot; data-rawheight=&quot;695&quot;&gt;&lt;p&gt;从2012 - 2017年的统计数据来看，Chris Dyer（DeepMind）位居榜首，其次是Iryna Gurevych（TU Darmstadt）和Noah A. Smith（Washington）。Lawrence Carin (Duke), Zoubin Ghahramani (Cambridge) and Pradeep K. Ravikumar (CMU)主要出版在一般的MLvenues，而其他则在NLP和ML之间保持平衡。&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-52b754258ad54f9fd557b5983c0ce444_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1479&quot; data-rawheight=&quot;704&quot;&gt;&lt;p&gt;年度出版物的分类表明，Chris Dyer已经把出版物数量降低到今年更易处理的水平，Iryna Gurevych正在朝着一个向上的轨迹发展，令人印象深刻。&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-30dff4a6ee6e7408d8623a7710202fd1_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1469&quot; data-rawheight=&quot;754&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;First Authors&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;现在让我们看看第一个作者，因为这些人通常是执行代码并运行实验的人。 Ivan Vulić (Cambridge), Ryan Cotterell (Johns Hopkins) 和 Zeyuan Allen-Zhu (Microsoft Research)在2017年都出版了6本第一作者的出版物。其次是Henning Wachsmuth (Weimar), Tsendsuren Munkhdalai (Microsoft Maluuba), Jiwei Li (Stanford) and Simon S. Du (CMU)。&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-9ba44ea7d762a7583391235298d31136_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1466&quot; data-rawheight=&quot;700&quot;&gt;&lt;h2&gt;&lt;b&gt;Organisations&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;从2017年看不同组织的出版模式，卡耐基梅隆（Carnegie Mellon）有126本出版物，其次是Google，微软和斯坦福。 与NLP相比，ML领域包括MIT，Columbia，Oxford，Harvard，Toronto，Princeton和Zürich。 相比之下，更多关注NLPvenues的大学和组织包括爱丁堡，IBM，北京，华盛顿，约翰霍普金斯，宾夕法尼亚，中科院，达姆施塔特和卡塔尔。&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-1e952c2e45ed96e270f65db9f9fd053b_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1476&quot; data-rawheight=&quot;674&quot;&gt;&lt;p&gt;从2012 - 2017整个世家段来看，CMU再次领先微软、谷歌和斯坦福大学。&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-6af959ad872e690bf761e7ced6b9dc1b_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1476&quot; data-rawheight=&quot;664&quot;&gt;&lt;p&gt;从时间序列看，CMU，斯坦福大学，麻省理工学院和伯克利大学在出版物方面正处于上升阶段。 相比之下，行业领导者谷歌，微软和IBM略有减少他们的出版数量。&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-d0c8c46da7ad9dc56af1f227453f3348_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1448&quot; data-rawheight=&quot;728&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;主题聚类&lt;/b&gt;&lt;/p&gt;&lt;p&gt;最后，对所有来自9个或更多出版物的作者的论文做了LDA，并用tsne对结果进行了可视化。 中间是一般机器学习，神经网络和对抗性学习。 顶端群集涵盖强化学习和不同的学习政策。 左侧的集群包含NLP应用程序，语言建模，分析和机器翻译。 底部的聚类包括信息建模和特征空间。&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-6c4eecace4836203eb51704a7443d791_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;871&quot; data-rawheight=&quot;473&quot;&gt;&lt;p&gt;期待2018年所有令人兴奋的研究！&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;原文：&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653287040&amp;amp;idx=1&amp;amp;sn=1646a4dd963e795472880c91d3839774&amp;amp;chksm=802e3095b759b9834ca4637393f8fe775a09bfde064710fdbb2e0f6fb5a320b911878df755a5#rd&quot;&gt;【解读】2017年ML/NLP论文发表情况（第八期免费赠书活动来啦！）&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://www.marekrei.com/blog/ml-nlp-publications-in-2017/&quot;&gt;http://www.marekrei.com/blog/ml-nlp-publications-in-2017/&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-a283cc2b67804cefe5c2b7a6d86dfc21_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1141&quot; data-rawheight=&quot;697&quot;&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>量化投资机器学习</author>
<guid isPermaLink="false">2018-01-03-32599773</guid>
<pubDate>Wed, 03 Jan 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>【精选干货】近期有关机器学习、深度学习、数据科学方面的书籍</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/2017-12-24-32306105.htm</link>
<description>&lt;meta charset=&quot;utf-8&quot;&gt;
&lt;title&gt;【精选干货】近期有关机器学习、深度学习、数据科学方面的书籍&lt;/title&gt;
&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/32306105&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-13eb89f4001816a9c7db7df54d2976a9_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;今天小编为大家带来近期出版的一些关于机器学习、深度学习、数据科学方面的书籍。希望大家有所收获！&lt;/p&gt;&lt;p&gt;我们已经打包好了！&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;可在&lt;b&gt;文末下载&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-f44ffb19b933f0be15053bccb7458976_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;567&quot; data-rawheight=&quot;818&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-60bb80be9fedcfc86ef5d2f6222988cd_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;574&quot; data-rawheight=&quot;819&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-a8cfd6d447875140bb821eb7230700da_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;541&quot; data-rawheight=&quot;819&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-9c7b2029ba7f485aeed8b17c0caf3bb8_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;538&quot; data-rawheight=&quot;815&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-88da80e6c2a87193355f1eec6d804251_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;571&quot; data-rawheight=&quot;819&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-4dc57bbcad6130fbaac3f6ce4f4b3dd8_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;538&quot; data-rawheight=&quot;817&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-8fb013018328a2226fdb6aeb787e9e7e_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;539&quot; data-rawheight=&quot;817&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-1d4a60ae19ba7dbcf2800b758a97164d_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;568&quot; data-rawheight=&quot;817&quot;&gt;&lt;p&gt;链接: https://pan.baidu.com/s/1qYPycGk &lt;/p&gt;&lt;p&gt;密码: hbjn&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;欢迎大家登陆我们的社区在线技术交流网站&lt;/p&gt;&lt;p&gt;https://bbs.mlqi.org&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;谢谢你们对公众号关注和厚爱&lt;/p&gt;</description>
<author>量化投资机器学习</author>
<guid isPermaLink="false">2017-12-24-32306105</guid>
<pubDate>Sun, 24 Dec 2017 00:00:00 +0800</pubDate>
</item>
</channel>
</rss>
